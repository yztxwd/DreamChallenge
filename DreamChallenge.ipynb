{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dream Challenge Model Prototye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapseclient \n",
    "import synapseutils \n",
    "\n",
    "syn = synapseclient.Synapse() \n",
    "syn.login('yztxwd@gmail.com','9zqqW9Jy5QhRFeS') \n",
    "synapseutils.syncFromSynapse(syn, 'syn30026233', './data/')\n",
    "synapseutils.syncFromSynapse(syn, 'syn28469146', './data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_table(\"./data/train_sequences.txt\", header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_full = pd.read_table('./data/train_sequences.txt', header=None, names=['seq', 'target'], dtype={'seq':str, 'target':np.float32})\n",
    "df_train, df_remain = train_test_split(df_full, test_size=0.2, random_state=1)\n",
    "df_val, df_test = train_test_split(df_remain, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DreamChallengeDataset(df_train.reset_index(), transform=DNA2OneHot())\n",
    "dataset_val = DreamChallengeDataset(df_val.reset_index(), transform=DNA2OneHot())\n",
    "dataset_test = DreamChallengeDataset(df_test.reset_index(), transform=DNA2OneHot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>seq</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4105474</td>\n",
       "      <td>TGCATTTTTTTCACATCTGCACATTTGATGTTCCTACCACGCGCTA...</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4654389</td>\n",
       "      <td>TGCATTTTTTTCACATCGATAGTTGGTTGGATTTTACCGTTGATAG...</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2266084</td>\n",
       "      <td>TGCATTTTTTTCACATCTCTGTAGGGTTCATCACCTGTTTTATGGG...</td>\n",
       "      <td>8.490439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5141768</td>\n",
       "      <td>TGCATTTTTTTCACATCGCTGTGCCTTGCGCATGCTTGGGGTGTAT...</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5849166</td>\n",
       "      <td>TGCATTTTTTTCACATCAATGATTCGCCTAGCTCGATTGTTTATGC...</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673921</th>\n",
       "      <td>4269572</td>\n",
       "      <td>TGCATTTTTTTCACATCATTAATGTGGCCCCCGTATCCATATAGTT...</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673922</th>\n",
       "      <td>2416930</td>\n",
       "      <td>TGCATTTTTTTCACATCCGGAGACCGATTGTTGGATCCTAGAAGCA...</td>\n",
       "      <td>13.242847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673923</th>\n",
       "      <td>5680606</td>\n",
       "      <td>TGCATTTTTTTCACATCTAATTTGAAGTTTATTTGTCACATATTCT...</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673924</th>\n",
       "      <td>4536704</td>\n",
       "      <td>TGCATTTTTTTCACATCTTCCGGCGCGCATGATCGGTCTAATGTAA...</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>673925</th>\n",
       "      <td>1299974</td>\n",
       "      <td>TGCATTTTTTTCACATCGCCATATCCGGGAAGAGGCCGCCTGTTTA...</td>\n",
       "      <td>12.933074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>673926 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          index                                                seq     target\n",
       "0       4105474  TGCATTTTTTTCACATCTGCACATTTGATGTTCCTACCACGCGCTA...  15.000000\n",
       "1       4654389  TGCATTTTTTTCACATCGATAGTTGGTTGGATTTTACCGTTGATAG...  11.000000\n",
       "2       2266084  TGCATTTTTTTCACATCTCTGTAGGGTTCATCACCTGTTTTATGGG...   8.490439\n",
       "3       5141768  TGCATTTTTTTCACATCGCTGTGCCTTGCGCATGCTTGGGGTGTAT...  13.000000\n",
       "4       5849166  TGCATTTTTTTCACATCAATGATTCGCCTAGCTCGATTGTTTATGC...   9.000000\n",
       "...         ...                                                ...        ...\n",
       "673921  4269572  TGCATTTTTTTCACATCATTAATGTGGCCCCCGTATCCATATAGTT...  12.000000\n",
       "673922  2416930  TGCATTTTTTTCACATCCGGAGACCGATTGTTGGATCCTAGAAGCA...  13.242847\n",
       "673923  5680606  TGCATTTTTTTCACATCTAATTTGAAGTTTATTTGTCACATATTCT...   8.000000\n",
       "673924  4536704  TGCATTTTTTTCACATCTTCCGGCGCGCATGATCGGTCTAATGTAA...  12.000000\n",
       "673925  1299974  TGCATTTTTTTCACATCGCCATATCCGGGAAGAGGCCGCCTGTTTA...  12.933074\n",
       "\n",
       "[673926 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfrecord\n",
    "\n",
    "def get_data_TFRecord_worker(dataset, outprefix):\n",
    "\n",
    "    TFRecord_file = outprefix + \".TFRecord\"\n",
    "    writer = tfrecord.TFRecordWriter(TFRecord_file)\n",
    "    count = 0\n",
    "    for idx, (seq, target) in enumerate(dataset):\n",
    "        writer.write({'seq': (list(seq.flatten()), 'float'),\n",
    "                        'target': (target[0], 'float')})\n",
    "        count += 1\n",
    "        if count % 100000 == 0: print(f\"{count} written\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data_TFRecord_worker(dataset_train, \"train\")\n",
    "#get_data_TFRecord_worker(dataset_val, \"val\")\n",
    "#get_data_TFRecord_worker(dataset_test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### webdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DreamChallengeDataset(df_train.reset_index(), transform=DNA2OneHot())\n",
    "dataset_val = DreamChallengeDataset(df_val.reset_index(), transform=DNA2OneHot())\n",
    "dataset_test = DreamChallengeDataset(df_test.reset_index(), transform=DNA2OneHot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeWDS(dataset, prefix):\n",
    "    sink = wds.ShardWriter(\"shards/\" + prefix + \"-%02d.tar\", maxcount=10000)\n",
    "    count = 0\n",
    "    for idx, (seq, target) in enumerate(dataset):\n",
    "        sink.write({\n",
    "            '__key__': 'sample%06d' % idx,\n",
    "            'seq.npy': seq,\n",
    "            'target.npy': target\n",
    "        })\n",
    "        count += 1\n",
    "        if count % 100000 == 0: print(f\"{count} written\")\n",
    "    sink.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing shards/train-00.tar 0 0.0 GB 0\n",
      "# writing shards/train-01.tar 10000 0.0 GB 10000\n",
      "# writing shards/train-02.tar 10000 0.0 GB 20000\n",
      "# writing shards/train-03.tar 10000 0.0 GB 30000\n",
      "# writing shards/train-04.tar 10000 0.0 GB 40000\n",
      "# writing shards/train-05.tar 10000 0.0 GB 50000\n",
      "# writing shards/train-06.tar 10000 0.0 GB 60000\n",
      "# writing shards/train-07.tar 10000 0.0 GB 70000\n",
      "# writing shards/train-08.tar 10000 0.0 GB 80000\n",
      "# writing shards/train-09.tar 10000 0.0 GB 90000\n",
      "100000 written\n",
      "# writing shards/train-10.tar 10000 0.0 GB 100000\n",
      "# writing shards/train-11.tar 10000 0.0 GB 110000\n",
      "# writing shards/train-12.tar 10000 0.0 GB 120000\n",
      "# writing shards/train-13.tar 10000 0.0 GB 130000\n",
      "# writing shards/train-14.tar 10000 0.0 GB 140000\n",
      "# writing shards/train-15.tar 10000 0.0 GB 150000\n",
      "# writing shards/train-16.tar 10000 0.0 GB 160000\n",
      "# writing shards/train-17.tar 10000 0.0 GB 170000\n",
      "# writing shards/train-18.tar 10000 0.0 GB 180000\n",
      "# writing shards/train-19.tar 10000 0.0 GB 190000\n",
      "200000 written\n",
      "# writing shards/train-20.tar 10000 0.0 GB 200000\n",
      "# writing shards/train-21.tar 10000 0.0 GB 210000\n",
      "# writing shards/train-22.tar 10000 0.0 GB 220000\n",
      "# writing shards/train-23.tar 10000 0.0 GB 230000\n",
      "# writing shards/train-24.tar 10000 0.0 GB 240000\n",
      "# writing shards/train-25.tar 10000 0.0 GB 250000\n",
      "# writing shards/train-26.tar 10000 0.0 GB 260000\n",
      "# writing shards/train-27.tar 10000 0.0 GB 270000\n",
      "# writing shards/train-28.tar 10000 0.0 GB 280000\n",
      "# writing shards/train-29.tar 10000 0.0 GB 290000\n",
      "300000 written\n",
      "# writing shards/train-30.tar 10000 0.0 GB 300000\n",
      "# writing shards/train-31.tar 10000 0.0 GB 310000\n",
      "# writing shards/train-32.tar 10000 0.0 GB 320000\n",
      "# writing shards/train-33.tar 10000 0.0 GB 330000\n",
      "# writing shards/train-34.tar 10000 0.0 GB 340000\n",
      "# writing shards/train-35.tar 10000 0.0 GB 350000\n",
      "# writing shards/train-36.tar 10000 0.0 GB 360000\n",
      "# writing shards/train-37.tar 10000 0.0 GB 370000\n",
      "# writing shards/train-38.tar 10000 0.0 GB 380000\n",
      "# writing shards/train-39.tar 10000 0.0 GB 390000\n",
      "400000 written\n",
      "# writing shards/train-40.tar 10000 0.0 GB 400000\n",
      "# writing shards/train-41.tar 10000 0.0 GB 410000\n",
      "# writing shards/train-42.tar 10000 0.0 GB 420000\n",
      "# writing shards/train-43.tar 10000 0.0 GB 430000\n",
      "# writing shards/train-44.tar 10000 0.0 GB 440000\n",
      "# writing shards/train-45.tar 10000 0.0 GB 450000\n",
      "# writing shards/train-46.tar 10000 0.0 GB 460000\n",
      "# writing shards/train-47.tar 10000 0.0 GB 470000\n",
      "# writing shards/train-48.tar 10000 0.0 GB 480000\n",
      "# writing shards/train-49.tar 10000 0.0 GB 490000\n",
      "500000 written\n",
      "# writing shards/train-50.tar 10000 0.0 GB 500000\n",
      "# writing shards/train-51.tar 10000 0.0 GB 510000\n",
      "# writing shards/train-52.tar 10000 0.0 GB 520000\n",
      "# writing shards/train-53.tar 10000 0.0 GB 530000\n",
      "# writing shards/train-54.tar 10000 0.0 GB 540000\n",
      "# writing shards/train-55.tar 10000 0.0 GB 550000\n",
      "# writing shards/train-56.tar 10000 0.0 GB 560000\n",
      "# writing shards/train-57.tar 10000 0.0 GB 570000\n",
      "# writing shards/train-58.tar 10000 0.0 GB 580000\n",
      "# writing shards/train-59.tar 10000 0.0 GB 590000\n",
      "600000 written\n",
      "# writing shards/train-60.tar 10000 0.0 GB 600000\n",
      "# writing shards/train-61.tar 10000 0.0 GB 610000\n",
      "# writing shards/train-62.tar 10000 0.0 GB 620000\n",
      "# writing shards/train-63.tar 10000 0.0 GB 630000\n",
      "# writing shards/train-64.tar 10000 0.0 GB 640000\n",
      "# writing shards/train-65.tar 10000 0.0 GB 650000\n",
      "# writing shards/train-66.tar 10000 0.0 GB 660000\n",
      "# writing shards/train-67.tar 10000 0.0 GB 670000\n",
      "# writing shards/train-68.tar 10000 0.0 GB 680000\n",
      "# writing shards/train-69.tar 10000 0.0 GB 690000\n",
      "700000 written\n",
      "# writing shards/train-70.tar 10000 0.0 GB 700000\n",
      "# writing shards/train-71.tar 10000 0.0 GB 710000\n",
      "# writing shards/train-72.tar 10000 0.0 GB 720000\n",
      "# writing shards/train-73.tar 10000 0.0 GB 730000\n",
      "# writing shards/train-74.tar 10000 0.0 GB 740000\n",
      "# writing shards/train-75.tar 10000 0.0 GB 750000\n",
      "# writing shards/train-76.tar 10000 0.0 GB 760000\n",
      "# writing shards/train-77.tar 10000 0.0 GB 770000\n",
      "# writing shards/train-78.tar 10000 0.0 GB 780000\n",
      "# writing shards/train-79.tar 10000 0.0 GB 790000\n",
      "800000 written\n",
      "# writing shards/train-80.tar 10000 0.0 GB 800000\n",
      "# writing shards/train-81.tar 10000 0.0 GB 810000\n",
      "# writing shards/train-82.tar 10000 0.0 GB 820000\n",
      "# writing shards/train-83.tar 10000 0.0 GB 830000\n",
      "# writing shards/train-84.tar 10000 0.0 GB 840000\n",
      "# writing shards/train-85.tar 10000 0.0 GB 850000\n",
      "# writing shards/train-86.tar 10000 0.0 GB 860000\n",
      "# writing shards/train-87.tar 10000 0.0 GB 870000\n",
      "# writing shards/train-88.tar 10000 0.0 GB 880000\n",
      "# writing shards/train-89.tar 10000 0.0 GB 890000\n",
      "900000 written\n",
      "# writing shards/train-90.tar 10000 0.0 GB 900000\n",
      "# writing shards/train-91.tar 10000 0.0 GB 910000\n",
      "# writing shards/train-92.tar 10000 0.0 GB 920000\n",
      "# writing shards/train-93.tar 10000 0.0 GB 930000\n",
      "# writing shards/train-94.tar 10000 0.0 GB 940000\n",
      "# writing shards/train-95.tar 10000 0.0 GB 950000\n",
      "# writing shards/train-96.tar 10000 0.0 GB 960000\n",
      "# writing shards/train-97.tar 10000 0.0 GB 970000\n",
      "# writing shards/train-98.tar 10000 0.0 GB 980000\n",
      "# writing shards/train-99.tar 10000 0.0 GB 990000\n",
      "1000000 written\n",
      "# writing shards/train-100.tar 10000 0.0 GB 1000000\n",
      "# writing shards/train-101.tar 10000 0.0 GB 1010000\n",
      "# writing shards/train-102.tar 10000 0.0 GB 1020000\n",
      "# writing shards/train-103.tar 10000 0.0 GB 1030000\n",
      "# writing shards/train-104.tar 10000 0.0 GB 1040000\n",
      "# writing shards/train-105.tar 10000 0.0 GB 1050000\n",
      "# writing shards/train-106.tar 10000 0.0 GB 1060000\n",
      "# writing shards/train-107.tar 10000 0.0 GB 1070000\n",
      "# writing shards/train-108.tar 10000 0.0 GB 1080000\n",
      "# writing shards/train-109.tar 10000 0.0 GB 1090000\n",
      "1100000 written\n",
      "# writing shards/train-110.tar 10000 0.0 GB 1100000\n",
      "# writing shards/train-111.tar 10000 0.0 GB 1110000\n",
      "# writing shards/train-112.tar 10000 0.0 GB 1120000\n",
      "# writing shards/train-113.tar 10000 0.0 GB 1130000\n",
      "# writing shards/train-114.tar 10000 0.0 GB 1140000\n",
      "# writing shards/train-115.tar 10000 0.0 GB 1150000\n",
      "# writing shards/train-116.tar 10000 0.0 GB 1160000\n",
      "# writing shards/train-117.tar 10000 0.0 GB 1170000\n",
      "# writing shards/train-118.tar 10000 0.0 GB 1180000\n",
      "# writing shards/train-119.tar 10000 0.0 GB 1190000\n",
      "1200000 written\n",
      "# writing shards/train-120.tar 10000 0.0 GB 1200000\n",
      "# writing shards/train-121.tar 10000 0.0 GB 1210000\n",
      "# writing shards/train-122.tar 10000 0.0 GB 1220000\n",
      "# writing shards/train-123.tar 10000 0.0 GB 1230000\n",
      "# writing shards/train-124.tar 10000 0.0 GB 1240000\n",
      "# writing shards/train-125.tar 10000 0.0 GB 1250000\n",
      "# writing shards/train-126.tar 10000 0.0 GB 1260000\n",
      "# writing shards/train-127.tar 10000 0.0 GB 1270000\n",
      "# writing shards/train-128.tar 10000 0.0 GB 1280000\n",
      "# writing shards/train-129.tar 10000 0.0 GB 1290000\n",
      "1300000 written\n",
      "# writing shards/train-130.tar 10000 0.0 GB 1300000\n",
      "# writing shards/train-131.tar 10000 0.0 GB 1310000\n",
      "# writing shards/train-132.tar 10000 0.0 GB 1320000\n",
      "# writing shards/train-133.tar 10000 0.0 GB 1330000\n",
      "# writing shards/train-134.tar 10000 0.0 GB 1340000\n",
      "# writing shards/train-135.tar 10000 0.0 GB 1350000\n",
      "# writing shards/train-136.tar 10000 0.0 GB 1360000\n",
      "# writing shards/train-137.tar 10000 0.0 GB 1370000\n",
      "# writing shards/train-138.tar 10000 0.0 GB 1380000\n",
      "# writing shards/train-139.tar 10000 0.0 GB 1390000\n",
      "1400000 written\n",
      "# writing shards/train-140.tar 10000 0.0 GB 1400000\n",
      "# writing shards/train-141.tar 10000 0.0 GB 1410000\n",
      "# writing shards/train-142.tar 10000 0.0 GB 1420000\n",
      "# writing shards/train-143.tar 10000 0.0 GB 1430000\n",
      "# writing shards/train-144.tar 10000 0.0 GB 1440000\n",
      "# writing shards/train-145.tar 10000 0.0 GB 1450000\n",
      "# writing shards/train-146.tar 10000 0.0 GB 1460000\n",
      "# writing shards/train-147.tar 10000 0.0 GB 1470000\n",
      "# writing shards/train-148.tar 10000 0.0 GB 1480000\n",
      "# writing shards/train-149.tar 10000 0.0 GB 1490000\n",
      "1500000 written\n",
      "# writing shards/train-150.tar 10000 0.0 GB 1500000\n",
      "# writing shards/train-151.tar 10000 0.0 GB 1510000\n",
      "# writing shards/train-152.tar 10000 0.0 GB 1520000\n",
      "# writing shards/train-153.tar 10000 0.0 GB 1530000\n",
      "# writing shards/train-154.tar 10000 0.0 GB 1540000\n",
      "# writing shards/train-155.tar 10000 0.0 GB 1550000\n",
      "# writing shards/train-156.tar 10000 0.0 GB 1560000\n",
      "# writing shards/train-157.tar 10000 0.0 GB 1570000\n",
      "# writing shards/train-158.tar 10000 0.0 GB 1580000\n",
      "# writing shards/train-159.tar 10000 0.0 GB 1590000\n",
      "1600000 written\n",
      "# writing shards/train-160.tar 10000 0.0 GB 1600000\n",
      "# writing shards/train-161.tar 10000 0.0 GB 1610000\n",
      "# writing shards/train-162.tar 10000 0.0 GB 1620000\n",
      "# writing shards/train-163.tar 10000 0.0 GB 1630000\n",
      "# writing shards/train-164.tar 10000 0.0 GB 1640000\n",
      "# writing shards/train-165.tar 10000 0.0 GB 1650000\n",
      "# writing shards/train-166.tar 10000 0.0 GB 1660000\n",
      "# writing shards/train-167.tar 10000 0.0 GB 1670000\n",
      "# writing shards/train-168.tar 10000 0.0 GB 1680000\n",
      "# writing shards/train-169.tar 10000 0.0 GB 1690000\n",
      "1700000 written\n",
      "# writing shards/train-170.tar 10000 0.0 GB 1700000\n",
      "# writing shards/train-171.tar 10000 0.0 GB 1710000\n",
      "# writing shards/train-172.tar 10000 0.0 GB 1720000\n",
      "# writing shards/train-173.tar 10000 0.0 GB 1730000\n",
      "# writing shards/train-174.tar 10000 0.0 GB 1740000\n",
      "# writing shards/train-175.tar 10000 0.0 GB 1750000\n",
      "# writing shards/train-176.tar 10000 0.0 GB 1760000\n",
      "# writing shards/train-177.tar 10000 0.0 GB 1770000\n",
      "# writing shards/train-178.tar 10000 0.0 GB 1780000\n",
      "# writing shards/train-179.tar 10000 0.0 GB 1790000\n",
      "1800000 written\n",
      "# writing shards/train-180.tar 10000 0.0 GB 1800000\n",
      "# writing shards/train-181.tar 10000 0.0 GB 1810000\n",
      "# writing shards/train-182.tar 10000 0.0 GB 1820000\n",
      "# writing shards/train-183.tar 10000 0.0 GB 1830000\n",
      "# writing shards/train-184.tar 10000 0.0 GB 1840000\n",
      "# writing shards/train-185.tar 10000 0.0 GB 1850000\n",
      "# writing shards/train-186.tar 10000 0.0 GB 1860000\n",
      "# writing shards/train-187.tar 10000 0.0 GB 1870000\n",
      "# writing shards/train-188.tar 10000 0.0 GB 1880000\n",
      "# writing shards/train-189.tar 10000 0.0 GB 1890000\n",
      "1900000 written\n",
      "# writing shards/train-190.tar 10000 0.0 GB 1900000\n",
      "# writing shards/train-191.tar 10000 0.0 GB 1910000\n",
      "# writing shards/train-192.tar 10000 0.0 GB 1920000\n",
      "# writing shards/train-193.tar 10000 0.0 GB 1930000\n",
      "# writing shards/train-194.tar 10000 0.0 GB 1940000\n",
      "# writing shards/train-195.tar 10000 0.0 GB 1950000\n",
      "# writing shards/train-196.tar 10000 0.0 GB 1960000\n",
      "# writing shards/train-197.tar 10000 0.0 GB 1970000\n",
      "# writing shards/train-198.tar 10000 0.0 GB 1980000\n",
      "# writing shards/train-199.tar 10000 0.0 GB 1990000\n",
      "2000000 written\n",
      "# writing shards/train-200.tar 10000 0.0 GB 2000000\n",
      "# writing shards/train-201.tar 10000 0.0 GB 2010000\n",
      "# writing shards/train-202.tar 10000 0.0 GB 2020000\n",
      "# writing shards/train-203.tar 10000 0.0 GB 2030000\n",
      "# writing shards/train-204.tar 10000 0.0 GB 2040000\n",
      "# writing shards/train-205.tar 10000 0.0 GB 2050000\n",
      "# writing shards/train-206.tar 10000 0.0 GB 2060000\n",
      "# writing shards/train-207.tar 10000 0.0 GB 2070000\n",
      "# writing shards/train-208.tar 10000 0.0 GB 2080000\n",
      "# writing shards/train-209.tar 10000 0.0 GB 2090000\n",
      "2100000 written\n",
      "# writing shards/train-210.tar 10000 0.0 GB 2100000\n",
      "# writing shards/train-211.tar 10000 0.0 GB 2110000\n",
      "# writing shards/train-212.tar 10000 0.0 GB 2120000\n",
      "# writing shards/train-213.tar 10000 0.0 GB 2130000\n",
      "# writing shards/train-214.tar 10000 0.0 GB 2140000\n",
      "# writing shards/train-215.tar 10000 0.0 GB 2150000\n",
      "# writing shards/train-216.tar 10000 0.0 GB 2160000\n",
      "# writing shards/train-217.tar 10000 0.0 GB 2170000\n",
      "# writing shards/train-218.tar 10000 0.0 GB 2180000\n",
      "# writing shards/train-219.tar 10000 0.0 GB 2190000\n",
      "2200000 written\n",
      "# writing shards/train-220.tar 10000 0.0 GB 2200000\n",
      "# writing shards/train-221.tar 10000 0.0 GB 2210000\n",
      "# writing shards/train-222.tar 10000 0.0 GB 2220000\n",
      "# writing shards/train-223.tar 10000 0.0 GB 2230000\n",
      "# writing shards/train-224.tar 10000 0.0 GB 2240000\n",
      "# writing shards/train-225.tar 10000 0.0 GB 2250000\n",
      "# writing shards/train-226.tar 10000 0.0 GB 2260000\n",
      "# writing shards/train-227.tar 10000 0.0 GB 2270000\n",
      "# writing shards/train-228.tar 10000 0.0 GB 2280000\n",
      "# writing shards/train-229.tar 10000 0.0 GB 2290000\n",
      "2300000 written\n",
      "# writing shards/train-230.tar 10000 0.0 GB 2300000\n",
      "# writing shards/train-231.tar 10000 0.0 GB 2310000\n",
      "# writing shards/train-232.tar 10000 0.0 GB 2320000\n",
      "# writing shards/train-233.tar 10000 0.0 GB 2330000\n",
      "# writing shards/train-234.tar 10000 0.0 GB 2340000\n",
      "# writing shards/train-235.tar 10000 0.0 GB 2350000\n",
      "# writing shards/train-236.tar 10000 0.0 GB 2360000\n",
      "# writing shards/train-237.tar 10000 0.0 GB 2370000\n",
      "# writing shards/train-238.tar 10000 0.0 GB 2380000\n",
      "# writing shards/train-239.tar 10000 0.0 GB 2390000\n",
      "2400000 written\n",
      "# writing shards/train-240.tar 10000 0.0 GB 2400000\n",
      "# writing shards/train-241.tar 10000 0.0 GB 2410000\n",
      "# writing shards/train-242.tar 10000 0.0 GB 2420000\n",
      "# writing shards/train-243.tar 10000 0.0 GB 2430000\n",
      "# writing shards/train-244.tar 10000 0.0 GB 2440000\n",
      "# writing shards/train-245.tar 10000 0.0 GB 2450000\n",
      "# writing shards/train-246.tar 10000 0.0 GB 2460000\n",
      "# writing shards/train-247.tar 10000 0.0 GB 2470000\n",
      "# writing shards/train-248.tar 10000 0.0 GB 2480000\n",
      "# writing shards/train-249.tar 10000 0.0 GB 2490000\n",
      "2500000 written\n",
      "# writing shards/train-250.tar 10000 0.0 GB 2500000\n",
      "# writing shards/train-251.tar 10000 0.0 GB 2510000\n",
      "# writing shards/train-252.tar 10000 0.0 GB 2520000\n",
      "# writing shards/train-253.tar 10000 0.0 GB 2530000\n",
      "# writing shards/train-254.tar 10000 0.0 GB 2540000\n",
      "# writing shards/train-255.tar 10000 0.0 GB 2550000\n",
      "# writing shards/train-256.tar 10000 0.0 GB 2560000\n",
      "# writing shards/train-257.tar 10000 0.0 GB 2570000\n",
      "# writing shards/train-258.tar 10000 0.0 GB 2580000\n",
      "# writing shards/train-259.tar 10000 0.0 GB 2590000\n",
      "2600000 written\n",
      "# writing shards/train-260.tar 10000 0.0 GB 2600000\n",
      "# writing shards/train-261.tar 10000 0.0 GB 2610000\n",
      "# writing shards/train-262.tar 10000 0.0 GB 2620000\n",
      "# writing shards/train-263.tar 10000 0.0 GB 2630000\n",
      "# writing shards/train-264.tar 10000 0.0 GB 2640000\n",
      "# writing shards/train-265.tar 10000 0.0 GB 2650000\n",
      "# writing shards/train-266.tar 10000 0.0 GB 2660000\n",
      "# writing shards/train-267.tar 10000 0.0 GB 2670000\n",
      "# writing shards/train-268.tar 10000 0.0 GB 2680000\n",
      "# writing shards/train-269.tar 10000 0.0 GB 2690000\n",
      "2700000 written\n",
      "# writing shards/train-270.tar 10000 0.0 GB 2700000\n",
      "# writing shards/train-271.tar 10000 0.0 GB 2710000\n",
      "# writing shards/train-272.tar 10000 0.0 GB 2720000\n",
      "# writing shards/train-273.tar 10000 0.0 GB 2730000\n",
      "# writing shards/train-274.tar 10000 0.0 GB 2740000\n",
      "# writing shards/train-275.tar 10000 0.0 GB 2750000\n",
      "# writing shards/train-276.tar 10000 0.0 GB 2760000\n",
      "# writing shards/train-277.tar 10000 0.0 GB 2770000\n",
      "# writing shards/train-278.tar 10000 0.0 GB 2780000\n",
      "# writing shards/train-279.tar 10000 0.0 GB 2790000\n",
      "2800000 written\n",
      "# writing shards/train-280.tar 10000 0.0 GB 2800000\n",
      "# writing shards/train-281.tar 10000 0.0 GB 2810000\n",
      "# writing shards/train-282.tar 10000 0.0 GB 2820000\n",
      "# writing shards/train-283.tar 10000 0.0 GB 2830000\n",
      "# writing shards/train-284.tar 10000 0.0 GB 2840000\n",
      "# writing shards/train-285.tar 10000 0.0 GB 2850000\n",
      "# writing shards/train-286.tar 10000 0.0 GB 2860000\n",
      "# writing shards/train-287.tar 10000 0.0 GB 2870000\n",
      "# writing shards/train-288.tar 10000 0.0 GB 2880000\n",
      "# writing shards/train-289.tar 10000 0.0 GB 2890000\n",
      "2900000 written\n",
      "# writing shards/train-290.tar 10000 0.0 GB 2900000\n",
      "# writing shards/train-291.tar 10000 0.0 GB 2910000\n",
      "# writing shards/train-292.tar 10000 0.0 GB 2920000\n",
      "# writing shards/train-293.tar 10000 0.0 GB 2930000\n",
      "# writing shards/train-294.tar 10000 0.0 GB 2940000\n",
      "# writing shards/train-295.tar 10000 0.0 GB 2950000\n",
      "# writing shards/train-296.tar 10000 0.0 GB 2960000\n",
      "# writing shards/train-297.tar 10000 0.0 GB 2970000\n",
      "# writing shards/train-298.tar 10000 0.0 GB 2980000\n",
      "# writing shards/train-299.tar 10000 0.0 GB 2990000\n",
      "3000000 written\n",
      "# writing shards/train-300.tar 10000 0.0 GB 3000000\n",
      "# writing shards/train-301.tar 10000 0.0 GB 3010000\n",
      "# writing shards/train-302.tar 10000 0.0 GB 3020000\n",
      "# writing shards/train-303.tar 10000 0.0 GB 3030000\n",
      "# writing shards/train-304.tar 10000 0.0 GB 3040000\n",
      "# writing shards/train-305.tar 10000 0.0 GB 3050000\n",
      "# writing shards/train-306.tar 10000 0.0 GB 3060000\n",
      "# writing shards/train-307.tar 10000 0.0 GB 3070000\n",
      "# writing shards/train-308.tar 10000 0.0 GB 3080000\n",
      "# writing shards/train-309.tar 10000 0.0 GB 3090000\n",
      "3100000 written\n",
      "# writing shards/train-310.tar 10000 0.0 GB 3100000\n",
      "# writing shards/train-311.tar 10000 0.0 GB 3110000\n",
      "# writing shards/train-312.tar 10000 0.0 GB 3120000\n",
      "# writing shards/train-313.tar 10000 0.0 GB 3130000\n",
      "# writing shards/train-314.tar 10000 0.0 GB 3140000\n",
      "# writing shards/train-315.tar 10000 0.0 GB 3150000\n",
      "# writing shards/train-316.tar 10000 0.0 GB 3160000\n",
      "# writing shards/train-317.tar 10000 0.0 GB 3170000\n",
      "# writing shards/train-318.tar 10000 0.0 GB 3180000\n",
      "# writing shards/train-319.tar 10000 0.0 GB 3190000\n",
      "3200000 written\n",
      "# writing shards/train-320.tar 10000 0.0 GB 3200000\n",
      "# writing shards/train-321.tar 10000 0.0 GB 3210000\n",
      "# writing shards/train-322.tar 10000 0.0 GB 3220000\n",
      "# writing shards/train-323.tar 10000 0.0 GB 3230000\n",
      "# writing shards/train-324.tar 10000 0.0 GB 3240000\n",
      "# writing shards/train-325.tar 10000 0.0 GB 3250000\n",
      "# writing shards/train-326.tar 10000 0.0 GB 3260000\n",
      "# writing shards/train-327.tar 10000 0.0 GB 3270000\n",
      "# writing shards/train-328.tar 10000 0.0 GB 3280000\n",
      "# writing shards/train-329.tar 10000 0.0 GB 3290000\n",
      "3300000 written\n",
      "# writing shards/train-330.tar 10000 0.0 GB 3300000\n",
      "# writing shards/train-331.tar 10000 0.0 GB 3310000\n",
      "# writing shards/train-332.tar 10000 0.0 GB 3320000\n",
      "# writing shards/train-333.tar 10000 0.0 GB 3330000\n",
      "# writing shards/train-334.tar 10000 0.0 GB 3340000\n",
      "# writing shards/train-335.tar 10000 0.0 GB 3350000\n",
      "# writing shards/train-336.tar 10000 0.0 GB 3360000\n",
      "# writing shards/train-337.tar 10000 0.0 GB 3370000\n",
      "# writing shards/train-338.tar 10000 0.0 GB 3380000\n",
      "# writing shards/train-339.tar 10000 0.0 GB 3390000\n",
      "3400000 written\n",
      "# writing shards/train-340.tar 10000 0.0 GB 3400000\n",
      "# writing shards/train-341.tar 10000 0.0 GB 3410000\n",
      "# writing shards/train-342.tar 10000 0.0 GB 3420000\n",
      "# writing shards/train-343.tar 10000 0.0 GB 3430000\n",
      "# writing shards/train-344.tar 10000 0.0 GB 3440000\n",
      "# writing shards/train-345.tar 10000 0.0 GB 3450000\n",
      "# writing shards/train-346.tar 10000 0.0 GB 3460000\n",
      "# writing shards/train-347.tar 10000 0.0 GB 3470000\n",
      "# writing shards/train-348.tar 10000 0.0 GB 3480000\n",
      "# writing shards/train-349.tar 10000 0.0 GB 3490000\n",
      "3500000 written\n",
      "# writing shards/train-350.tar 10000 0.0 GB 3500000\n",
      "# writing shards/train-351.tar 10000 0.0 GB 3510000\n",
      "# writing shards/train-352.tar 10000 0.0 GB 3520000\n",
      "# writing shards/train-353.tar 10000 0.0 GB 3530000\n",
      "# writing shards/train-354.tar 10000 0.0 GB 3540000\n",
      "# writing shards/train-355.tar 10000 0.0 GB 3550000\n",
      "# writing shards/train-356.tar 10000 0.0 GB 3560000\n",
      "# writing shards/train-357.tar 10000 0.0 GB 3570000\n",
      "# writing shards/train-358.tar 10000 0.0 GB 3580000\n",
      "# writing shards/train-359.tar 10000 0.0 GB 3590000\n",
      "3600000 written\n",
      "# writing shards/train-360.tar 10000 0.0 GB 3600000\n",
      "# writing shards/train-361.tar 10000 0.0 GB 3610000\n",
      "# writing shards/train-362.tar 10000 0.0 GB 3620000\n",
      "# writing shards/train-363.tar 10000 0.0 GB 3630000\n",
      "# writing shards/train-364.tar 10000 0.0 GB 3640000\n",
      "# writing shards/train-365.tar 10000 0.0 GB 3650000\n",
      "# writing shards/train-366.tar 10000 0.0 GB 3660000\n",
      "# writing shards/train-367.tar 10000 0.0 GB 3670000\n",
      "# writing shards/train-368.tar 10000 0.0 GB 3680000\n",
      "# writing shards/train-369.tar 10000 0.0 GB 3690000\n",
      "3700000 written\n",
      "# writing shards/train-370.tar 10000 0.0 GB 3700000\n",
      "# writing shards/train-371.tar 10000 0.0 GB 3710000\n",
      "# writing shards/train-372.tar 10000 0.0 GB 3720000\n",
      "# writing shards/train-373.tar 10000 0.0 GB 3730000\n",
      "# writing shards/train-374.tar 10000 0.0 GB 3740000\n",
      "# writing shards/train-375.tar 10000 0.0 GB 3750000\n",
      "# writing shards/train-376.tar 10000 0.0 GB 3760000\n",
      "# writing shards/train-377.tar 10000 0.0 GB 3770000\n",
      "# writing shards/train-378.tar 10000 0.0 GB 3780000\n",
      "# writing shards/train-379.tar 10000 0.0 GB 3790000\n",
      "3800000 written\n",
      "# writing shards/train-380.tar 10000 0.0 GB 3800000\n",
      "# writing shards/train-381.tar 10000 0.0 GB 3810000\n",
      "# writing shards/train-382.tar 10000 0.0 GB 3820000\n",
      "# writing shards/train-383.tar 10000 0.0 GB 3830000\n",
      "# writing shards/train-384.tar 10000 0.0 GB 3840000\n",
      "# writing shards/train-385.tar 10000 0.0 GB 3850000\n",
      "# writing shards/train-386.tar 10000 0.0 GB 3860000\n",
      "# writing shards/train-387.tar 10000 0.0 GB 3870000\n",
      "# writing shards/train-388.tar 10000 0.0 GB 3880000\n",
      "# writing shards/train-389.tar 10000 0.0 GB 3890000\n",
      "3900000 written\n",
      "# writing shards/train-390.tar 10000 0.0 GB 3900000\n",
      "# writing shards/train-391.tar 10000 0.0 GB 3910000\n",
      "# writing shards/train-392.tar 10000 0.0 GB 3920000\n",
      "# writing shards/train-393.tar 10000 0.0 GB 3930000\n",
      "# writing shards/train-394.tar 10000 0.0 GB 3940000\n",
      "# writing shards/train-395.tar 10000 0.0 GB 3950000\n",
      "# writing shards/train-396.tar 10000 0.0 GB 3960000\n",
      "# writing shards/train-397.tar 10000 0.0 GB 3970000\n",
      "# writing shards/train-398.tar 10000 0.0 GB 3980000\n",
      "# writing shards/train-399.tar 10000 0.0 GB 3990000\n",
      "4000000 written\n",
      "# writing shards/train-400.tar 10000 0.0 GB 4000000\n",
      "# writing shards/train-401.tar 10000 0.0 GB 4010000\n",
      "# writing shards/train-402.tar 10000 0.0 GB 4020000\n",
      "# writing shards/train-403.tar 10000 0.0 GB 4030000\n",
      "# writing shards/train-404.tar 10000 0.0 GB 4040000\n",
      "# writing shards/train-405.tar 10000 0.0 GB 4050000\n",
      "# writing shards/train-406.tar 10000 0.0 GB 4060000\n",
      "# writing shards/train-407.tar 10000 0.0 GB 4070000\n",
      "# writing shards/train-408.tar 10000 0.0 GB 4080000\n",
      "# writing shards/train-409.tar 10000 0.0 GB 4090000\n",
      "4100000 written\n",
      "# writing shards/train-410.tar 10000 0.0 GB 4100000\n",
      "# writing shards/train-411.tar 10000 0.0 GB 4110000\n",
      "# writing shards/train-412.tar 10000 0.0 GB 4120000\n",
      "# writing shards/train-413.tar 10000 0.0 GB 4130000\n",
      "# writing shards/train-414.tar 10000 0.0 GB 4140000\n",
      "# writing shards/train-415.tar 10000 0.0 GB 4150000\n",
      "# writing shards/train-416.tar 10000 0.0 GB 4160000\n",
      "# writing shards/train-417.tar 10000 0.0 GB 4170000\n",
      "# writing shards/train-418.tar 10000 0.0 GB 4180000\n",
      "# writing shards/train-419.tar 10000 0.0 GB 4190000\n",
      "4200000 written\n",
      "# writing shards/train-420.tar 10000 0.0 GB 4200000\n",
      "# writing shards/train-421.tar 10000 0.0 GB 4210000\n",
      "# writing shards/train-422.tar 10000 0.0 GB 4220000\n",
      "# writing shards/train-423.tar 10000 0.0 GB 4230000\n",
      "# writing shards/train-424.tar 10000 0.0 GB 4240000\n",
      "# writing shards/train-425.tar 10000 0.0 GB 4250000\n",
      "# writing shards/train-426.tar 10000 0.0 GB 4260000\n",
      "# writing shards/train-427.tar 10000 0.0 GB 4270000\n",
      "# writing shards/train-428.tar 10000 0.0 GB 4280000\n",
      "# writing shards/train-429.tar 10000 0.0 GB 4290000\n",
      "4300000 written\n",
      "# writing shards/train-430.tar 10000 0.0 GB 4300000\n",
      "# writing shards/train-431.tar 10000 0.0 GB 4310000\n",
      "# writing shards/train-432.tar 10000 0.0 GB 4320000\n",
      "# writing shards/train-433.tar 10000 0.0 GB 4330000\n",
      "# writing shards/train-434.tar 10000 0.0 GB 4340000\n",
      "# writing shards/train-435.tar 10000 0.0 GB 4350000\n",
      "# writing shards/train-436.tar 10000 0.0 GB 4360000\n",
      "# writing shards/train-437.tar 10000 0.0 GB 4370000\n",
      "# writing shards/train-438.tar 10000 0.0 GB 4380000\n",
      "# writing shards/train-439.tar 10000 0.0 GB 4390000\n",
      "4400000 written\n",
      "# writing shards/train-440.tar 10000 0.0 GB 4400000\n",
      "# writing shards/train-441.tar 10000 0.0 GB 4410000\n",
      "# writing shards/train-442.tar 10000 0.0 GB 4420000\n",
      "# writing shards/train-443.tar 10000 0.0 GB 4430000\n",
      "# writing shards/train-444.tar 10000 0.0 GB 4440000\n",
      "# writing shards/train-445.tar 10000 0.0 GB 4450000\n",
      "# writing shards/train-446.tar 10000 0.0 GB 4460000\n",
      "# writing shards/train-447.tar 10000 0.0 GB 4470000\n",
      "# writing shards/train-448.tar 10000 0.0 GB 4480000\n",
      "# writing shards/train-449.tar 10000 0.0 GB 4490000\n",
      "4500000 written\n",
      "# writing shards/train-450.tar 10000 0.0 GB 4500000\n",
      "# writing shards/train-451.tar 10000 0.0 GB 4510000\n",
      "# writing shards/train-452.tar 10000 0.0 GB 4520000\n",
      "# writing shards/train-453.tar 10000 0.0 GB 4530000\n",
      "# writing shards/train-454.tar 10000 0.0 GB 4540000\n",
      "# writing shards/train-455.tar 10000 0.0 GB 4550000\n",
      "# writing shards/train-456.tar 10000 0.0 GB 4560000\n",
      "# writing shards/train-457.tar 10000 0.0 GB 4570000\n",
      "# writing shards/train-458.tar 10000 0.0 GB 4580000\n",
      "# writing shards/train-459.tar 10000 0.0 GB 4590000\n",
      "4600000 written\n",
      "# writing shards/train-460.tar 10000 0.0 GB 4600000\n",
      "# writing shards/train-461.tar 10000 0.0 GB 4610000\n",
      "# writing shards/train-462.tar 10000 0.0 GB 4620000\n",
      "# writing shards/train-463.tar 10000 0.0 GB 4630000\n",
      "# writing shards/train-464.tar 10000 0.0 GB 4640000\n",
      "# writing shards/train-465.tar 10000 0.0 GB 4650000\n",
      "# writing shards/train-466.tar 10000 0.0 GB 4660000\n",
      "# writing shards/train-467.tar 10000 0.0 GB 4670000\n",
      "# writing shards/train-468.tar 10000 0.0 GB 4680000\n",
      "# writing shards/train-469.tar 10000 0.0 GB 4690000\n",
      "4700000 written\n",
      "# writing shards/train-470.tar 10000 0.0 GB 4700000\n",
      "# writing shards/train-471.tar 10000 0.0 GB 4710000\n",
      "# writing shards/train-472.tar 10000 0.0 GB 4720000\n",
      "# writing shards/train-473.tar 10000 0.0 GB 4730000\n",
      "# writing shards/train-474.tar 10000 0.0 GB 4740000\n",
      "# writing shards/train-475.tar 10000 0.0 GB 4750000\n",
      "# writing shards/train-476.tar 10000 0.0 GB 4760000\n",
      "# writing shards/train-477.tar 10000 0.0 GB 4770000\n",
      "# writing shards/train-478.tar 10000 0.0 GB 4780000\n",
      "# writing shards/train-479.tar 10000 0.0 GB 4790000\n",
      "4800000 written\n",
      "# writing shards/train-480.tar 10000 0.0 GB 4800000\n",
      "# writing shards/train-481.tar 10000 0.0 GB 4810000\n",
      "# writing shards/train-482.tar 10000 0.0 GB 4820000\n",
      "# writing shards/train-483.tar 10000 0.0 GB 4830000\n",
      "# writing shards/train-484.tar 10000 0.0 GB 4840000\n",
      "# writing shards/train-485.tar 10000 0.0 GB 4850000\n",
      "# writing shards/train-486.tar 10000 0.0 GB 4860000\n",
      "# writing shards/train-487.tar 10000 0.0 GB 4870000\n",
      "# writing shards/train-488.tar 10000 0.0 GB 4880000\n",
      "# writing shards/train-489.tar 10000 0.0 GB 4890000\n",
      "4900000 written\n",
      "# writing shards/train-490.tar 10000 0.0 GB 4900000\n",
      "# writing shards/train-491.tar 10000 0.0 GB 4910000\n",
      "# writing shards/train-492.tar 10000 0.0 GB 4920000\n",
      "# writing shards/train-493.tar 10000 0.0 GB 4930000\n",
      "# writing shards/train-494.tar 10000 0.0 GB 4940000\n",
      "# writing shards/train-495.tar 10000 0.0 GB 4950000\n",
      "# writing shards/train-496.tar 10000 0.0 GB 4960000\n",
      "# writing shards/train-497.tar 10000 0.0 GB 4970000\n",
      "# writing shards/train-498.tar 10000 0.0 GB 4980000\n",
      "# writing shards/train-499.tar 10000 0.0 GB 4990000\n",
      "5000000 written\n",
      "# writing shards/train-500.tar 10000 0.0 GB 5000000\n",
      "# writing shards/train-501.tar 10000 0.0 GB 5010000\n",
      "# writing shards/train-502.tar 10000 0.0 GB 5020000\n",
      "# writing shards/train-503.tar 10000 0.0 GB 5030000\n",
      "# writing shards/train-504.tar 10000 0.0 GB 5040000\n",
      "# writing shards/train-505.tar 10000 0.0 GB 5050000\n",
      "# writing shards/train-506.tar 10000 0.0 GB 5060000\n",
      "# writing shards/train-507.tar 10000 0.0 GB 5070000\n",
      "# writing shards/train-508.tar 10000 0.0 GB 5080000\n",
      "# writing shards/train-509.tar 10000 0.0 GB 5090000\n",
      "5100000 written\n",
      "# writing shards/train-510.tar 10000 0.0 GB 5100000\n",
      "# writing shards/train-511.tar 10000 0.0 GB 5110000\n",
      "# writing shards/train-512.tar 10000 0.0 GB 5120000\n",
      "# writing shards/train-513.tar 10000 0.0 GB 5130000\n",
      "# writing shards/train-514.tar 10000 0.0 GB 5140000\n",
      "# writing shards/train-515.tar 10000 0.0 GB 5150000\n",
      "# writing shards/train-516.tar 10000 0.0 GB 5160000\n",
      "# writing shards/train-517.tar 10000 0.0 GB 5170000\n",
      "# writing shards/train-518.tar 10000 0.0 GB 5180000\n",
      "# writing shards/train-519.tar 10000 0.0 GB 5190000\n",
      "5200000 written\n",
      "# writing shards/train-520.tar 10000 0.0 GB 5200000\n",
      "# writing shards/train-521.tar 10000 0.0 GB 5210000\n",
      "# writing shards/train-522.tar 10000 0.0 GB 5220000\n",
      "# writing shards/train-523.tar 10000 0.0 GB 5230000\n",
      "# writing shards/train-524.tar 10000 0.0 GB 5240000\n",
      "# writing shards/train-525.tar 10000 0.0 GB 5250000\n",
      "# writing shards/train-526.tar 10000 0.0 GB 5260000\n",
      "# writing shards/train-527.tar 10000 0.0 GB 5270000\n",
      "# writing shards/train-528.tar 10000 0.0 GB 5280000\n",
      "# writing shards/train-529.tar 10000 0.0 GB 5290000\n",
      "5300000 written\n",
      "# writing shards/train-530.tar 10000 0.0 GB 5300000\n",
      "# writing shards/train-531.tar 10000 0.0 GB 5310000\n",
      "# writing shards/train-532.tar 10000 0.0 GB 5320000\n",
      "# writing shards/train-533.tar 10000 0.0 GB 5330000\n",
      "# writing shards/train-534.tar 10000 0.0 GB 5340000\n",
      "# writing shards/train-535.tar 10000 0.0 GB 5350000\n",
      "# writing shards/train-536.tar 10000 0.0 GB 5360000\n",
      "# writing shards/train-537.tar 10000 0.0 GB 5370000\n",
      "# writing shards/train-538.tar 10000 0.0 GB 5380000\n",
      "# writing shards/train-539.tar 10000 0.0 GB 5390000\n"
     ]
    }
   ],
   "source": [
    "writeWDS(dataset_train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing shards/val-00.tar 0 0.0 GB 0\n",
      "# writing shards/val-01.tar 10000 0.0 GB 10000\n",
      "# writing shards/val-02.tar 10000 0.0 GB 20000\n",
      "# writing shards/val-03.tar 10000 0.0 GB 30000\n",
      "# writing shards/val-04.tar 10000 0.0 GB 40000\n",
      "# writing shards/val-05.tar 10000 0.0 GB 50000\n",
      "# writing shards/val-06.tar 10000 0.0 GB 60000\n",
      "# writing shards/val-07.tar 10000 0.0 GB 70000\n",
      "# writing shards/val-08.tar 10000 0.0 GB 80000\n",
      "# writing shards/val-09.tar 10000 0.0 GB 90000\n",
      "100000 written\n",
      "# writing shards/val-10.tar 10000 0.0 GB 100000\n",
      "# writing shards/val-11.tar 10000 0.0 GB 110000\n",
      "# writing shards/val-12.tar 10000 0.0 GB 120000\n",
      "# writing shards/val-13.tar 10000 0.0 GB 130000\n",
      "# writing shards/val-14.tar 10000 0.0 GB 140000\n",
      "# writing shards/val-15.tar 10000 0.0 GB 150000\n",
      "# writing shards/val-16.tar 10000 0.0 GB 160000\n",
      "# writing shards/val-17.tar 10000 0.0 GB 170000\n",
      "# writing shards/val-18.tar 10000 0.0 GB 180000\n",
      "# writing shards/val-19.tar 10000 0.0 GB 190000\n",
      "200000 written\n",
      "# writing shards/val-20.tar 10000 0.0 GB 200000\n",
      "# writing shards/val-21.tar 10000 0.0 GB 210000\n",
      "# writing shards/val-22.tar 10000 0.0 GB 220000\n",
      "# writing shards/val-23.tar 10000 0.0 GB 230000\n",
      "# writing shards/val-24.tar 10000 0.0 GB 240000\n",
      "# writing shards/val-25.tar 10000 0.0 GB 250000\n",
      "# writing shards/val-26.tar 10000 0.0 GB 260000\n",
      "# writing shards/val-27.tar 10000 0.0 GB 270000\n",
      "# writing shards/val-28.tar 10000 0.0 GB 280000\n",
      "# writing shards/val-29.tar 10000 0.0 GB 290000\n",
      "300000 written\n",
      "# writing shards/val-30.tar 10000 0.0 GB 300000\n",
      "# writing shards/val-31.tar 10000 0.0 GB 310000\n",
      "# writing shards/val-32.tar 10000 0.0 GB 320000\n",
      "# writing shards/val-33.tar 10000 0.0 GB 330000\n",
      "# writing shards/val-34.tar 10000 0.0 GB 340000\n",
      "# writing shards/val-35.tar 10000 0.0 GB 350000\n",
      "# writing shards/val-36.tar 10000 0.0 GB 360000\n",
      "# writing shards/val-37.tar 10000 0.0 GB 370000\n",
      "# writing shards/val-38.tar 10000 0.0 GB 380000\n",
      "# writing shards/val-39.tar 10000 0.0 GB 390000\n",
      "400000 written\n",
      "# writing shards/val-40.tar 10000 0.0 GB 400000\n",
      "# writing shards/val-41.tar 10000 0.0 GB 410000\n",
      "# writing shards/val-42.tar 10000 0.0 GB 420000\n",
      "# writing shards/val-43.tar 10000 0.0 GB 430000\n",
      "# writing shards/val-44.tar 10000 0.0 GB 440000\n",
      "# writing shards/val-45.tar 10000 0.0 GB 450000\n",
      "# writing shards/val-46.tar 10000 0.0 GB 460000\n",
      "# writing shards/val-47.tar 10000 0.0 GB 470000\n",
      "# writing shards/val-48.tar 10000 0.0 GB 480000\n",
      "# writing shards/val-49.tar 10000 0.0 GB 490000\n",
      "500000 written\n",
      "# writing shards/val-50.tar 10000 0.0 GB 500000\n",
      "# writing shards/val-51.tar 10000 0.0 GB 510000\n",
      "# writing shards/val-52.tar 10000 0.0 GB 520000\n",
      "# writing shards/val-53.tar 10000 0.0 GB 530000\n",
      "# writing shards/val-54.tar 10000 0.0 GB 540000\n",
      "# writing shards/val-55.tar 10000 0.0 GB 550000\n",
      "# writing shards/val-56.tar 10000 0.0 GB 560000\n",
      "# writing shards/val-57.tar 10000 0.0 GB 570000\n",
      "# writing shards/val-58.tar 10000 0.0 GB 580000\n",
      "# writing shards/val-59.tar 10000 0.0 GB 590000\n",
      "600000 written\n",
      "# writing shards/val-60.tar 10000 0.0 GB 600000\n",
      "# writing shards/val-61.tar 10000 0.0 GB 610000\n",
      "# writing shards/val-62.tar 10000 0.0 GB 620000\n",
      "# writing shards/val-63.tar 10000 0.0 GB 630000\n",
      "# writing shards/val-64.tar 10000 0.0 GB 640000\n",
      "# writing shards/val-65.tar 10000 0.0 GB 650000\n",
      "# writing shards/val-66.tar 10000 0.0 GB 660000\n",
      "# writing shards/val-67.tar 10000 0.0 GB 670000\n"
     ]
    }
   ],
   "source": [
    "writeWDS(dataset_val, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing shards/test-00.tar 0 0.0 GB 0\n",
      "# writing shards/test-01.tar 10000 0.0 GB 10000\n",
      "# writing shards/test-02.tar 10000 0.0 GB 20000\n",
      "# writing shards/test-03.tar 10000 0.0 GB 30000\n",
      "# writing shards/test-04.tar 10000 0.0 GB 40000\n",
      "# writing shards/test-05.tar 10000 0.0 GB 50000\n",
      "# writing shards/test-06.tar 10000 0.0 GB 60000\n",
      "# writing shards/test-07.tar 10000 0.0 GB 70000\n",
      "# writing shards/test-08.tar 10000 0.0 GB 80000\n",
      "# writing shards/test-09.tar 10000 0.0 GB 90000\n",
      "100000 written\n",
      "# writing shards/test-10.tar 10000 0.0 GB 100000\n",
      "# writing shards/test-11.tar 10000 0.0 GB 110000\n",
      "# writing shards/test-12.tar 10000 0.0 GB 120000\n",
      "# writing shards/test-13.tar 10000 0.0 GB 130000\n",
      "# writing shards/test-14.tar 10000 0.0 GB 140000\n",
      "# writing shards/test-15.tar 10000 0.0 GB 150000\n",
      "# writing shards/test-16.tar 10000 0.0 GB 160000\n",
      "# writing shards/test-17.tar 10000 0.0 GB 170000\n",
      "# writing shards/test-18.tar 10000 0.0 GB 180000\n",
      "# writing shards/test-19.tar 10000 0.0 GB 190000\n",
      "200000 written\n",
      "# writing shards/test-20.tar 10000 0.0 GB 200000\n",
      "# writing shards/test-21.tar 10000 0.0 GB 210000\n",
      "# writing shards/test-22.tar 10000 0.0 GB 220000\n",
      "# writing shards/test-23.tar 10000 0.0 GB 230000\n",
      "# writing shards/test-24.tar 10000 0.0 GB 240000\n",
      "# writing shards/test-25.tar 10000 0.0 GB 250000\n",
      "# writing shards/test-26.tar 10000 0.0 GB 260000\n",
      "# writing shards/test-27.tar 10000 0.0 GB 270000\n",
      "# writing shards/test-28.tar 10000 0.0 GB 280000\n",
      "# writing shards/test-29.tar 10000 0.0 GB 290000\n",
      "300000 written\n",
      "# writing shards/test-30.tar 10000 0.0 GB 300000\n",
      "# writing shards/test-31.tar 10000 0.0 GB 310000\n",
      "# writing shards/test-32.tar 10000 0.0 GB 320000\n",
      "# writing shards/test-33.tar 10000 0.0 GB 330000\n",
      "# writing shards/test-34.tar 10000 0.0 GB 340000\n",
      "# writing shards/test-35.tar 10000 0.0 GB 350000\n",
      "# writing shards/test-36.tar 10000 0.0 GB 360000\n",
      "# writing shards/test-37.tar 10000 0.0 GB 370000\n",
      "# writing shards/test-38.tar 10000 0.0 GB 380000\n",
      "# writing shards/test-39.tar 10000 0.0 GB 390000\n",
      "400000 written\n",
      "# writing shards/test-40.tar 10000 0.0 GB 400000\n",
      "# writing shards/test-41.tar 10000 0.0 GB 410000\n",
      "# writing shards/test-42.tar 10000 0.0 GB 420000\n",
      "# writing shards/test-43.tar 10000 0.0 GB 430000\n",
      "# writing shards/test-44.tar 10000 0.0 GB 440000\n",
      "# writing shards/test-45.tar 10000 0.0 GB 450000\n",
      "# writing shards/test-46.tar 10000 0.0 GB 460000\n",
      "# writing shards/test-47.tar 10000 0.0 GB 470000\n",
      "# writing shards/test-48.tar 10000 0.0 GB 480000\n",
      "# writing shards/test-49.tar 10000 0.0 GB 490000\n",
      "500000 written\n",
      "# writing shards/test-50.tar 10000 0.0 GB 500000\n",
      "# writing shards/test-51.tar 10000 0.0 GB 510000\n",
      "# writing shards/test-52.tar 10000 0.0 GB 520000\n",
      "# writing shards/test-53.tar 10000 0.0 GB 530000\n",
      "# writing shards/test-54.tar 10000 0.0 GB 540000\n",
      "# writing shards/test-55.tar 10000 0.0 GB 550000\n",
      "# writing shards/test-56.tar 10000 0.0 GB 560000\n",
      "# writing shards/test-57.tar 10000 0.0 GB 570000\n",
      "# writing shards/test-58.tar 10000 0.0 GB 580000\n",
      "# writing shards/test-59.tar 10000 0.0 GB 590000\n",
      "600000 written\n",
      "# writing shards/test-60.tar 10000 0.0 GB 600000\n",
      "# writing shards/test-61.tar 10000 0.0 GB 610000\n",
      "# writing shards/test-62.tar 10000 0.0 GB 620000\n",
      "# writing shards/test-63.tar 10000 0.0 GB 630000\n",
      "# writing shards/test-64.tar 10000 0.0 GB 640000\n",
      "# writing shards/test-65.tar 10000 0.0 GB 650000\n",
      "# writing shards/test-66.tar 10000 0.0 GB 660000\n",
      "# writing shards/test-67.tar 10000 0.0 GB 670000\n"
     ]
    }
   ],
   "source": [
    "writeWDS(dataset_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.read_table(\"./data/test_sequences.txt\", header=None, names=['seq', 'target'], dtype={'seq':str, 'target':np.float32})\n",
    "dataset_predict = DreamChallengeDataset(df_predict, transform=DNA2OneHot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing shards/pred-00.tar 0 0.0 GB 0\n",
      "# writing shards/pred-01.tar 10000 0.0 GB 10000\n",
      "# writing shards/pred-02.tar 10000 0.0 GB 20000\n",
      "# writing shards/pred-03.tar 10000 0.0 GB 30000\n",
      "# writing shards/pred-04.tar 10000 0.0 GB 40000\n",
      "# writing shards/pred-05.tar 10000 0.0 GB 50000\n",
      "# writing shards/pred-06.tar 10000 0.0 GB 60000\n",
      "# writing shards/pred-07.tar 10000 0.0 GB 70000\n"
     ]
    }
   ],
   "source": [
    "writeWDS(dataset_predict, \"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataloader.py\n",
    "\n",
    "# lightning data module\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import cli as pl_cli\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "class DNA2OneHot(object):\n",
    "    def __init__(self, expect_len=110, rev=False):\n",
    "        self.expect_len = expect_len\n",
    "        self.rev = rev\n",
    "        self.DNA2Index = {\n",
    "            \"A\": 0,\n",
    "            \"C\": 1,\n",
    "            \"G\": 2,\n",
    "            \"T\": 3\n",
    "        }\n",
    "    \n",
    "    def __call__(self, dnaSeq):\n",
    "        # initialize the matrix as 4 x self.expect_len\n",
    "        seqMatrixs = np.zeros((4, self.expect_len), dtype=np.float32)\n",
    "        # change the value to matrix\n",
    "        seqLen = len(dnaSeq)\n",
    "        dnaSeq = self.rev_comp(dnaSeq.upper()) if self.rev else dnaSeq.upper()\n",
    "        for j in range(0, min(seqLen, self.expect_len)):\n",
    "            try:\n",
    "                seqMatrixs[self.DNA2Index[dnaSeq[j]], j] = 1\n",
    "            except KeyError as e:\n",
    "                continue\n",
    "        return seqMatrixs\n",
    "\n",
    "class DreamChallengeDataset(Dataset):\n",
    "    def __init__(self, df=None, transform=None, target_transform=None):\n",
    "        self.seq = df.seq\n",
    "        self.target = df.target\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.seq)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seq.iloc[idx]\n",
    "        target = self.target.iloc[idx][np.newaxis]\n",
    "        if self.transform:\n",
    "            seq = self.transform(seq)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return seq, target\n",
    "\n",
    "    def rev_comp(self, inp_str):\n",
    "        rc_dict = {'A': 'T', 'G': 'C', 'T': 'A', 'C': 'G', 'c': 'g',\n",
    "                   'g': 'c', 't': 'a', 'a': 't', 'n': 'n', 'N': 'N'}\n",
    "        outp_str = list()\n",
    "        for nucl in inp_str:\n",
    "            outp_str.append(rc_dict[nucl])\n",
    "        return ''.join(outp_str)[::-1]    \n",
    "\n",
    "def npy_decoder(key, value):\n",
    "    if not key.endswith(\".npy\"):\n",
    "        return None\n",
    "    assert isinstance(value, bytes)\n",
    "    return np.load(io.BytesIO(value))\n",
    "\n",
    "def worker_splitter(urls):\n",
    "    urls = [url for url in urls]\n",
    "    assert isinstance(urls, list)\n",
    "\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    if worker_info is not None:\n",
    "        wid = worker_info.id\n",
    "        num_workers = worker_info.num_workers\n",
    "\n",
    "        return iter(urls[wid::num_workers])\n",
    "    else:\n",
    "        return iter(urls)\n",
    "\n",
    "def node_splitter(urls):\n",
    "    urls = [url for url in urls]\n",
    "\n",
    "    rank = xm.get_ordinal()\n",
    "    num_replicas = xm.xrt_world_size()\n",
    "\n",
    "    urls_this = urls[rank::num_replicas]\n",
    "\n",
    "    return iter(urls_this)\n",
    "\n",
    "@pl_cli.DATAMODULE_REGISTRY\n",
    "class DreamChallengeDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str=\"./shards/\", train_epochs=1, batch_size=512, num_workers=6):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = DNA2OneHot()\n",
    "        self.train_epochs = train_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.description = {\"seq\": \"float\", \"target\": \"float\"}\n",
    "\n",
    "        self.dataset_train = None\n",
    "        self.dataset_val = None\n",
    "        self.dataset_test = None\n",
    "        self.dataset_predict = None\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # Assume data already in self.data_dir\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "\n",
    "            self.dataset_train = wds.DataPipeline(\n",
    "                wds.SimpleShardList(\"shards/train-{00..539}.tar\"),\n",
    "                node_splitter,\n",
    "                worker_splitter,\n",
    "                wds.tarfile_to_samples(),\n",
    "                wds.decode(npy_decoder),\n",
    "                wds.to_tuple(\"seq.npy\", \"target.npy\"),\n",
    "                wds.batched(self.batch_size, partial=False)\n",
    "                ).with_epoch(self.train_epochs)\n",
    "\n",
    "            self.dataset_val = wds.DataPipeline(\n",
    "                wds.SimpleShardList(\"shards/val-{00..67}.tar\"),\n",
    "                node_splitter,\n",
    "                worker_splitter,\n",
    "                wds.tarfile_to_samples(),\n",
    "                wds.decode(npy_decoder),\n",
    "                wds.to_tuple(\"seq.npy\", \"target.npy\"),\n",
    "                wds.batched(self.batch_size, partial=False)\n",
    "                ).with_epoch(1)\n",
    "            #.with_length(673926//num_data_instances)\n",
    "\n",
    "            self.dataset_test = wds.DataPipeline(\n",
    "                wds.SimpleShardList(\"shards/test-{00..67}.tar\"),\n",
    "                worker_splitter,\n",
    "                wds.tarfile_to_samples(),\n",
    "                wds.decode(npy_decoder),\n",
    "                wds.to_tuple(\"seq.npy\", \"target.npy\"),\n",
    "                wds.batched(self.batch_size, partial=False)\n",
    "                )\n",
    "            #.with_length(673926)\n",
    "\n",
    "            self.dataset_pred = wds.DataPipeline(\n",
    "                wds.SimpleShardList(\"shards/pred-{00-07}.tar\"),\n",
    "                wds.tarfile_to_samples(),\n",
    "                wds.decode(npy_decoder),\n",
    "                wds.to_tuple(\"seq.npy\", \"target.npy\"),\n",
    "                wds.batched(self.batch_size, partial=False)\n",
    "            )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_train, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_val, num_workers=1)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset_test, num_workers=1)\n",
    "    \n",
    "    def pred_dataloader(self):\n",
    "        return DataLoader(self.dataset_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = wds.DataPipeline(\n",
    "    wds.SimpleShardList(\"shards/train-{00..539}.tar\"),\n",
    "    wds.tarfile_to_samples(),\n",
    "    wds.decode(npy_decoder),\n",
    "    wds.to_tuple(\"seq.npy\", \"target.npy\"),\n",
    "    wds.batched(512, partial=False)\n",
    ").with_epoch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 1., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 1.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 1., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 1.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 1., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 1.]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 1., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 1.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 1., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 1.]],\n",
      "\n",
      "       [[0., 0., 0., ..., 0., 0., 0.],\n",
      "        [0., 0., 1., ..., 0., 0., 0.],\n",
      "        [0., 1., 0., ..., 1., 0., 0.],\n",
      "        [1., 0., 0., ..., 0., 1., 1.]]], dtype=float32), array([[12.270287 ],\n",
      "       [14.919623 ],\n",
      "       [ 8.       ],\n",
      "       [14.       ],\n",
      "       [11.135542 ],\n",
      "       [15.       ],\n",
      "       [ 9.       ],\n",
      "       [13.414012 ],\n",
      "       [11.77508  ],\n",
      "       [12.       ],\n",
      "       [ 8.89142  ],\n",
      "       [13.       ],\n",
      "       [ 9.649432 ],\n",
      "       [12.       ],\n",
      "       [12.       ],\n",
      "       [14.       ],\n",
      "       [15.       ],\n",
      "       [ 8.464342 ],\n",
      "       [16.       ],\n",
      "       [10.425069 ],\n",
      "       [10.775293 ],\n",
      "       [ 9.55657  ],\n",
      "       [ 6.2227187],\n",
      "       [ 8.375604 ],\n",
      "       [14.135064 ],\n",
      "       [11.       ],\n",
      "       [12.       ],\n",
      "       [10.       ],\n",
      "       [12.       ],\n",
      "       [10.       ],\n",
      "       [15.       ],\n",
      "       [11.146976 ],\n",
      "       [11.       ],\n",
      "       [11.       ],\n",
      "       [12.       ],\n",
      "       [11.747961 ],\n",
      "       [ 7.       ],\n",
      "       [10.       ],\n",
      "       [10.       ],\n",
      "       [ 9.437589 ],\n",
      "       [10.       ],\n",
      "       [15.       ],\n",
      "       [12.       ],\n",
      "       [12.       ],\n",
      "       [ 3.433108 ],\n",
      "       [16.       ],\n",
      "       [10.184998 ],\n",
      "       [11.       ],\n",
      "       [12.       ],\n",
      "       [13.       ],\n",
      "       [11.       ],\n",
      "       [13.758564 ],\n",
      "       [ 9.862306 ],\n",
      "       [13.       ],\n",
      "       [ 6.552605 ],\n",
      "       [11.       ],\n",
      "       [ 9.141672 ],\n",
      "       [16.       ],\n",
      "       [16.       ],\n",
      "       [15.       ],\n",
      "       [11.       ],\n",
      "       [11.126585 ],\n",
      "       [11.       ],\n",
      "       [ 9.       ],\n",
      "       [14.251629 ],\n",
      "       [16.88354  ],\n",
      "       [12.       ],\n",
      "       [13.       ],\n",
      "       [11.       ],\n",
      "       [ 9.       ],\n",
      "       [11.       ],\n",
      "       [ 8.       ],\n",
      "       [15.       ],\n",
      "       [ 9.842532 ],\n",
      "       [ 9.509753 ],\n",
      "       [15.       ],\n",
      "       [12.       ],\n",
      "       [12.       ],\n",
      "       [10.868469 ],\n",
      "       [12.       ],\n",
      "       [ 7.3036866],\n",
      "       [13.       ],\n",
      "       [ 9.483618 ],\n",
      "       [15.       ],\n",
      "       [10.847264 ],\n",
      "       [ 7.3325424],\n",
      "       [14.       ],\n",
      "       [11.       ],\n",
      "       [ 7.0459185],\n",
      "       [15.       ],\n",
      "       [11.       ],\n",
      "       [11.660388 ],\n",
      "       [10.       ],\n",
      "       [ 6.6451654],\n",
      "       [10.19656  ],\n",
      "       [11.       ],\n",
      "       [ 9.205845 ],\n",
      "       [11.       ],\n",
      "       [13.       ],\n",
      "       [12.310027 ],\n",
      "       [14.554664 ],\n",
      "       [11.927932 ],\n",
      "       [11.       ],\n",
      "       [14.700808 ],\n",
      "       [10.40774  ],\n",
      "       [11.       ],\n",
      "       [11.       ],\n",
      "       [ 9.       ],\n",
      "       [13.       ],\n",
      "       [14.       ],\n",
      "       [10.       ],\n",
      "       [11.94283  ],\n",
      "       [12.939216 ],\n",
      "       [13.271824 ],\n",
      "       [11.       ],\n",
      "       [10.696984 ],\n",
      "       [13.       ],\n",
      "       [10.079407 ],\n",
      "       [13.539746 ],\n",
      "       [10.463105 ],\n",
      "       [14.       ],\n",
      "       [10.159027 ],\n",
      "       [10.       ],\n",
      "       [12.       ],\n",
      "       [11.787042 ],\n",
      "       [11.       ],\n",
      "       [10.       ],\n",
      "       [12.       ],\n",
      "       [ 8.613191 ],\n",
      "       [10.       ],\n",
      "       [15.       ],\n",
      "       [ 6.759568 ],\n",
      "       [10.416747 ],\n",
      "       [16.       ],\n",
      "       [ 9.313382 ],\n",
      "       [10.       ],\n",
      "       [ 9.       ],\n",
      "       [ 9.       ],\n",
      "       [10.       ],\n",
      "       [ 9.248645 ],\n",
      "       [11.012881 ],\n",
      "       [14.365517 ],\n",
      "       [ 8.       ],\n",
      "       [13.       ],\n",
      "       [14.       ],\n",
      "       [12.141371 ],\n",
      "       [10.       ],\n",
      "       [12.844926 ],\n",
      "       [12.       ],\n",
      "       [ 8.375604 ],\n",
      "       [10.       ],\n",
      "       [10.       ],\n",
      "       [11.       ],\n",
      "       [ 7.329018 ],\n",
      "       [ 9.850053 ],\n",
      "       [15.       ],\n",
      "       [10.       ],\n",
      "       [ 8.       ],\n",
      "       [ 9.       ],\n",
      "       [11.       ],\n",
      "       [13.       ],\n",
      "       [10.589761 ],\n",
      "       [13.       ],\n",
      "       [11.       ],\n",
      "       [ 9.       ],\n",
      "       [12.673216 ],\n",
      "       [14.       ],\n",
      "       [12.       ],\n",
      "       [ 9.       ],\n",
      "       [13.       ],\n",
      "       [10.993867 ],\n",
      "       [11.       ],\n",
      "       [15.       ],\n",
      "       [10.223313 ],\n",
      "       [14.       ],\n",
      "       [11.       ],\n",
      "       [12.       ],\n",
      "       [ 9.       ],\n",
      "       [12.       ],\n",
      "       [ 6.       ],\n",
      "       [15.       ],\n",
      "       [ 9.       ],\n",
      "       [ 8.673464 ],\n",
      "       [12.       ],\n",
      "       [13.157848 ],\n",
      "       [ 9.       ],\n",
      "       [14.776665 ],\n",
      "       [ 9.040945 ],\n",
      "       [13.       ],\n",
      "       [ 7.       ],\n",
      "       [11.       ],\n",
      "       [12.327338 ],\n",
      "       [15.576355 ],\n",
      "       [ 3.056649 ],\n",
      "       [13.       ],\n",
      "       [ 9.       ],\n",
      "       [14.       ],\n",
      "       [13.       ],\n",
      "       [11.438356 ],\n",
      "       [12.       ],\n",
      "       [ 9.       ],\n",
      "       [12.       ],\n",
      "       [10.       ],\n",
      "       [14.       ],\n",
      "       [ 7.5270762],\n",
      "       [11.273286 ],\n",
      "       [14.431273 ],\n",
      "       [10.       ],\n",
      "       [14.466214 ],\n",
      "       [13.       ],\n",
      "       [11.829696 ],\n",
      "       [12.738906 ],\n",
      "       [ 9.978939 ],\n",
      "       [ 6.2250013],\n",
      "       [ 9.       ],\n",
      "       [14.75388  ],\n",
      "       [12.948695 ],\n",
      "       [ 9.       ],\n",
      "       [11.       ],\n",
      "       [12.       ],\n",
      "       [11.838016 ],\n",
      "       [10.463968 ],\n",
      "       [16.       ],\n",
      "       [13.       ],\n",
      "       [11.       ],\n",
      "       [10.       ],\n",
      "       [11.       ],\n",
      "       [10.       ],\n",
      "       [11.       ],\n",
      "       [14.868039 ],\n",
      "       [11.489191 ],\n",
      "       [11.       ],\n",
      "       [10.012526 ],\n",
      "       [11.634649 ],\n",
      "       [13.491133 ],\n",
      "       [11.       ],\n",
      "       [10.105923 ],\n",
      "       [ 8.       ],\n",
      "       [12.       ],\n",
      "       [16.892355 ],\n",
      "       [11.794219 ],\n",
      "       [11.799895 ],\n",
      "       [11.553756 ],\n",
      "       [11.       ],\n",
      "       [12.       ],\n",
      "       [14.883891 ],\n",
      "       [13.106638 ],\n",
      "       [11.       ],\n",
      "       [ 5.779997 ],\n",
      "       [10.352536 ],\n",
      "       [14.       ],\n",
      "       [ 9.125939 ],\n",
      "       [ 8.       ],\n",
      "       [13.97832  ],\n",
      "       [15.       ],\n",
      "       [14.       ],\n",
      "       [14.9065695],\n",
      "       [11.759351 ],\n",
      "       [10.       ],\n",
      "       [15.       ],\n",
      "       [ 9.       ],\n",
      "       [ 8.       ],\n",
      "       [ 5.962623 ],\n",
      "       [13.       ],\n",
      "       [13.       ],\n",
      "       [12.       ],\n",
      "       [12.774209 ],\n",
      "       [ 8.896645 ],\n",
      "       [11.       ],\n",
      "       [12.       ],\n",
      "       [13.       ],\n",
      "       [12.       ],\n",
      "       [14.345863 ],\n",
      "       [10.237358 ],\n",
      "       [12.       ],\n",
      "       [12.       ],\n",
      "       [15.472188 ],\n",
      "       [10.177388 ],\n",
      "       [13.       ],\n",
      "       [10.       ],\n",
      "       [10.885884 ],\n",
      "       [ 9.463666 ],\n",
      "       [15.       ],\n",
      "       [15.       ],\n",
      "       [11.311896 ],\n",
      "       [11.       ],\n",
      "       [13.589989 ],\n",
      "       [11.       ],\n",
      "       [10.525844 ],\n",
      "       [12.       ],\n",
      "       [13.       ],\n",
      "       [11.635378 ],\n",
      "       [13.       ],\n",
      "       [12.070891 ],\n",
      "       [13.       ],\n",
      "       [11.       ],\n",
      "       [14.207811 ],\n",
      "       [15.       ],\n",
      "       [ 7.       ],\n",
      "       [14.700808 ],\n",
      "       [13.600915 ],\n",
      "       [14.       ],\n",
      "       [14.       ],\n",
      "       [15.       ],\n",
      "       [10.35027  ],\n",
      "       [13.19868  ],\n",
      "       [11.829356 ],\n",
      "       [13.392143 ],\n",
      "       [10.721269 ],\n",
      "       [14.       ],\n",
      "       [ 8.121069 ],\n",
      "       [14.322027 ],\n",
      "       [13.       ],\n",
      "       [ 7.49137  ],\n",
      "       [12.       ],\n",
      "       [ 9.602052 ],\n",
      "       [14.       ],\n",
      "       [13.       ],\n",
      "       [ 9.       ],\n",
      "       [14.       ],\n",
      "       [10.       ],\n",
      "       [10.497371 ],\n",
      "       [ 8.1904125],\n",
      "       [11.       ],\n",
      "       [ 8.       ],\n",
      "       [ 9.754202 ],\n",
      "       [15.       ],\n",
      "       [15.       ],\n",
      "       [10.561126 ],\n",
      "       [11.064477 ],\n",
      "       [ 9.       ],\n",
      "       [12.       ],\n",
      "       [15.       ],\n",
      "       [10.651432 ],\n",
      "       [13.077485 ],\n",
      "       [10.       ],\n",
      "       [11.       ],\n",
      "       [14.       ],\n",
      "       [12.       ],\n",
      "       [10.507397 ],\n",
      "       [ 9.       ],\n",
      "       [11.       ],\n",
      "       [13.       ],\n",
      "       [12.       ],\n",
      "       [14.438447 ],\n",
      "       [15.       ],\n",
      "       [11.       ],\n",
      "       [ 8.       ],\n",
      "       [12.563329 ],\n",
      "       [10.463105 ],\n",
      "       [11.       ],\n",
      "       [ 8.609147 ],\n",
      "       [ 8.643449 ],\n",
      "       [13.       ],\n",
      "       [12.       ],\n",
      "       [11.       ],\n",
      "       [13.       ],\n",
      "       [11.       ],\n",
      "       [11.       ],\n",
      "       [13.292525 ],\n",
      "       [14.638329 ],\n",
      "       [11.       ],\n",
      "       [10.730457 ],\n",
      "       [13.       ],\n",
      "       [11.231801 ],\n",
      "       [11.       ],\n",
      "       [10.       ],\n",
      "       [13.       ],\n",
      "       [10.857913 ],\n",
      "       [10.219077 ],\n",
      "       [ 9.200631 ],\n",
      "       [11.826102 ],\n",
      "       [14.       ],\n",
      "       [12.648535 ],\n",
      "       [13.       ],\n",
      "       [ 9.032022 ],\n",
      "       [12.882172 ],\n",
      "       [11.       ],\n",
      "       [ 3.       ],\n",
      "       [ 9.256463 ],\n",
      "       [13.       ],\n",
      "       [13.400464 ],\n",
      "       [ 8.304342 ],\n",
      "       [11.514261 ],\n",
      "       [ 5.502232 ],\n",
      "       [14.       ],\n",
      "       [12.61056  ],\n",
      "       [14.       ],\n",
      "       [11.992271 ],\n",
      "       [10.463105 ],\n",
      "       [ 8.230643 ],\n",
      "       [10.441553 ],\n",
      "       [13.       ],\n",
      "       [12.678134 ],\n",
      "       [10.       ],\n",
      "       [13.       ],\n",
      "       [10.5475025],\n",
      "       [11.       ],\n",
      "       [16.742426 ],\n",
      "       [13.       ],\n",
      "       [10.32511  ],\n",
      "       [12.11874  ],\n",
      "       [ 2.068764 ],\n",
      "       [14.104993 ],\n",
      "       [10.75118  ],\n",
      "       [11.       ],\n",
      "       [ 5.0497828],\n",
      "       [ 5.689861 ],\n",
      "       [ 9.759552 ],\n",
      "       [14.       ],\n",
      "       [ 9.       ],\n",
      "       [ 8.       ],\n",
      "       [ 8.       ],\n",
      "       [10.       ],\n",
      "       [14.       ],\n",
      "       [13.       ],\n",
      "       [14.700808 ],\n",
      "       [14.       ],\n",
      "       [15.       ],\n",
      "       [10.026241 ],\n",
      "       [10.       ],\n",
      "       [10.703444 ],\n",
      "       [13.       ],\n",
      "       [ 5.599386 ],\n",
      "       [14.       ],\n",
      "       [ 9.832769 ],\n",
      "       [10.       ],\n",
      "       [10.168242 ],\n",
      "       [12.       ],\n",
      "       [ 8.209791 ],\n",
      "       [ 8.643449 ],\n",
      "       [14.       ],\n",
      "       [11.128682 ],\n",
      "       [14.       ],\n",
      "       [13.       ],\n",
      "       [ 7.8715196],\n",
      "       [10.       ],\n",
      "       [12.       ],\n",
      "       [12.       ],\n",
      "       [ 8.       ],\n",
      "       [10.938052 ],\n",
      "       [14.       ],\n",
      "       [ 9.206798 ],\n",
      "       [10.463105 ],\n",
      "       [12.278802 ],\n",
      "       [13.       ],\n",
      "       [11.       ],\n",
      "       [13.       ],\n",
      "       [ 6.2028136],\n",
      "       [15.       ],\n",
      "       [12.       ],\n",
      "       [14.       ],\n",
      "       [12.255336 ],\n",
      "       [14.787706 ],\n",
      "       [15.       ],\n",
      "       [ 4.       ],\n",
      "       [15.       ],\n",
      "       [11.       ],\n",
      "       [ 8.331279 ],\n",
      "       [11.973106 ],\n",
      "       [10.       ],\n",
      "       [14.875421 ],\n",
      "       [13.       ],\n",
      "       [15.       ],\n",
      "       [10.       ],\n",
      "       [13.391518 ],\n",
      "       [13.141715 ],\n",
      "       [11.       ],\n",
      "       [ 8.445081 ],\n",
      "       [ 7.115586 ],\n",
      "       [14.       ],\n",
      "       [10.931438 ],\n",
      "       [16.259954 ],\n",
      "       [ 9.694127 ],\n",
      "       [12.       ],\n",
      "       [10.536282 ],\n",
      "       [12.0201645],\n",
      "       [14.       ],\n",
      "       [14.       ],\n",
      "       [15.       ],\n",
      "       [14.       ],\n",
      "       [10.       ],\n",
      "       [12.       ],\n",
      "       [ 8.375604 ],\n",
      "       [15.       ],\n",
      "       [12.843945 ],\n",
      "       [10.       ],\n",
      "       [12.       ],\n",
      "       [13.       ],\n",
      "       [13.       ],\n",
      "       [14.       ],\n",
      "       [12.443646 ],\n",
      "       [13.       ],\n",
      "       [11.       ],\n",
      "       [13.       ],\n",
      "       [14.       ],\n",
      "       [14.       ],\n",
      "       [15.       ],\n",
      "       [14.985492 ],\n",
      "       [ 8.507754 ],\n",
      "       [12.       ],\n",
      "       [ 9.       ],\n",
      "       [10.       ],\n",
      "       [13.       ],\n",
      "       [ 9.       ],\n",
      "       [11.       ],\n",
      "       [ 9.672772 ],\n",
      "       [12.       ],\n",
      "       [10.095092 ],\n",
      "       [ 7.332988 ],\n",
      "       [14.       ],\n",
      "       [10.616324 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for i in ds: print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DreamChallengeDataModule(transform=DNA2OneHot())\n",
    "ds.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "ds_train = ds.train_dataloader()\n",
    "targets = []\n",
    "for seq, target in ds_train:\n",
    "    targets.append(target)\n",
    "targets = torch.cat(targets)\n",
    "# plot histogram on targets\n",
    "targets.numpy()\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = sns.histplot(x=targets, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# val\n",
    "ds_val = ds.val_dataloader()\n",
    "targets = []\n",
    "for seq, target in ds_val:\n",
    "    targets.append(target)\n",
    "targets = torch.cat(targets)\n",
    "# plot histogram on targets\n",
    "targets.numpy()\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = sns.histplot(x=targets, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "ds_test = ds.test_dataloader()\n",
    "targets = []\n",
    "for seq, target in ds_test:\n",
    "    targets.append(target)\n",
    "targets = torch.cat(targets)\n",
    "# plot histogram on targets\n",
    "targets.numpy()\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = sns.histplot(x=targets, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lightning_dreamchallenge.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lightning_dreamchallenge.py\n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning.utilities.cli import LightningCLI\n",
    "from pytorch_lightning.utilities import cli as pl_cli\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from dataloader import DreamChallengeDataModule\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, start_dim=1)\n",
    "\n",
    "class NNHooks(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seq, y = batch\n",
    "        y_hat = self(seq)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seq, y = batch\n",
    "        y_hat = self(seq)\n",
    "\n",
    "        # compute loss\n",
    "        val_loss = F.mse_loss(y_hat, y)\n",
    "        self.log('val_loss', val_loss)\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        seq, y = batch\n",
    "        y_hat = self(seq)\n",
    "\n",
    "        # compute loss\n",
    "        test_loss = F.mse_loss(y_hat, y)\n",
    "        self.log('test_loss', test_loss)\n",
    "        return {'test_loss': test_loss}\n",
    "\n",
    "@pl_cli.MODEL_REGISTRY\n",
    "class ConvolutionalModel(NNHooks):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_strand_specific_forward = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_strand_specific_reverse = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_body = nn.Sequential(OrderedDict([\n",
    "            ('conv3', nn.Conv1d(512, 256, 31, padding=15)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('conv4', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('faltten', Flatten()),\n",
    "            ('dense1', nn.Linear(110*256, 256)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2)),\n",
    "            ('dense2', nn.Linear(256, 256)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2))\n",
    "        ]))\n",
    "        self.model_head = nn.Sequential(OrderedDict([\n",
    "            ('dense_head', nn.Linear(256, 1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, seq):\n",
    "        seq_revcomp = torch.flip(seq.detach().clone(), [1, 2])\n",
    "        y_hat_for = self.model_strand_specific_forward(seq)\n",
    "        y_hat_rev = self.model_strand_specific_reverse(seq_revcomp)\n",
    "        y_hat = torch.cat([y_hat_for, y_hat_rev], dim=1)\n",
    "        y_hat = self.model_body(y_hat)\n",
    "        y_hat = self.model_head(y_hat)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "class MyLightningCLI(LightningCLI):\n",
    "    def add_arguments_to_parser(self, parser):\n",
    "        parser.add_optimizer_args(torch.optim.Adam)\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.trainer.test(ckpt_path='best')\n",
    "\n",
    "def main():\n",
    "    cli = MyLightningCLI(auto_registry=True, save_config_overwrite=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting native_pytorch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile native_pytorch.py\n",
    "\n",
    "import sys\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "\n",
    "from dataloader import DreamChallengeDataModule\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ConvolutionalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalModel, self).__init__()\n",
    "\n",
    "        self.model_strand_specific_forward = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_strand_specific_reverse = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_body = nn.Sequential(OrderedDict([\n",
    "            ('conv3', nn.Conv1d(512, 256, 31, padding=15)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('conv4', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('faltten', nn.Flatten()),\n",
    "            ('dense1', nn.Linear(110*256, 256)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2)),\n",
    "            ('dense2', nn.Linear(256, 256)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2))\n",
    "        ]))\n",
    "        self.model_head = nn.Sequential(OrderedDict([\n",
    "            ('dense_head', nn.Linear(256, 1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, seq):\n",
    "        seq_revcomp = torch.flip(seq.detach().clone(), [1, 2])\n",
    "        y_hat_for = self.model_strand_specific_forward(seq)\n",
    "        y_hat_rev = self.model_strand_specific_reverse(seq_revcomp)\n",
    "        y_hat = torch.cat([y_hat_for, y_hat_rev], dim=1)\n",
    "        y_hat = self.model_body(y_hat)\n",
    "        y_hat = self.model_head(y_hat)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    pbar = tqdm(enumerate(dataloader))\n",
    "    for batch, (X, y) in pbar:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        xm.mark_step()\n",
    "\n",
    "        pbar.set_description(f\"Training loss: {loss:>7f}  [{batch:>5d}/{num_batches:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    metric = torchmetrics.MeanSquaredError()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            mse = metric(pred, y)\n",
    "\n",
    "    mse = metric.compute()\n",
    "    print(f\"MSE on all data: {mse}\") \n",
    "\n",
    "def main():\n",
    "    # load config\n",
    "    config = yaml.safe_load(open(sys.argv[1], \"r\"))\n",
    "\n",
    "    device = xm.xla_device()\n",
    "    print(f\"XLA device: {device}\")\n",
    "\n",
    "    model = ConvolutionalModel()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"optimizer\"][\"lr\"])\n",
    "\n",
    "    datamodule = DreamChallengeDataModule(batch_size=config[\"data\"][\"init_args\"][\"batch_size\"], num_workers=config[\"data\"][\"init_args\"][\"num_workers\"])\n",
    "    datamodule.setup()\n",
    "\n",
    "    epochs = 3\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------\")\n",
    "        train_loop(datamodule.train_dataloader(), model.train(), loss_fn, optimizer, device)\n",
    "        test_loop(datamodule.val_dataloader(), model.eval(), loss_fn, device)\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPU DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting args_parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile args_parse.py\n",
    "\n",
    "# This module cannot import any other PyTorch/XLA module. Only Python core modules.\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def parse_common_options(datadir=None,\n",
    "                         logdir=None,\n",
    "                         num_cores=None,\n",
    "                         batch_size=128,\n",
    "                         num_epochs=10,\n",
    "                         num_workers=2,\n",
    "                         log_steps=20,\n",
    "                         lr=None,\n",
    "                         momentum=None,\n",
    "                         target_accuracy=None,\n",
    "                         profiler_port=9012,\n",
    "                         opts=None):\n",
    "  parser = argparse.ArgumentParser(add_help=True)\n",
    "  parser.add_argument('--datadir', type=str, default=datadir)\n",
    "  parser.add_argument('--logdir', type=str, default=logdir)\n",
    "  parser.add_argument('--num_cores', type=int, default=num_cores)\n",
    "  parser.add_argument('--batch_size', type=int, default=batch_size)\n",
    "  parser.add_argument('--num_epochs', type=int, default=num_epochs)\n",
    "  parser.add_argument('--num_workers', type=int, default=num_workers)\n",
    "  parser.add_argument('--log_steps', type=int, default=log_steps)\n",
    "  parser.add_argument('--profiler_port', type=int, default=profiler_port)\n",
    "  parser.add_argument('--lr', type=float, default=lr)\n",
    "  parser.add_argument('--momentum', type=float, default=momentum)\n",
    "  parser.add_argument('--target_accuracy', type=float, default=target_accuracy)\n",
    "  parser.add_argument('--drop_last', action='store_true')\n",
    "  parser.add_argument('--fake_data', action='store_true')\n",
    "  parser.add_argument('--tidy', action='store_true')\n",
    "  parser.add_argument('--metrics_debug', action='store_true')\n",
    "  parser.add_argument('--async_closures', action='store_true')\n",
    "  if opts:\n",
    "    for name, aopts in opts:\n",
    "      parser.add_argument(name, **aopts)\n",
    "  args, leftovers = parser.parse_known_args()\n",
    "  sys.argv = [sys.argv[0]] + leftovers\n",
    "  # Setup import folders.\n",
    "  xla_folder = os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))\n",
    "  sys.path.append(os.path.join(os.path.dirname(xla_folder), 'test'))\n",
    "  return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile models.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ConvolutionalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalModel, self).__init__()\n",
    "\n",
    "        self.model_strand_specific_forward = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_strand_specific_reverse = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_body = nn.Sequential(OrderedDict([\n",
    "            ('conv3', nn.Conv1d(512, 256, 31, padding=15)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('conv4', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('faltten', nn.Flatten()),\n",
    "            ('dense1', nn.Linear(110*256, 256)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2)),\n",
    "            ('dense2', nn.Linear(256, 256)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2))\n",
    "        ]))\n",
    "        self.model_head = nn.Sequential(OrderedDict([\n",
    "            ('dense_head', nn.Linear(256, 1))\n",
    "        ]))\n",
    "\n",
    "        self.example_input = torch.zeros(512, 4, 110).index_fill_(1, torch.tensor(2), 1)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        seq_revcomp = torch.flip(seq.detach().clone(), [1, 2])\n",
    "        y_hat_for = self.model_strand_specific_forward(seq)\n",
    "        y_hat_rev = self.model_strand_specific_reverse(seq_revcomp)\n",
    "        y_hat = torch.cat([y_hat_for, y_hat_rev], dim=1)\n",
    "        y_hat = self.model_body(y_hat)\n",
    "        y_hat = self.model_head(y_hat)\n",
    "\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3., 4.])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.arange(10).reshape(2,5))[0,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tpu_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tpu_ddp.py\n",
    "\n",
    "import args_parse\n",
    "\n",
    "FLAGS = args_parse.parse_common_options(\n",
    "    log_steps=200,\n",
    "    datadir='./shards/',\n",
    "    logdir='./conv_model/',\n",
    "    batch_size=512,\n",
    "    momentum=0.5,\n",
    "    lr=0.0001,\n",
    "    num_epochs=100,\n",
    "    num_workers=32,\n",
    "    opts=[('--patience', {'type': int, 'default': 5}),\n",
    "          ('--train_epochs', {'type': int, 'default': None})])\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_xla\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataloader import DreamChallengeDataModule\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "import models\n",
    "\n",
    "def _train_update(device, x, loss, tracker, writer):\n",
    "  loss_item = loss.item()\n",
    "  #test_utils.print_training_update(\n",
    "  #    device,\n",
    "  #    x\n",
    "  #    loss_item,\n",
    "  #    tracker.rate(),\n",
    "  #    tracker.global_rate(),\n",
    "  #    summary_writer=writer)\n",
    "  test_utils.write_to_summary(\n",
    "      writer,\n",
    "      dict_to_write={\n",
    "        \"loss\": loss_item\n",
    "      })\n",
    "\n",
    "def train_model(flags, **kwargs):\n",
    "  torch.manual_seed(1)\n",
    "\n",
    "  print(f\"Device {xm.get_ordinal()} in world size of {xm.xrt_world_size()}\")\n",
    "  num_data_instances = xm.xrt_world_size() * flags.num_workers\n",
    "  train_epochs = flags.train_epochs if flags.train_epochs is not None else 5391406//num_data_instances//flags.batch_size\n",
    "  datamodule = DreamChallengeDataModule(data_dir=flags.datadir, train_epochs=train_epochs, batch_size=flags.batch_size, num_workers=flags.num_workers)\n",
    "  datamodule.setup()\n",
    "  train_loader = datamodule.train_dataloader()\n",
    "  val_loader = datamodule.val_dataloader()\n",
    "\n",
    "  # Scale learning rate to num cores\n",
    "  lr = flags.lr * xm.xrt_world_size()\n",
    "\n",
    "  model = models.ConvolutionalModel()\n",
    "  writer = None\n",
    "  if xm.is_master_ordinal():\n",
    "    writer = test_utils.get_summary_writer(flags.logdir)\n",
    "    writer.add_graph(model, model.example_input)\n",
    "  device = xm.xla_device()\n",
    "  model = model.to(device)\n",
    "  #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=flags.momentum)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "  loss_fn = nn.MSELoss()\n",
    "\n",
    "  def train_loop_fn(epoch, loader):\n",
    "    tracker = xm.RateTracker()\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(loader))\n",
    "    for step, (data, target) in pbar:\n",
    "      data = torch.squeeze(data, 0)\n",
    "      target = torch.squeeze(target, 0)\n",
    "      optimizer.zero_grad()\n",
    "      output = model(data)\n",
    "      loss = loss_fn(output, target)\n",
    "      loss.backward()\n",
    "      #xm.optimizer_step(optimizer)\n",
    "      xm.reduce_gradients(optimizer)\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1, error_if_nonfinite=True)\n",
    "      optimizer.step()\n",
    "      tracker.add(flags.batch_size)\n",
    "      if xm.is_master_ordinal():\n",
    "        pbar.set_description(\"Loss %s\" % loss.item())\n",
    "        #if step % flags.log_steps == 0:\n",
    "        #  xm.add_step_closure(\n",
    "        #    _train_update,\n",
    "        #    args=(device, step, loss, tracker, writer),\n",
    "        #    run_async=flags.async_closures\n",
    "        #  )\n",
    "    if xm.is_master_ordinal():\n",
    "      torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.detach().cpu().numpy()\n",
    "      }, os.path.join(flags.logdir, f\"checkpoint-epoch{epoch}.ckpt\"))\n",
    "      \n",
    "  def val_loop_fn(loader):\n",
    "    model.eval()\n",
    "    metrics = MeanSquaredError()\n",
    "    pbar = tqdm(enumerate(loader))\n",
    "    for step, (data, target) in pbar:\n",
    "      data = torch.squeeze(data, 0)\n",
    "      target = torch.squeeze(target, 0)\n",
    "      pred = model(data)\n",
    "      metrics.update(pred, target)\n",
    "      if xm.is_master_ordinal(): \n",
    "        if step == 0:\n",
    "          print(torch.stack([target, pred], 1)[:10])\n",
    "\n",
    "    mse = metrics.compute()  \n",
    "    mse = xm.mesh_reduce('val_mse', mse, np.mean)\n",
    "    return mse\n",
    "\n",
    "  train_device_loader = pl.MpDeviceLoader(train_loader, device)\n",
    "  val_device_loader = pl.MpDeviceLoader(val_loader, device)\n",
    "  # Early stopping\n",
    "  mse, min_mse = 1e3, 1e3\n",
    "  best_epoch = -1\n",
    "  patience = FLAGS.patience\n",
    "  trigger_times = 0\n",
    "  for epoch in range(1, flags.num_epochs + 1):\n",
    "    xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n",
    "    train_loop_fn(epoch, train_device_loader)\n",
    "    xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))\n",
    "\n",
    "    mse = val_loop_fn(val_device_loader)\n",
    "    xm.master_print('Epoch {} end {}, Val MSE={:.2f}'.format(\n",
    "        epoch, test_utils.now(), mse))\n",
    "    test_utils.write_to_summary(\n",
    "        writer,\n",
    "        epoch,\n",
    "        dict_to_write={'MSE/val': mse},\n",
    "        write_xla_metrics=True)\n",
    "    if flags.metrics_debug:\n",
    "      xm.master_print(met.metrics_report())\n",
    "\n",
    "    # Early stopping\n",
    "    if mse > min_mse:\n",
    "        trigger_times += 1\n",
    "        #print('Trigger Times:', trigger_times)\n",
    "        if trigger_times >= patience:\n",
    "            print('Early stopping!\\nStart to test process.')\n",
    "            break\n",
    "    else:\n",
    "        #print('trigger times: 0')\n",
    "        trigger_times = 0\n",
    "\n",
    "        min_mse = mse\n",
    "        best_epoch = epoch\n",
    "\n",
    "  test_utils.close_summary_writer(writer)\n",
    "  xm.master_print(f'Min MSE: {min_mse:.2f} from Epoch {best_epoch}')\n",
    "  if xm.is_master_ordinal(): subprocess.run(['cp', os.path.join(flags.logdir, f\"checkpoint-epoch{best_epoch}.ckpt\"), os.path.join(flags.logdir, f\"checkpoint-epoch{best_epoch}-best.ckpt\")], shell=True)\n",
    "\n",
    "  return min_mse, best_epoch\n",
    "  \n",
    "def _mp_fn(index, flags):\n",
    "  torch.set_default_tensor_type('torch.FloatTensor')\n",
    "  mse, best_epoch = train_model(flags)\n",
    "  if flags.tidy and os.path.isdir(flags.datadir):\n",
    "    shutil.rmtree(flags.datadir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS.num_cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def npy_decoder(key, value):\n",
    "    if not key.endswith(\".npy\"):\n",
    "        return None\n",
    "    assert isinstance(value, bytes)\n",
    "    return np.load(io.BytesIO(value))\n",
    "\n",
    "dataset_train = wds.DataPipeline(\n",
    "    wds.ResampledShards(\"shards/train-{00..539}.tar\"),\n",
    "    #node_splitter,\n",
    "    #worker_splitter,\n",
    "    wds.tarfile_to_samples(),\n",
    "    wds.decode(npy_decoder),\n",
    "    wds.to_tuple(\"seq.npy\", \"target.npy\"),\n",
    "    wds.batched(512, partial=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq, target in dataset_train: break\n",
    "seq = torch.Tensor(seq)\n",
    "target = torch.Tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models\n",
    "\n",
    "model = models.ConvolutionalModel()\n",
    "model = model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model.train()\n",
    "for i in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(seq)\n",
    "    loss = loss_fn(pred, target)\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5, error_if_nonfinite=True)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13.0000],\n",
       "        [ 9.0000],\n",
       "        [14.0000],\n",
       "        [14.0000],\n",
       "        [10.3339],\n",
       "        [13.3526],\n",
       "        [13.0000],\n",
       "        [ 8.3066],\n",
       "        [16.0000],\n",
       "        [13.6824],\n",
       "        [ 6.6399],\n",
       "        [ 9.3630],\n",
       "        [11.8379],\n",
       "        [14.3459],\n",
       "        [12.0000],\n",
       "        [10.8038],\n",
       "        [14.0000],\n",
       "        [ 9.0000],\n",
       "        [11.0000],\n",
       "        [10.7753],\n",
       "        [ 9.7211],\n",
       "        [11.0000],\n",
       "        [13.0000],\n",
       "        [ 8.0000],\n",
       "        [14.4526],\n",
       "        [13.0000],\n",
       "        [ 8.0000],\n",
       "        [ 9.0000],\n",
       "        [13.0000],\n",
       "        [13.0000],\n",
       "        [12.0000],\n",
       "        [ 9.9026],\n",
       "        [ 9.1308],\n",
       "        [14.0000],\n",
       "        [12.0000],\n",
       "        [15.0000],\n",
       "        [ 9.0000],\n",
       "        [14.0000],\n",
       "        [15.0000],\n",
       "        [ 9.8679],\n",
       "        [ 6.0000],\n",
       "        [13.0000],\n",
       "        [ 4.9915],\n",
       "        [ 3.0561],\n",
       "        [12.2280],\n",
       "        [14.4234],\n",
       "        [ 8.9939],\n",
       "        [11.0000],\n",
       "        [14.0000],\n",
       "        [ 8.8595],\n",
       "        [ 8.0000],\n",
       "        [ 7.7813],\n",
       "        [12.1728],\n",
       "        [ 8.3558],\n",
       "        [ 9.0000],\n",
       "        [10.0000],\n",
       "        [13.5370],\n",
       "        [16.0000],\n",
       "        [ 9.7029],\n",
       "        [11.0000],\n",
       "        [14.0000],\n",
       "        [14.0000],\n",
       "        [ 8.6515],\n",
       "        [10.0000],\n",
       "        [12.0000],\n",
       "        [ 7.9226],\n",
       "        [14.0000],\n",
       "        [12.8763],\n",
       "        [14.0000],\n",
       "        [11.1590],\n",
       "        [ 7.5531],\n",
       "        [12.6631],\n",
       "        [ 9.3933],\n",
       "        [ 9.0975],\n",
       "        [ 6.7394],\n",
       "        [ 9.8901],\n",
       "        [12.0000],\n",
       "        [13.0000],\n",
       "        [14.8528],\n",
       "        [11.0000],\n",
       "        [10.0000],\n",
       "        [ 8.2089],\n",
       "        [10.6330],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [13.0000],\n",
       "        [13.9666],\n",
       "        [10.3013],\n",
       "        [ 9.0000],\n",
       "        [ 5.9890],\n",
       "        [10.0061],\n",
       "        [10.0000],\n",
       "        [13.0000],\n",
       "        [10.1660],\n",
       "        [10.3013],\n",
       "        [12.0000],\n",
       "        [12.3752],\n",
       "        [10.0000],\n",
       "        [10.8191],\n",
       "        [ 9.2461],\n",
       "        [ 8.1347],\n",
       "        [13.0000],\n",
       "        [13.0000],\n",
       "        [ 9.9383],\n",
       "        [12.0000],\n",
       "        [13.2963],\n",
       "        [12.0000],\n",
       "        [ 7.5809],\n",
       "        [15.0000],\n",
       "        [14.0000],\n",
       "        [15.0000],\n",
       "        [ 9.0000],\n",
       "        [12.9114],\n",
       "        [11.0000],\n",
       "        [15.0000],\n",
       "        [10.2847],\n",
       "        [11.3345],\n",
       "        [13.0000],\n",
       "        [14.0000],\n",
       "        [11.5467],\n",
       "        [10.1695],\n",
       "        [ 8.9414],\n",
       "        [13.0000],\n",
       "        [12.0000],\n",
       "        [11.0000],\n",
       "        [12.0000],\n",
       "        [14.0000],\n",
       "        [ 9.0000],\n",
       "        [11.0000],\n",
       "        [10.9767],\n",
       "        [11.0000],\n",
       "        [ 8.2242],\n",
       "        [12.8238],\n",
       "        [11.0000],\n",
       "        [ 9.0000],\n",
       "        [14.0237],\n",
       "        [ 9.0000],\n",
       "        [ 8.8014],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [12.0000],\n",
       "        [ 7.1044],\n",
       "        [11.6588],\n",
       "        [13.0000],\n",
       "        [11.0000],\n",
       "        [10.0404],\n",
       "        [13.0000],\n",
       "        [13.3092],\n",
       "        [13.0000],\n",
       "        [12.3466],\n",
       "        [11.0000],\n",
       "        [14.0000],\n",
       "        [ 9.1445],\n",
       "        [13.0000],\n",
       "        [ 9.0000],\n",
       "        [14.0000],\n",
       "        [11.0000],\n",
       "        [ 8.9290],\n",
       "        [ 9.7578],\n",
       "        [13.0000],\n",
       "        [12.0000],\n",
       "        [12.0542],\n",
       "        [11.0000],\n",
       "        [ 9.0000],\n",
       "        [11.0000],\n",
       "        [13.9355],\n",
       "        [ 9.0000],\n",
       "        [11.0000],\n",
       "        [ 8.0000],\n",
       "        [ 9.7116],\n",
       "        [14.0000],\n",
       "        [12.0669],\n",
       "        [11.0000],\n",
       "        [14.3106],\n",
       "        [11.6801],\n",
       "        [ 8.0000],\n",
       "        [ 9.1307],\n",
       "        [11.4393],\n",
       "        [ 8.8157],\n",
       "        [10.3939],\n",
       "        [ 7.0000],\n",
       "        [11.0000],\n",
       "        [10.0000],\n",
       "        [13.0000],\n",
       "        [10.0329],\n",
       "        [ 8.8052],\n",
       "        [12.0000],\n",
       "        [14.0000],\n",
       "        [15.0000],\n",
       "        [10.6346],\n",
       "        [ 7.9565],\n",
       "        [10.0000],\n",
       "        [ 7.9562],\n",
       "        [10.0000],\n",
       "        [11.9585],\n",
       "        [ 9.0891],\n",
       "        [10.4707],\n",
       "        [11.0000],\n",
       "        [12.0000],\n",
       "        [14.0000],\n",
       "        [11.3402],\n",
       "        [13.8960],\n",
       "        [12.0000],\n",
       "        [13.0000],\n",
       "        [11.0000],\n",
       "        [ 8.1467],\n",
       "        [15.0000],\n",
       "        [12.0000],\n",
       "        [15.0000],\n",
       "        [11.0000],\n",
       "        [15.0000],\n",
       "        [11.2925],\n",
       "        [10.0000],\n",
       "        [ 8.5519],\n",
       "        [14.0000],\n",
       "        [10.8154],\n",
       "        [10.3108],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [16.9055],\n",
       "        [11.0000],\n",
       "        [ 9.0000],\n",
       "        [13.0000],\n",
       "        [11.0000],\n",
       "        [ 8.4906],\n",
       "        [ 9.0000],\n",
       "        [ 8.5893],\n",
       "        [11.0000],\n",
       "        [ 8.1467],\n",
       "        [13.0000],\n",
       "        [12.0000],\n",
       "        [ 8.4904],\n",
       "        [12.0000],\n",
       "        [ 7.0000],\n",
       "        [ 9.9352],\n",
       "        [15.0000],\n",
       "        [ 7.2940],\n",
       "        [ 9.6640],\n",
       "        [11.0000],\n",
       "        [16.0000],\n",
       "        [13.0000],\n",
       "        [11.0000],\n",
       "        [ 8.3109],\n",
       "        [10.0000],\n",
       "        [12.0000],\n",
       "        [ 9.2448],\n",
       "        [ 8.1940],\n",
       "        [ 6.2128],\n",
       "        [11.0000],\n",
       "        [14.0000],\n",
       "        [10.0000],\n",
       "        [13.0000],\n",
       "        [14.8910],\n",
       "        [11.7056],\n",
       "        [11.4606],\n",
       "        [12.0000],\n",
       "        [ 7.8540],\n",
       "        [10.5566],\n",
       "        [13.0000],\n",
       "        [10.0000],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [ 8.7427],\n",
       "        [ 8.0000],\n",
       "        [ 8.0000],\n",
       "        [10.0000],\n",
       "        [ 9.0000],\n",
       "        [ 2.0000],\n",
       "        [ 6.0835],\n",
       "        [11.0000],\n",
       "        [ 9.8229],\n",
       "        [14.0000],\n",
       "        [10.0000],\n",
       "        [11.8865],\n",
       "        [11.8699],\n",
       "        [11.0000],\n",
       "        [11.7095],\n",
       "        [ 8.0000],\n",
       "        [10.4895],\n",
       "        [13.0000],\n",
       "        [ 9.4353],\n",
       "        [ 8.0507],\n",
       "        [ 7.7577],\n",
       "        [13.2763],\n",
       "        [ 9.6121],\n",
       "        [10.6647],\n",
       "        [12.0000],\n",
       "        [ 9.0000],\n",
       "        [14.0000],\n",
       "        [13.4235],\n",
       "        [13.6339],\n",
       "        [ 3.0000],\n",
       "        [14.0000],\n",
       "        [ 9.7173],\n",
       "        [11.7069],\n",
       "        [11.0000],\n",
       "        [ 9.2756],\n",
       "        [ 8.0000],\n",
       "        [ 9.0000],\n",
       "        [11.0000],\n",
       "        [13.5064],\n",
       "        [11.6166],\n",
       "        [14.0000],\n",
       "        [11.0000],\n",
       "        [ 9.9306],\n",
       "        [14.5843],\n",
       "        [ 9.8325],\n",
       "        [13.0865],\n",
       "        [11.0000],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [ 9.8537],\n",
       "        [14.0000],\n",
       "        [16.0000],\n",
       "        [11.6640],\n",
       "        [14.0000],\n",
       "        [12.0000],\n",
       "        [ 7.7334],\n",
       "        [16.8891],\n",
       "        [12.0848],\n",
       "        [ 8.0328],\n",
       "        [11.0000],\n",
       "        [10.6374],\n",
       "        [ 7.4764],\n",
       "        [14.0000],\n",
       "        [10.6334],\n",
       "        [14.8590],\n",
       "        [11.1256],\n",
       "        [ 9.0000],\n",
       "        [ 8.0000],\n",
       "        [ 9.0000],\n",
       "        [12.0000],\n",
       "        [11.0000],\n",
       "        [10.4040],\n",
       "        [ 9.0000],\n",
       "        [13.0000],\n",
       "        [ 9.0000],\n",
       "        [10.3822],\n",
       "        [ 9.0000],\n",
       "        [14.0000],\n",
       "        [11.0000],\n",
       "        [ 9.2912],\n",
       "        [11.0000],\n",
       "        [13.0000],\n",
       "        [10.0000],\n",
       "        [ 8.0000],\n",
       "        [12.0000],\n",
       "        [13.0000],\n",
       "        [11.0000],\n",
       "        [ 8.0000],\n",
       "        [ 7.3661],\n",
       "        [14.0000],\n",
       "        [12.0000],\n",
       "        [13.0000],\n",
       "        [ 7.0000],\n",
       "        [12.1867],\n",
       "        [13.1769],\n",
       "        [13.6066],\n",
       "        [14.3162],\n",
       "        [11.0000],\n",
       "        [12.4714],\n",
       "        [10.0000],\n",
       "        [10.0000],\n",
       "        [11.0169],\n",
       "        [16.0000],\n",
       "        [14.0000],\n",
       "        [ 8.6842],\n",
       "        [12.0000],\n",
       "        [11.0000],\n",
       "        [16.0000],\n",
       "        [14.6200],\n",
       "        [ 9.4012],\n",
       "        [ 9.5048],\n",
       "        [ 8.0860],\n",
       "        [11.7908],\n",
       "        [15.0000],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [ 9.4492],\n",
       "        [10.0000],\n",
       "        [11.7921],\n",
       "        [10.3410],\n",
       "        [10.6489],\n",
       "        [10.0000],\n",
       "        [ 9.7632],\n",
       "        [12.0000],\n",
       "        [16.0000],\n",
       "        [ 7.2076],\n",
       "        [11.1470],\n",
       "        [ 6.7513],\n",
       "        [10.4583],\n",
       "        [10.0205],\n",
       "        [13.0000],\n",
       "        [12.0000],\n",
       "        [13.0000],\n",
       "        [12.0000],\n",
       "        [15.7529],\n",
       "        [13.0000],\n",
       "        [ 6.0000],\n",
       "        [16.0000],\n",
       "        [13.5179],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [10.0000],\n",
       "        [13.0000],\n",
       "        [ 6.5197],\n",
       "        [13.0000],\n",
       "        [ 9.0000],\n",
       "        [11.0000],\n",
       "        [ 5.1959],\n",
       "        [10.6015],\n",
       "        [15.0000],\n",
       "        [12.8736],\n",
       "        [ 4.8709],\n",
       "        [13.0000],\n",
       "        [10.7813],\n",
       "        [14.8032],\n",
       "        [10.4732],\n",
       "        [15.0000],\n",
       "        [13.5179],\n",
       "        [ 9.0000],\n",
       "        [13.0000],\n",
       "        [13.5456],\n",
       "        [11.0000],\n",
       "        [14.7834],\n",
       "        [10.0000],\n",
       "        [13.0000],\n",
       "        [10.9047],\n",
       "        [ 8.3756],\n",
       "        [ 9.3093],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [13.0000],\n",
       "        [11.8831],\n",
       "        [ 8.5492],\n",
       "        [11.3104],\n",
       "        [13.7747],\n",
       "        [ 9.0000],\n",
       "        [ 9.0000],\n",
       "        [15.0000],\n",
       "        [11.0000],\n",
       "        [10.0000],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [ 9.1148],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [12.0000],\n",
       "        [ 9.8603],\n",
       "        [11.4778],\n",
       "        [ 9.0000],\n",
       "        [11.1030],\n",
       "        [12.0000],\n",
       "        [12.0000],\n",
       "        [10.3989],\n",
       "        [ 9.1846],\n",
       "        [ 8.8233],\n",
       "        [13.0000],\n",
       "        [ 9.2501],\n",
       "        [10.7213],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [ 7.0834],\n",
       "        [13.3623],\n",
       "        [ 8.4127],\n",
       "        [11.0000],\n",
       "        [ 8.0000],\n",
       "        [12.0000],\n",
       "        [11.0000],\n",
       "        [15.0000],\n",
       "        [15.7537],\n",
       "        [ 9.0982],\n",
       "        [ 5.1955],\n",
       "        [12.0000],\n",
       "        [11.2323],\n",
       "        [10.1890],\n",
       "        [10.0361],\n",
       "        [10.0000],\n",
       "        [ 9.6185],\n",
       "        [13.6519],\n",
       "        [12.0000],\n",
       "        [ 8.7198],\n",
       "        [10.0000],\n",
       "        [13.4310],\n",
       "        [10.0000],\n",
       "        [11.0000],\n",
       "        [ 9.1908],\n",
       "        [11.0000],\n",
       "        [11.0000],\n",
       "        [12.0000],\n",
       "        [ 9.4836],\n",
       "        [ 9.2908],\n",
       "        [ 9.0000],\n",
       "        [ 9.0000],\n",
       "        [11.0000],\n",
       "        [12.1976],\n",
       "        [13.0131],\n",
       "        [ 9.0056],\n",
       "        [11.0000],\n",
       "        [ 8.0000],\n",
       "        [10.7753],\n",
       "        [14.9518],\n",
       "        [11.3129],\n",
       "        [10.0000],\n",
       "        [ 8.0000],\n",
       "        [ 7.6021],\n",
       "        [10.0000],\n",
       "        [13.8911],\n",
       "        [11.0896],\n",
       "        [13.7014],\n",
       "        [12.0000]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.9016],\n",
       "        [ 9.6708],\n",
       "        [11.3493],\n",
       "        [10.8915],\n",
       "        [ 8.5614],\n",
       "        [10.9262],\n",
       "        [ 9.6297],\n",
       "        [10.0785],\n",
       "        [ 9.6328],\n",
       "        [10.5889],\n",
       "        [10.0576],\n",
       "        [ 9.9147],\n",
       "        [ 9.6295],\n",
       "        [10.0298],\n",
       "        [ 7.9633],\n",
       "        [10.0797],\n",
       "        [ 9.5212],\n",
       "        [ 9.7571],\n",
       "        [ 8.0592],\n",
       "        [ 9.3206],\n",
       "        [ 9.1791],\n",
       "        [ 9.0155],\n",
       "        [ 9.3588],\n",
       "        [10.2527],\n",
       "        [ 9.8630],\n",
       "        [ 9.5891],\n",
       "        [ 9.4165],\n",
       "        [ 9.7657],\n",
       "        [10.6098],\n",
       "        [11.3421],\n",
       "        [11.1193],\n",
       "        [10.0601],\n",
       "        [10.5406],\n",
       "        [11.4538],\n",
       "        [10.7587],\n",
       "        [11.1018],\n",
       "        [10.5536],\n",
       "        [ 8.1834],\n",
       "        [11.4398],\n",
       "        [ 9.5553],\n",
       "        [ 8.9887],\n",
       "        [10.4332],\n",
       "        [ 9.7693],\n",
       "        [10.4827],\n",
       "        [ 9.2942],\n",
       "        [10.6011],\n",
       "        [10.1375],\n",
       "        [ 9.7819],\n",
       "        [10.7514],\n",
       "        [10.3909],\n",
       "        [10.2272],\n",
       "        [10.5189],\n",
       "        [ 9.3560],\n",
       "        [ 9.5182],\n",
       "        [10.8139],\n",
       "        [10.5572],\n",
       "        [ 9.7398],\n",
       "        [10.8317],\n",
       "        [ 8.6642],\n",
       "        [ 9.9130],\n",
       "        [ 9.1799],\n",
       "        [10.3299],\n",
       "        [10.4760],\n",
       "        [ 9.7833],\n",
       "        [10.5891],\n",
       "        [ 8.8140],\n",
       "        [10.5360],\n",
       "        [ 8.5716],\n",
       "        [10.0303],\n",
       "        [ 9.8519],\n",
       "        [ 9.2432],\n",
       "        [11.2016],\n",
       "        [ 9.2363],\n",
       "        [ 9.9469],\n",
       "        [ 9.6798],\n",
       "        [ 9.8396],\n",
       "        [ 9.3752],\n",
       "        [10.1846],\n",
       "        [ 9.7582],\n",
       "        [10.0633],\n",
       "        [ 9.7633],\n",
       "        [ 9.8254],\n",
       "        [10.6760],\n",
       "        [ 9.3040],\n",
       "        [10.8697],\n",
       "        [12.1352],\n",
       "        [ 9.6577],\n",
       "        [10.3543],\n",
       "        [ 9.0039],\n",
       "        [10.0908],\n",
       "        [10.0055],\n",
       "        [ 9.8273],\n",
       "        [10.8982],\n",
       "        [ 8.9379],\n",
       "        [10.7195],\n",
       "        [10.7942],\n",
       "        [10.0723],\n",
       "        [ 9.2673],\n",
       "        [ 9.3226],\n",
       "        [ 9.7652],\n",
       "        [ 9.9734],\n",
       "        [10.8582],\n",
       "        [10.4025],\n",
       "        [ 8.9118],\n",
       "        [12.1208],\n",
       "        [ 9.7692],\n",
       "        [ 9.3768],\n",
       "        [10.1729],\n",
       "        [ 9.9360],\n",
       "        [10.3205],\n",
       "        [ 9.6250],\n",
       "        [10.7955],\n",
       "        [10.3725],\n",
       "        [ 9.7683],\n",
       "        [ 8.9711],\n",
       "        [10.8045],\n",
       "        [10.5814],\n",
       "        [ 9.0358],\n",
       "        [10.2654],\n",
       "        [10.8068],\n",
       "        [ 9.8725],\n",
       "        [ 9.4153],\n",
       "        [10.5781],\n",
       "        [10.6576],\n",
       "        [ 8.8377],\n",
       "        [ 8.7440],\n",
       "        [10.2674],\n",
       "        [ 8.8729],\n",
       "        [ 9.5533],\n",
       "        [10.0365],\n",
       "        [ 9.4058],\n",
       "        [10.3413],\n",
       "        [10.8094],\n",
       "        [ 9.6270],\n",
       "        [10.2097],\n",
       "        [10.1150],\n",
       "        [ 9.7202],\n",
       "        [ 9.6427],\n",
       "        [ 9.9762],\n",
       "        [10.0021],\n",
       "        [ 8.9785],\n",
       "        [ 9.8990],\n",
       "        [ 8.3659],\n",
       "        [ 9.8366],\n",
       "        [10.8005],\n",
       "        [10.9763],\n",
       "        [ 9.9892],\n",
       "        [10.0985],\n",
       "        [11.2412],\n",
       "        [11.9751],\n",
       "        [ 9.6727],\n",
       "        [10.4078],\n",
       "        [ 9.5829],\n",
       "        [ 9.6092],\n",
       "        [ 8.4861],\n",
       "        [10.3622],\n",
       "        [ 9.9748],\n",
       "        [ 9.1003],\n",
       "        [10.8271],\n",
       "        [10.1475],\n",
       "        [ 9.9070],\n",
       "        [ 8.9734],\n",
       "        [10.3006],\n",
       "        [10.8845],\n",
       "        [ 9.3138],\n",
       "        [10.3900],\n",
       "        [ 9.5831],\n",
       "        [10.4285],\n",
       "        [ 9.4591],\n",
       "        [10.1802],\n",
       "        [10.2758],\n",
       "        [ 9.7804],\n",
       "        [ 9.5877],\n",
       "        [10.0451],\n",
       "        [10.3820],\n",
       "        [ 9.6667],\n",
       "        [ 8.9302],\n",
       "        [10.3992],\n",
       "        [10.3476],\n",
       "        [ 9.1101],\n",
       "        [10.3359],\n",
       "        [ 9.6425],\n",
       "        [ 9.9124],\n",
       "        [ 8.9108],\n",
       "        [ 9.1448],\n",
       "        [ 9.1938],\n",
       "        [ 8.5684],\n",
       "        [ 9.9748],\n",
       "        [11.3129],\n",
       "        [10.4264],\n",
       "        [ 9.5145],\n",
       "        [ 8.7294],\n",
       "        [ 9.4444],\n",
       "        [ 9.8281],\n",
       "        [10.1482],\n",
       "        [ 9.4868],\n",
       "        [ 9.0541],\n",
       "        [ 8.6106],\n",
       "        [10.7366],\n",
       "        [ 9.1424],\n",
       "        [ 8.7888],\n",
       "        [10.1738],\n",
       "        [10.1525],\n",
       "        [10.6490],\n",
       "        [10.9064],\n",
       "        [10.1431],\n",
       "        [ 9.7731],\n",
       "        [ 9.2679],\n",
       "        [10.5973],\n",
       "        [10.2669],\n",
       "        [11.4431],\n",
       "        [10.0869],\n",
       "        [ 9.3050],\n",
       "        [ 9.5515],\n",
       "        [ 9.9499],\n",
       "        [ 8.0938],\n",
       "        [10.3064],\n",
       "        [ 9.4081],\n",
       "        [ 8.3530],\n",
       "        [ 9.4307],\n",
       "        [10.4572],\n",
       "        [ 9.4356],\n",
       "        [11.1844],\n",
       "        [10.0334],\n",
       "        [ 9.6204],\n",
       "        [ 8.8384],\n",
       "        [ 9.9351],\n",
       "        [11.5980],\n",
       "        [ 9.6876],\n",
       "        [ 8.7926],\n",
       "        [11.1213],\n",
       "        [ 9.5491],\n",
       "        [10.4914],\n",
       "        [ 9.3991],\n",
       "        [10.0717],\n",
       "        [10.7201],\n",
       "        [ 9.5975],\n",
       "        [ 9.5610],\n",
       "        [ 8.6354],\n",
       "        [11.2962],\n",
       "        [10.8305],\n",
       "        [ 9.4298],\n",
       "        [10.1741],\n",
       "        [ 9.8486],\n",
       "        [10.2829],\n",
       "        [ 9.9970],\n",
       "        [ 9.2267],\n",
       "        [10.0385],\n",
       "        [10.7505],\n",
       "        [ 9.8113],\n",
       "        [10.3827],\n",
       "        [ 9.4661],\n",
       "        [ 8.9550],\n",
       "        [ 9.8876],\n",
       "        [ 9.2398],\n",
       "        [ 9.7456],\n",
       "        [ 9.7947],\n",
       "        [ 9.0546],\n",
       "        [10.3614],\n",
       "        [ 9.7975],\n",
       "        [10.7001],\n",
       "        [10.4475],\n",
       "        [ 8.7128],\n",
       "        [10.4574],\n",
       "        [11.1993],\n",
       "        [10.5987],\n",
       "        [ 9.0195],\n",
       "        [ 7.7877],\n",
       "        [10.1887],\n",
       "        [ 9.9705],\n",
       "        [ 9.0702],\n",
       "        [10.8970],\n",
       "        [10.1088],\n",
       "        [ 8.9053],\n",
       "        [ 9.3584],\n",
       "        [ 9.9523],\n",
       "        [10.0834],\n",
       "        [ 8.4756],\n",
       "        [10.0888],\n",
       "        [10.0547],\n",
       "        [ 9.1735],\n",
       "        [10.3557],\n",
       "        [ 9.6534],\n",
       "        [ 9.9233],\n",
       "        [ 9.9753],\n",
       "        [ 9.2802],\n",
       "        [10.6134],\n",
       "        [10.6118],\n",
       "        [10.1631],\n",
       "        [10.4030],\n",
       "        [10.8861],\n",
       "        [ 9.2608],\n",
       "        [ 9.5911],\n",
       "        [ 9.4097],\n",
       "        [10.2774],\n",
       "        [10.5139],\n",
       "        [10.1534],\n",
       "        [ 7.3105],\n",
       "        [ 8.6636],\n",
       "        [11.0140],\n",
       "        [ 9.7098],\n",
       "        [10.4442],\n",
       "        [10.0315],\n",
       "        [ 9.4773],\n",
       "        [ 8.8213],\n",
       "        [10.7426],\n",
       "        [10.0792],\n",
       "        [10.5218],\n",
       "        [ 9.7382],\n",
       "        [10.0730],\n",
       "        [ 9.7215],\n",
       "        [ 9.0573],\n",
       "        [11.4659],\n",
       "        [10.2878],\n",
       "        [ 9.7589],\n",
       "        [10.6399],\n",
       "        [10.9734],\n",
       "        [ 9.6830],\n",
       "        [ 9.3508],\n",
       "        [ 9.0973],\n",
       "        [ 9.4463],\n",
       "        [10.1688],\n",
       "        [ 9.9649],\n",
       "        [10.5636],\n",
       "        [ 9.2154],\n",
       "        [ 9.5654],\n",
       "        [ 9.6652],\n",
       "        [ 8.9259],\n",
       "        [10.1384],\n",
       "        [10.7503],\n",
       "        [ 9.6891],\n",
       "        [10.5645],\n",
       "        [10.0693],\n",
       "        [10.2967],\n",
       "        [ 9.1687],\n",
       "        [10.2616],\n",
       "        [ 8.2382],\n",
       "        [10.4829],\n",
       "        [11.2774],\n",
       "        [10.4368],\n",
       "        [11.0002],\n",
       "        [ 8.9096],\n",
       "        [10.7497],\n",
       "        [10.0551],\n",
       "        [ 9.7429],\n",
       "        [ 8.4577],\n",
       "        [10.5709],\n",
       "        [10.1122],\n",
       "        [10.1302],\n",
       "        [10.1209],\n",
       "        [10.3447],\n",
       "        [10.6993],\n",
       "        [10.5272],\n",
       "        [11.9234],\n",
       "        [10.3138],\n",
       "        [10.9233],\n",
       "        [10.9156],\n",
       "        [ 9.9105],\n",
       "        [ 9.8817],\n",
       "        [10.1568],\n",
       "        [10.5770],\n",
       "        [10.2820],\n",
       "        [10.5960],\n",
       "        [10.1624],\n",
       "        [10.6875],\n",
       "        [11.4349],\n",
       "        [ 9.6464],\n",
       "        [ 9.2742],\n",
       "        [10.5825],\n",
       "        [ 9.5382],\n",
       "        [11.4326],\n",
       "        [ 9.9393],\n",
       "        [10.9639],\n",
       "        [ 9.9586],\n",
       "        [ 9.7236],\n",
       "        [11.0353],\n",
       "        [ 8.0593],\n",
       "        [ 9.0869],\n",
       "        [ 9.8728],\n",
       "        [10.8269],\n",
       "        [10.3606],\n",
       "        [ 8.9983],\n",
       "        [ 9.4286],\n",
       "        [10.2638],\n",
       "        [ 9.2396],\n",
       "        [ 9.7658],\n",
       "        [11.9905],\n",
       "        [ 8.7686],\n",
       "        [ 9.2257],\n",
       "        [10.1916],\n",
       "        [ 9.0657],\n",
       "        [10.7375],\n",
       "        [10.3995],\n",
       "        [10.5236],\n",
       "        [ 9.9548],\n",
       "        [10.1816],\n",
       "        [10.7773],\n",
       "        [ 9.9165],\n",
       "        [10.0797],\n",
       "        [10.5195],\n",
       "        [10.1149],\n",
       "        [ 8.7879],\n",
       "        [10.5220],\n",
       "        [10.8298],\n",
       "        [ 9.2055],\n",
       "        [10.4524],\n",
       "        [ 9.3893],\n",
       "        [11.8150],\n",
       "        [ 9.1718],\n",
       "        [10.0077],\n",
       "        [ 9.2835],\n",
       "        [ 9.6374],\n",
       "        [ 8.8549],\n",
       "        [ 8.3218],\n",
       "        [ 9.5242],\n",
       "        [ 9.5685],\n",
       "        [ 9.7533],\n",
       "        [ 8.8070],\n",
       "        [10.0876],\n",
       "        [10.4839],\n",
       "        [ 8.1316],\n",
       "        [10.1368],\n",
       "        [11.2064],\n",
       "        [ 9.9833],\n",
       "        [10.2668],\n",
       "        [10.3951],\n",
       "        [ 9.8343],\n",
       "        [ 9.6193],\n",
       "        [ 9.7029],\n",
       "        [10.5922],\n",
       "        [ 9.8635],\n",
       "        [ 9.6922],\n",
       "        [ 9.4438],\n",
       "        [ 9.6946],\n",
       "        [10.6032],\n",
       "        [ 9.0596],\n",
       "        [10.3832],\n",
       "        [10.5129],\n",
       "        [ 9.9060],\n",
       "        [ 9.3126],\n",
       "        [10.4414],\n",
       "        [ 9.7863],\n",
       "        [ 9.9558],\n",
       "        [ 9.8378],\n",
       "        [10.6713],\n",
       "        [10.3663],\n",
       "        [ 8.3232],\n",
       "        [ 9.9820],\n",
       "        [ 9.1643],\n",
       "        [ 9.3416],\n",
       "        [ 9.4097],\n",
       "        [ 9.6146],\n",
       "        [ 9.1148],\n",
       "        [10.5300],\n",
       "        [11.0325],\n",
       "        [10.2198],\n",
       "        [ 8.9935],\n",
       "        [ 9.8545],\n",
       "        [ 9.9116],\n",
       "        [ 9.1596],\n",
       "        [ 9.9526],\n",
       "        [ 9.8826],\n",
       "        [10.2353],\n",
       "        [ 9.7157],\n",
       "        [ 9.5741],\n",
       "        [ 9.7621],\n",
       "        [10.1963],\n",
       "        [10.1237],\n",
       "        [10.2369],\n",
       "        [ 9.6177],\n",
       "        [ 9.8485],\n",
       "        [ 9.6419],\n",
       "        [10.3233],\n",
       "        [ 9.6520],\n",
       "        [10.4896],\n",
       "        [10.2164],\n",
       "        [10.6845],\n",
       "        [ 9.9130],\n",
       "        [11.0219],\n",
       "        [10.0770],\n",
       "        [10.8326],\n",
       "        [ 8.8614],\n",
       "        [10.3328],\n",
       "        [ 9.7580],\n",
       "        [ 9.9636],\n",
       "        [ 9.8571],\n",
       "        [10.3294],\n",
       "        [10.6706],\n",
       "        [10.6210],\n",
       "        [ 9.1769],\n",
       "        [10.0946],\n",
       "        [10.4740],\n",
       "        [ 9.9959],\n",
       "        [ 9.4400],\n",
       "        [ 9.5035],\n",
       "        [11.1623],\n",
       "        [10.3554],\n",
       "        [11.2872],\n",
       "        [10.0313],\n",
       "        [11.6563],\n",
       "        [ 9.3135],\n",
       "        [10.1642],\n",
       "        [10.9321],\n",
       "        [10.7932],\n",
       "        [ 9.5022],\n",
       "        [ 9.5999],\n",
       "        [ 9.3516],\n",
       "        [11.9072],\n",
       "        [11.0902],\n",
       "        [ 9.7151],\n",
       "        [10.9887],\n",
       "        [ 9.3137]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy5455/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from '/home/jmy5455/models.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import importlib\n",
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla.core.xla_model as xm\n",
    "model = models.ConvolutionalModel()\n",
    "device = xm.xla_device()\n",
    "model = model.to(device)\n",
    "\n",
    "checkpoint = torch.load(\"./conv_model/v1/checkpoint-epoch6-best.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalModel(\n",
       "  (model_strand_specific_forward): Sequential(\n",
       "    (conv1): Conv1d(4, 256, kernel_size=(31,), stride=(1,), padding=(15,))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (model_strand_specific_reverse): Sequential(\n",
       "    (conv1): Conv1d(4, 256, kernel_size=(31,), stride=(1,), padding=(15,))\n",
       "    (relu1): ReLU()\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,))\n",
       "    (relu2): ReLU()\n",
       "  )\n",
       "  (model_body): Sequential(\n",
       "    (conv3): Conv1d(512, 256, kernel_size=(31,), stride=(1,), padding=(15,))\n",
       "    (relu3): ReLU()\n",
       "    (conv4): Conv1d(256, 256, kernel_size=(31,), stride=(1,), padding=(15,))\n",
       "    (relu4): ReLU()\n",
       "    (faltten): Flatten(start_dim=1, end_dim=-1)\n",
       "    (dense1): Linear(in_features=28160, out_features=256, bias=True)\n",
       "    (relu5): ReLU()\n",
       "    (dropout1): Dropout(p=0.2, inplace=False)\n",
       "    (dense2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (relu6): ReLU()\n",
       "  )\n",
       "  (model_head): Sequential(\n",
       "    (dense_head): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0 in world size of 1\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DreamChallengeDataModule\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "\n",
    "datamodule = DreamChallengeDataModule(data_dir=\"./shards/\", batch_size=128, num_workers=1)\n",
    "datamodule.setup()\n",
    "test_loader = datamodule.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "def test_loop_fn(model, loader, device):\n",
    "  model.eval()\n",
    "  metrics = MeanSquaredError()\n",
    "  pbar = tqdm(enumerate(loader))\n",
    "  preds = []; targets = []\n",
    "  for step, (data, target) in pbar:\n",
    "    data = torch.squeeze(data, 0).to(device)\n",
    "    target = torch.squeeze(target, 0).to(device)\n",
    "    pred = model(data)\n",
    "    print(pred)\n",
    "    metrics.update(pred, target)\n",
    "\n",
    "    targets.append(target); preds.append(pred)\n",
    "\n",
    "  mse = metrics.compute()  \n",
    "  return mse, targets, preds\n",
    "\n",
    "def plot_scatter(x, y, lim=20):\n",
    "  fig = plt.figure(figsize=(12, 12))\n",
    "  ax = sns.scatterplot(x=x, y=y)\n",
    "  ax.set_xlim(left=0, right=lim)\n",
    "  ax.set_ylim(bottom=0, right=lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse, targets, preds = test_loop_fn(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_npy = [i.detach().cpu().numpy() for i in targets]\n",
    "preds_npy = [i.detach().cpu().numpy() for i in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_npy = np.concatenate(targets_npy).flatten()\n",
    "preds_npy = np.concatenate(preds_npy).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.0625, 9.0625, 9.0625, ..., 9.0625, 9.0625, 9.0625], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_npy[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=targets_npy[:10000], y=preds_npy[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.5625, 10.5625, 12.9375, ..., 14.    ,  9.    , 15.    ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets_npy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
