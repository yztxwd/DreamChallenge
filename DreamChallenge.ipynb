{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dream Challenge Model Prototye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synapseclient \n",
    "import synapseutils \n",
    "\n",
    "syn = synapseclient.Synapse() \n",
    "syn.login('yztxwd@gmail.com','9zqqW9Jy5QhRFeS') \n",
    "synapseutils.syncFromSynapse(syn, 'syn30026233', './data/')\n",
    "synapseutils.syncFromSynapse(syn, 'syn28469146', './data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.read_table(\"./data/train_sequences.txt\", header=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_full = pd.read_table('./data/train_sequences.txt', header=None, names=['seq', 'target'], dtype={'seq':str, 'target':np.float32})\n",
    "df_train, df_remain = train_test_split(df_full, test_size=0.2, random_state=1)\n",
    "df_val, df_test = train_test_split(df_remain, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DreamChallengeDataset(df_train.reset_index(), transform=DNA2OneHot())\n",
    "dataset_val = DreamChallengeDataset(df_val.reset_index(), transform=DNA2OneHot())\n",
    "dataset_test = DreamChallengeDataset(df_test.reset_index(), transform=DNA2OneHot())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tfrecord\n",
    "\n",
    "def get_data_TFRecord_worker(dataset, outprefix):\n",
    "\n",
    "    TFRecord_file = outprefix + \".TFRecord\"\n",
    "    writer = tfrecord.TFRecordWriter(TFRecord_file)\n",
    "    count = 0\n",
    "    for idx, (seq, target) in enumerate(dataset):\n",
    "        writer.write({'seq': (list(seq.flatten()), 'float'),\n",
    "                        'target': (target[0], 'float')})\n",
    "        count += 1\n",
    "        if count % 100000 == 0: print(f\"{count} written\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data_TFRecord_worker(dataset_train, \"train\")\n",
    "#get_data_TFRecord_worker(dataset_val, \"val\")\n",
    "#get_data_TFRecord_worker(dataset_test, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### webdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = DreamChallengeDataset(df_train.reset_index(), transform=DNA2OneHot())\n",
    "dataset_val = DreamChallengeDataset(df_val.reset_index(), transform=DNA2OneHot())\n",
    "dataset_test = DreamChallengeDataset(df_test.reset_index(), transform=DNA2OneHot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeWDS(dataset, prefix):\n",
    "    sink = wds.ShardWriter(\"shards/\" + prefix + \"-%02d.tar\", maxcount=10000)\n",
    "    count = 0\n",
    "    for idx, (seq, target) in enumerate(dataset):\n",
    "        sink.write({\n",
    "            '__key__': 'sample%06d' % idx,\n",
    "            'seq.npy': seq,\n",
    "            'target.npy': target\n",
    "        })\n",
    "        count += 1\n",
    "        if count % 100000 == 0: print(f\"{count} written\")\n",
    "    sink.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing shards/train-00.tar 0 0.0 GB 0\n",
      "# writing shards/train-01.tar 10000 0.0 GB 10000\n",
      "# writing shards/train-02.tar 10000 0.0 GB 20000\n",
      "# writing shards/train-03.tar 10000 0.0 GB 30000\n",
      "# writing shards/train-04.tar 10000 0.0 GB 40000\n",
      "# writing shards/train-05.tar 10000 0.0 GB 50000\n",
      "# writing shards/train-06.tar 10000 0.0 GB 60000\n",
      "# writing shards/train-07.tar 10000 0.0 GB 70000\n",
      "# writing shards/train-08.tar 10000 0.0 GB 80000\n",
      "# writing shards/train-09.tar 10000 0.0 GB 90000\n",
      "100000 written\n",
      "# writing shards/train-10.tar 10000 0.0 GB 100000\n",
      "# writing shards/train-11.tar 10000 0.0 GB 110000\n",
      "# writing shards/train-12.tar 10000 0.0 GB 120000\n",
      "# writing shards/train-13.tar 10000 0.0 GB 130000\n",
      "# writing shards/train-14.tar 10000 0.0 GB 140000\n",
      "# writing shards/train-15.tar 10000 0.0 GB 150000\n",
      "# writing shards/train-16.tar 10000 0.0 GB 160000\n",
      "# writing shards/train-17.tar 10000 0.0 GB 170000\n",
      "# writing shards/train-18.tar 10000 0.0 GB 180000\n",
      "# writing shards/train-19.tar 10000 0.0 GB 190000\n",
      "200000 written\n",
      "# writing shards/train-20.tar 10000 0.0 GB 200000\n",
      "# writing shards/train-21.tar 10000 0.0 GB 210000\n",
      "# writing shards/train-22.tar 10000 0.0 GB 220000\n",
      "# writing shards/train-23.tar 10000 0.0 GB 230000\n",
      "# writing shards/train-24.tar 10000 0.0 GB 240000\n",
      "# writing shards/train-25.tar 10000 0.0 GB 250000\n",
      "# writing shards/train-26.tar 10000 0.0 GB 260000\n",
      "# writing shards/train-27.tar 10000 0.0 GB 270000\n",
      "# writing shards/train-28.tar 10000 0.0 GB 280000\n",
      "# writing shards/train-29.tar 10000 0.0 GB 290000\n",
      "300000 written\n",
      "# writing shards/train-30.tar 10000 0.0 GB 300000\n",
      "# writing shards/train-31.tar 10000 0.0 GB 310000\n",
      "# writing shards/train-32.tar 10000 0.0 GB 320000\n",
      "# writing shards/train-33.tar 10000 0.0 GB 330000\n",
      "# writing shards/train-34.tar 10000 0.0 GB 340000\n",
      "# writing shards/train-35.tar 10000 0.0 GB 350000\n",
      "# writing shards/train-36.tar 10000 0.0 GB 360000\n",
      "# writing shards/train-37.tar 10000 0.0 GB 370000\n",
      "# writing shards/train-38.tar 10000 0.0 GB 380000\n",
      "# writing shards/train-39.tar 10000 0.0 GB 390000\n",
      "400000 written\n",
      "# writing shards/train-40.tar 10000 0.0 GB 400000\n",
      "# writing shards/train-41.tar 10000 0.0 GB 410000\n",
      "# writing shards/train-42.tar 10000 0.0 GB 420000\n",
      "# writing shards/train-43.tar 10000 0.0 GB 430000\n",
      "# writing shards/train-44.tar 10000 0.0 GB 440000\n",
      "# writing shards/train-45.tar 10000 0.0 GB 450000\n",
      "# writing shards/train-46.tar 10000 0.0 GB 460000\n",
      "# writing shards/train-47.tar 10000 0.0 GB 470000\n",
      "# writing shards/train-48.tar 10000 0.0 GB 480000\n",
      "# writing shards/train-49.tar 10000 0.0 GB 490000\n",
      "500000 written\n",
      "# writing shards/train-50.tar 10000 0.0 GB 500000\n",
      "# writing shards/train-51.tar 10000 0.0 GB 510000\n",
      "# writing shards/train-52.tar 10000 0.0 GB 520000\n",
      "# writing shards/train-53.tar 10000 0.0 GB 530000\n",
      "# writing shards/train-54.tar 10000 0.0 GB 540000\n",
      "# writing shards/train-55.tar 10000 0.0 GB 550000\n",
      "# writing shards/train-56.tar 10000 0.0 GB 560000\n",
      "# writing shards/train-57.tar 10000 0.0 GB 570000\n",
      "# writing shards/train-58.tar 10000 0.0 GB 580000\n",
      "# writing shards/train-59.tar 10000 0.0 GB 590000\n",
      "600000 written\n",
      "# writing shards/train-60.tar 10000 0.0 GB 600000\n",
      "# writing shards/train-61.tar 10000 0.0 GB 610000\n",
      "# writing shards/train-62.tar 10000 0.0 GB 620000\n",
      "# writing shards/train-63.tar 10000 0.0 GB 630000\n",
      "# writing shards/train-64.tar 10000 0.0 GB 640000\n",
      "# writing shards/train-65.tar 10000 0.0 GB 650000\n",
      "# writing shards/train-66.tar 10000 0.0 GB 660000\n",
      "# writing shards/train-67.tar 10000 0.0 GB 670000\n",
      "# writing shards/train-68.tar 10000 0.0 GB 680000\n",
      "# writing shards/train-69.tar 10000 0.0 GB 690000\n",
      "700000 written\n",
      "# writing shards/train-70.tar 10000 0.0 GB 700000\n",
      "# writing shards/train-71.tar 10000 0.0 GB 710000\n",
      "# writing shards/train-72.tar 10000 0.0 GB 720000\n",
      "# writing shards/train-73.tar 10000 0.0 GB 730000\n",
      "# writing shards/train-74.tar 10000 0.0 GB 740000\n",
      "# writing shards/train-75.tar 10000 0.0 GB 750000\n",
      "# writing shards/train-76.tar 10000 0.0 GB 760000\n",
      "# writing shards/train-77.tar 10000 0.0 GB 770000\n",
      "# writing shards/train-78.tar 10000 0.0 GB 780000\n",
      "# writing shards/train-79.tar 10000 0.0 GB 790000\n",
      "800000 written\n",
      "# writing shards/train-80.tar 10000 0.0 GB 800000\n",
      "# writing shards/train-81.tar 10000 0.0 GB 810000\n",
      "# writing shards/train-82.tar 10000 0.0 GB 820000\n",
      "# writing shards/train-83.tar 10000 0.0 GB 830000\n",
      "# writing shards/train-84.tar 10000 0.0 GB 840000\n",
      "# writing shards/train-85.tar 10000 0.0 GB 850000\n",
      "# writing shards/train-86.tar 10000 0.0 GB 860000\n",
      "# writing shards/train-87.tar 10000 0.0 GB 870000\n",
      "# writing shards/train-88.tar 10000 0.0 GB 880000\n",
      "# writing shards/train-89.tar 10000 0.0 GB 890000\n",
      "900000 written\n",
      "# writing shards/train-90.tar 10000 0.0 GB 900000\n",
      "# writing shards/train-91.tar 10000 0.0 GB 910000\n",
      "# writing shards/train-92.tar 10000 0.0 GB 920000\n",
      "# writing shards/train-93.tar 10000 0.0 GB 930000\n",
      "# writing shards/train-94.tar 10000 0.0 GB 940000\n",
      "# writing shards/train-95.tar 10000 0.0 GB 950000\n",
      "# writing shards/train-96.tar 10000 0.0 GB 960000\n",
      "# writing shards/train-97.tar 10000 0.0 GB 970000\n",
      "# writing shards/train-98.tar 10000 0.0 GB 980000\n",
      "# writing shards/train-99.tar 10000 0.0 GB 990000\n",
      "1000000 written\n",
      "# writing shards/train-100.tar 10000 0.0 GB 1000000\n",
      "# writing shards/train-101.tar 10000 0.0 GB 1010000\n",
      "# writing shards/train-102.tar 10000 0.0 GB 1020000\n",
      "# writing shards/train-103.tar 10000 0.0 GB 1030000\n",
      "# writing shards/train-104.tar 10000 0.0 GB 1040000\n",
      "# writing shards/train-105.tar 10000 0.0 GB 1050000\n",
      "# writing shards/train-106.tar 10000 0.0 GB 1060000\n",
      "# writing shards/train-107.tar 10000 0.0 GB 1070000\n",
      "# writing shards/train-108.tar 10000 0.0 GB 1080000\n",
      "# writing shards/train-109.tar 10000 0.0 GB 1090000\n",
      "1100000 written\n",
      "# writing shards/train-110.tar 10000 0.0 GB 1100000\n",
      "# writing shards/train-111.tar 10000 0.0 GB 1110000\n",
      "# writing shards/train-112.tar 10000 0.0 GB 1120000\n",
      "# writing shards/train-113.tar 10000 0.0 GB 1130000\n",
      "# writing shards/train-114.tar 10000 0.0 GB 1140000\n",
      "# writing shards/train-115.tar 10000 0.0 GB 1150000\n",
      "# writing shards/train-116.tar 10000 0.0 GB 1160000\n",
      "# writing shards/train-117.tar 10000 0.0 GB 1170000\n",
      "# writing shards/train-118.tar 10000 0.0 GB 1180000\n",
      "# writing shards/train-119.tar 10000 0.0 GB 1190000\n",
      "1200000 written\n",
      "# writing shards/train-120.tar 10000 0.0 GB 1200000\n",
      "# writing shards/train-121.tar 10000 0.0 GB 1210000\n",
      "# writing shards/train-122.tar 10000 0.0 GB 1220000\n",
      "# writing shards/train-123.tar 10000 0.0 GB 1230000\n",
      "# writing shards/train-124.tar 10000 0.0 GB 1240000\n",
      "# writing shards/train-125.tar 10000 0.0 GB 1250000\n",
      "# writing shards/train-126.tar 10000 0.0 GB 1260000\n",
      "# writing shards/train-127.tar 10000 0.0 GB 1270000\n",
      "# writing shards/train-128.tar 10000 0.0 GB 1280000\n",
      "# writing shards/train-129.tar 10000 0.0 GB 1290000\n",
      "1300000 written\n",
      "# writing shards/train-130.tar 10000 0.0 GB 1300000\n",
      "# writing shards/train-131.tar 10000 0.0 GB 1310000\n",
      "# writing shards/train-132.tar 10000 0.0 GB 1320000\n",
      "# writing shards/train-133.tar 10000 0.0 GB 1330000\n",
      "# writing shards/train-134.tar 10000 0.0 GB 1340000\n",
      "# writing shards/train-135.tar 10000 0.0 GB 1350000\n",
      "# writing shards/train-136.tar 10000 0.0 GB 1360000\n",
      "# writing shards/train-137.tar 10000 0.0 GB 1370000\n",
      "# writing shards/train-138.tar 10000 0.0 GB 1380000\n",
      "# writing shards/train-139.tar 10000 0.0 GB 1390000\n",
      "1400000 written\n",
      "# writing shards/train-140.tar 10000 0.0 GB 1400000\n",
      "# writing shards/train-141.tar 10000 0.0 GB 1410000\n",
      "# writing shards/train-142.tar 10000 0.0 GB 1420000\n",
      "# writing shards/train-143.tar 10000 0.0 GB 1430000\n",
      "# writing shards/train-144.tar 10000 0.0 GB 1440000\n",
      "# writing shards/train-145.tar 10000 0.0 GB 1450000\n",
      "# writing shards/train-146.tar 10000 0.0 GB 1460000\n",
      "# writing shards/train-147.tar 10000 0.0 GB 1470000\n",
      "# writing shards/train-148.tar 10000 0.0 GB 1480000\n",
      "# writing shards/train-149.tar 10000 0.0 GB 1490000\n",
      "1500000 written\n",
      "# writing shards/train-150.tar 10000 0.0 GB 1500000\n",
      "# writing shards/train-151.tar 10000 0.0 GB 1510000\n",
      "# writing shards/train-152.tar 10000 0.0 GB 1520000\n",
      "# writing shards/train-153.tar 10000 0.0 GB 1530000\n",
      "# writing shards/train-154.tar 10000 0.0 GB 1540000\n",
      "# writing shards/train-155.tar 10000 0.0 GB 1550000\n",
      "# writing shards/train-156.tar 10000 0.0 GB 1560000\n",
      "# writing shards/train-157.tar 10000 0.0 GB 1570000\n",
      "# writing shards/train-158.tar 10000 0.0 GB 1580000\n",
      "# writing shards/train-159.tar 10000 0.0 GB 1590000\n",
      "1600000 written\n",
      "# writing shards/train-160.tar 10000 0.0 GB 1600000\n",
      "# writing shards/train-161.tar 10000 0.0 GB 1610000\n",
      "# writing shards/train-162.tar 10000 0.0 GB 1620000\n",
      "# writing shards/train-163.tar 10000 0.0 GB 1630000\n",
      "# writing shards/train-164.tar 10000 0.0 GB 1640000\n",
      "# writing shards/train-165.tar 10000 0.0 GB 1650000\n",
      "# writing shards/train-166.tar 10000 0.0 GB 1660000\n",
      "# writing shards/train-167.tar 10000 0.0 GB 1670000\n",
      "# writing shards/train-168.tar 10000 0.0 GB 1680000\n",
      "# writing shards/train-169.tar 10000 0.0 GB 1690000\n",
      "1700000 written\n",
      "# writing shards/train-170.tar 10000 0.0 GB 1700000\n",
      "# writing shards/train-171.tar 10000 0.0 GB 1710000\n",
      "# writing shards/train-172.tar 10000 0.0 GB 1720000\n",
      "# writing shards/train-173.tar 10000 0.0 GB 1730000\n",
      "# writing shards/train-174.tar 10000 0.0 GB 1740000\n",
      "# writing shards/train-175.tar 10000 0.0 GB 1750000\n",
      "# writing shards/train-176.tar 10000 0.0 GB 1760000\n",
      "# writing shards/train-177.tar 10000 0.0 GB 1770000\n",
      "# writing shards/train-178.tar 10000 0.0 GB 1780000\n",
      "# writing shards/train-179.tar 10000 0.0 GB 1790000\n",
      "1800000 written\n",
      "# writing shards/train-180.tar 10000 0.0 GB 1800000\n",
      "# writing shards/train-181.tar 10000 0.0 GB 1810000\n",
      "# writing shards/train-182.tar 10000 0.0 GB 1820000\n",
      "# writing shards/train-183.tar 10000 0.0 GB 1830000\n",
      "# writing shards/train-184.tar 10000 0.0 GB 1840000\n",
      "# writing shards/train-185.tar 10000 0.0 GB 1850000\n",
      "# writing shards/train-186.tar 10000 0.0 GB 1860000\n",
      "# writing shards/train-187.tar 10000 0.0 GB 1870000\n",
      "# writing shards/train-188.tar 10000 0.0 GB 1880000\n",
      "# writing shards/train-189.tar 10000 0.0 GB 1890000\n",
      "1900000 written\n",
      "# writing shards/train-190.tar 10000 0.0 GB 1900000\n",
      "# writing shards/train-191.tar 10000 0.0 GB 1910000\n",
      "# writing shards/train-192.tar 10000 0.0 GB 1920000\n",
      "# writing shards/train-193.tar 10000 0.0 GB 1930000\n",
      "# writing shards/train-194.tar 10000 0.0 GB 1940000\n",
      "# writing shards/train-195.tar 10000 0.0 GB 1950000\n",
      "# writing shards/train-196.tar 10000 0.0 GB 1960000\n",
      "# writing shards/train-197.tar 10000 0.0 GB 1970000\n",
      "# writing shards/train-198.tar 10000 0.0 GB 1980000\n",
      "# writing shards/train-199.tar 10000 0.0 GB 1990000\n",
      "2000000 written\n",
      "# writing shards/train-200.tar 10000 0.0 GB 2000000\n",
      "# writing shards/train-201.tar 10000 0.0 GB 2010000\n",
      "# writing shards/train-202.tar 10000 0.0 GB 2020000\n",
      "# writing shards/train-203.tar 10000 0.0 GB 2030000\n",
      "# writing shards/train-204.tar 10000 0.0 GB 2040000\n",
      "# writing shards/train-205.tar 10000 0.0 GB 2050000\n",
      "# writing shards/train-206.tar 10000 0.0 GB 2060000\n",
      "# writing shards/train-207.tar 10000 0.0 GB 2070000\n",
      "# writing shards/train-208.tar 10000 0.0 GB 2080000\n",
      "# writing shards/train-209.tar 10000 0.0 GB 2090000\n",
      "2100000 written\n",
      "# writing shards/train-210.tar 10000 0.0 GB 2100000\n",
      "# writing shards/train-211.tar 10000 0.0 GB 2110000\n",
      "# writing shards/train-212.tar 10000 0.0 GB 2120000\n",
      "# writing shards/train-213.tar 10000 0.0 GB 2130000\n",
      "# writing shards/train-214.tar 10000 0.0 GB 2140000\n",
      "# writing shards/train-215.tar 10000 0.0 GB 2150000\n",
      "# writing shards/train-216.tar 10000 0.0 GB 2160000\n",
      "# writing shards/train-217.tar 10000 0.0 GB 2170000\n",
      "# writing shards/train-218.tar 10000 0.0 GB 2180000\n",
      "# writing shards/train-219.tar 10000 0.0 GB 2190000\n",
      "2200000 written\n",
      "# writing shards/train-220.tar 10000 0.0 GB 2200000\n",
      "# writing shards/train-221.tar 10000 0.0 GB 2210000\n",
      "# writing shards/train-222.tar 10000 0.0 GB 2220000\n",
      "# writing shards/train-223.tar 10000 0.0 GB 2230000\n",
      "# writing shards/train-224.tar 10000 0.0 GB 2240000\n",
      "# writing shards/train-225.tar 10000 0.0 GB 2250000\n",
      "# writing shards/train-226.tar 10000 0.0 GB 2260000\n",
      "# writing shards/train-227.tar 10000 0.0 GB 2270000\n",
      "# writing shards/train-228.tar 10000 0.0 GB 2280000\n",
      "# writing shards/train-229.tar 10000 0.0 GB 2290000\n",
      "2300000 written\n",
      "# writing shards/train-230.tar 10000 0.0 GB 2300000\n",
      "# writing shards/train-231.tar 10000 0.0 GB 2310000\n",
      "# writing shards/train-232.tar 10000 0.0 GB 2320000\n",
      "# writing shards/train-233.tar 10000 0.0 GB 2330000\n",
      "# writing shards/train-234.tar 10000 0.0 GB 2340000\n",
      "# writing shards/train-235.tar 10000 0.0 GB 2350000\n",
      "# writing shards/train-236.tar 10000 0.0 GB 2360000\n",
      "# writing shards/train-237.tar 10000 0.0 GB 2370000\n",
      "# writing shards/train-238.tar 10000 0.0 GB 2380000\n",
      "# writing shards/train-239.tar 10000 0.0 GB 2390000\n",
      "2400000 written\n",
      "# writing shards/train-240.tar 10000 0.0 GB 2400000\n",
      "# writing shards/train-241.tar 10000 0.0 GB 2410000\n",
      "# writing shards/train-242.tar 10000 0.0 GB 2420000\n",
      "# writing shards/train-243.tar 10000 0.0 GB 2430000\n",
      "# writing shards/train-244.tar 10000 0.0 GB 2440000\n",
      "# writing shards/train-245.tar 10000 0.0 GB 2450000\n",
      "# writing shards/train-246.tar 10000 0.0 GB 2460000\n",
      "# writing shards/train-247.tar 10000 0.0 GB 2470000\n",
      "# writing shards/train-248.tar 10000 0.0 GB 2480000\n",
      "# writing shards/train-249.tar 10000 0.0 GB 2490000\n",
      "2500000 written\n",
      "# writing shards/train-250.tar 10000 0.0 GB 2500000\n",
      "# writing shards/train-251.tar 10000 0.0 GB 2510000\n",
      "# writing shards/train-252.tar 10000 0.0 GB 2520000\n",
      "# writing shards/train-253.tar 10000 0.0 GB 2530000\n",
      "# writing shards/train-254.tar 10000 0.0 GB 2540000\n",
      "# writing shards/train-255.tar 10000 0.0 GB 2550000\n",
      "# writing shards/train-256.tar 10000 0.0 GB 2560000\n",
      "# writing shards/train-257.tar 10000 0.0 GB 2570000\n",
      "# writing shards/train-258.tar 10000 0.0 GB 2580000\n",
      "# writing shards/train-259.tar 10000 0.0 GB 2590000\n",
      "2600000 written\n",
      "# writing shards/train-260.tar 10000 0.0 GB 2600000\n",
      "# writing shards/train-261.tar 10000 0.0 GB 2610000\n",
      "# writing shards/train-262.tar 10000 0.0 GB 2620000\n",
      "# writing shards/train-263.tar 10000 0.0 GB 2630000\n",
      "# writing shards/train-264.tar 10000 0.0 GB 2640000\n",
      "# writing shards/train-265.tar 10000 0.0 GB 2650000\n",
      "# writing shards/train-266.tar 10000 0.0 GB 2660000\n",
      "# writing shards/train-267.tar 10000 0.0 GB 2670000\n",
      "# writing shards/train-268.tar 10000 0.0 GB 2680000\n",
      "# writing shards/train-269.tar 10000 0.0 GB 2690000\n",
      "2700000 written\n",
      "# writing shards/train-270.tar 10000 0.0 GB 2700000\n",
      "# writing shards/train-271.tar 10000 0.0 GB 2710000\n",
      "# writing shards/train-272.tar 10000 0.0 GB 2720000\n",
      "# writing shards/train-273.tar 10000 0.0 GB 2730000\n",
      "# writing shards/train-274.tar 10000 0.0 GB 2740000\n",
      "# writing shards/train-275.tar 10000 0.0 GB 2750000\n",
      "# writing shards/train-276.tar 10000 0.0 GB 2760000\n",
      "# writing shards/train-277.tar 10000 0.0 GB 2770000\n",
      "# writing shards/train-278.tar 10000 0.0 GB 2780000\n",
      "# writing shards/train-279.tar 10000 0.0 GB 2790000\n",
      "2800000 written\n",
      "# writing shards/train-280.tar 10000 0.0 GB 2800000\n",
      "# writing shards/train-281.tar 10000 0.0 GB 2810000\n",
      "# writing shards/train-282.tar 10000 0.0 GB 2820000\n",
      "# writing shards/train-283.tar 10000 0.0 GB 2830000\n",
      "# writing shards/train-284.tar 10000 0.0 GB 2840000\n",
      "# writing shards/train-285.tar 10000 0.0 GB 2850000\n",
      "# writing shards/train-286.tar 10000 0.0 GB 2860000\n",
      "# writing shards/train-287.tar 10000 0.0 GB 2870000\n",
      "# writing shards/train-288.tar 10000 0.0 GB 2880000\n",
      "# writing shards/train-289.tar 10000 0.0 GB 2890000\n",
      "2900000 written\n",
      "# writing shards/train-290.tar 10000 0.0 GB 2900000\n",
      "# writing shards/train-291.tar 10000 0.0 GB 2910000\n",
      "# writing shards/train-292.tar 10000 0.0 GB 2920000\n",
      "# writing shards/train-293.tar 10000 0.0 GB 2930000\n",
      "# writing shards/train-294.tar 10000 0.0 GB 2940000\n",
      "# writing shards/train-295.tar 10000 0.0 GB 2950000\n",
      "# writing shards/train-296.tar 10000 0.0 GB 2960000\n",
      "# writing shards/train-297.tar 10000 0.0 GB 2970000\n",
      "# writing shards/train-298.tar 10000 0.0 GB 2980000\n",
      "# writing shards/train-299.tar 10000 0.0 GB 2990000\n",
      "3000000 written\n",
      "# writing shards/train-300.tar 10000 0.0 GB 3000000\n",
      "# writing shards/train-301.tar 10000 0.0 GB 3010000\n",
      "# writing shards/train-302.tar 10000 0.0 GB 3020000\n",
      "# writing shards/train-303.tar 10000 0.0 GB 3030000\n",
      "# writing shards/train-304.tar 10000 0.0 GB 3040000\n",
      "# writing shards/train-305.tar 10000 0.0 GB 3050000\n",
      "# writing shards/train-306.tar 10000 0.0 GB 3060000\n",
      "# writing shards/train-307.tar 10000 0.0 GB 3070000\n",
      "# writing shards/train-308.tar 10000 0.0 GB 3080000\n",
      "# writing shards/train-309.tar 10000 0.0 GB 3090000\n",
      "3100000 written\n",
      "# writing shards/train-310.tar 10000 0.0 GB 3100000\n",
      "# writing shards/train-311.tar 10000 0.0 GB 3110000\n",
      "# writing shards/train-312.tar 10000 0.0 GB 3120000\n",
      "# writing shards/train-313.tar 10000 0.0 GB 3130000\n",
      "# writing shards/train-314.tar 10000 0.0 GB 3140000\n",
      "# writing shards/train-315.tar 10000 0.0 GB 3150000\n",
      "# writing shards/train-316.tar 10000 0.0 GB 3160000\n",
      "# writing shards/train-317.tar 10000 0.0 GB 3170000\n",
      "# writing shards/train-318.tar 10000 0.0 GB 3180000\n",
      "# writing shards/train-319.tar 10000 0.0 GB 3190000\n",
      "3200000 written\n",
      "# writing shards/train-320.tar 10000 0.0 GB 3200000\n",
      "# writing shards/train-321.tar 10000 0.0 GB 3210000\n",
      "# writing shards/train-322.tar 10000 0.0 GB 3220000\n",
      "# writing shards/train-323.tar 10000 0.0 GB 3230000\n",
      "# writing shards/train-324.tar 10000 0.0 GB 3240000\n",
      "# writing shards/train-325.tar 10000 0.0 GB 3250000\n",
      "# writing shards/train-326.tar 10000 0.0 GB 3260000\n",
      "# writing shards/train-327.tar 10000 0.0 GB 3270000\n",
      "# writing shards/train-328.tar 10000 0.0 GB 3280000\n",
      "# writing shards/train-329.tar 10000 0.0 GB 3290000\n",
      "3300000 written\n",
      "# writing shards/train-330.tar 10000 0.0 GB 3300000\n",
      "# writing shards/train-331.tar 10000 0.0 GB 3310000\n",
      "# writing shards/train-332.tar 10000 0.0 GB 3320000\n",
      "# writing shards/train-333.tar 10000 0.0 GB 3330000\n",
      "# writing shards/train-334.tar 10000 0.0 GB 3340000\n",
      "# writing shards/train-335.tar 10000 0.0 GB 3350000\n",
      "# writing shards/train-336.tar 10000 0.0 GB 3360000\n",
      "# writing shards/train-337.tar 10000 0.0 GB 3370000\n",
      "# writing shards/train-338.tar 10000 0.0 GB 3380000\n",
      "# writing shards/train-339.tar 10000 0.0 GB 3390000\n",
      "3400000 written\n",
      "# writing shards/train-340.tar 10000 0.0 GB 3400000\n",
      "# writing shards/train-341.tar 10000 0.0 GB 3410000\n",
      "# writing shards/train-342.tar 10000 0.0 GB 3420000\n",
      "# writing shards/train-343.tar 10000 0.0 GB 3430000\n",
      "# writing shards/train-344.tar 10000 0.0 GB 3440000\n",
      "# writing shards/train-345.tar 10000 0.0 GB 3450000\n",
      "# writing shards/train-346.tar 10000 0.0 GB 3460000\n",
      "# writing shards/train-347.tar 10000 0.0 GB 3470000\n",
      "# writing shards/train-348.tar 10000 0.0 GB 3480000\n",
      "# writing shards/train-349.tar 10000 0.0 GB 3490000\n",
      "3500000 written\n",
      "# writing shards/train-350.tar 10000 0.0 GB 3500000\n",
      "# writing shards/train-351.tar 10000 0.0 GB 3510000\n",
      "# writing shards/train-352.tar 10000 0.0 GB 3520000\n",
      "# writing shards/train-353.tar 10000 0.0 GB 3530000\n",
      "# writing shards/train-354.tar 10000 0.0 GB 3540000\n",
      "# writing shards/train-355.tar 10000 0.0 GB 3550000\n",
      "# writing shards/train-356.tar 10000 0.0 GB 3560000\n",
      "# writing shards/train-357.tar 10000 0.0 GB 3570000\n",
      "# writing shards/train-358.tar 10000 0.0 GB 3580000\n",
      "# writing shards/train-359.tar 10000 0.0 GB 3590000\n",
      "3600000 written\n",
      "# writing shards/train-360.tar 10000 0.0 GB 3600000\n",
      "# writing shards/train-361.tar 10000 0.0 GB 3610000\n",
      "# writing shards/train-362.tar 10000 0.0 GB 3620000\n",
      "# writing shards/train-363.tar 10000 0.0 GB 3630000\n",
      "# writing shards/train-364.tar 10000 0.0 GB 3640000\n",
      "# writing shards/train-365.tar 10000 0.0 GB 3650000\n",
      "# writing shards/train-366.tar 10000 0.0 GB 3660000\n",
      "# writing shards/train-367.tar 10000 0.0 GB 3670000\n",
      "# writing shards/train-368.tar 10000 0.0 GB 3680000\n",
      "# writing shards/train-369.tar 10000 0.0 GB 3690000\n",
      "3700000 written\n",
      "# writing shards/train-370.tar 10000 0.0 GB 3700000\n",
      "# writing shards/train-371.tar 10000 0.0 GB 3710000\n",
      "# writing shards/train-372.tar 10000 0.0 GB 3720000\n",
      "# writing shards/train-373.tar 10000 0.0 GB 3730000\n",
      "# writing shards/train-374.tar 10000 0.0 GB 3740000\n",
      "# writing shards/train-375.tar 10000 0.0 GB 3750000\n",
      "# writing shards/train-376.tar 10000 0.0 GB 3760000\n",
      "# writing shards/train-377.tar 10000 0.0 GB 3770000\n",
      "# writing shards/train-378.tar 10000 0.0 GB 3780000\n",
      "# writing shards/train-379.tar 10000 0.0 GB 3790000\n",
      "3800000 written\n",
      "# writing shards/train-380.tar 10000 0.0 GB 3800000\n",
      "# writing shards/train-381.tar 10000 0.0 GB 3810000\n",
      "# writing shards/train-382.tar 10000 0.0 GB 3820000\n",
      "# writing shards/train-383.tar 10000 0.0 GB 3830000\n",
      "# writing shards/train-384.tar 10000 0.0 GB 3840000\n",
      "# writing shards/train-385.tar 10000 0.0 GB 3850000\n",
      "# writing shards/train-386.tar 10000 0.0 GB 3860000\n",
      "# writing shards/train-387.tar 10000 0.0 GB 3870000\n",
      "# writing shards/train-388.tar 10000 0.0 GB 3880000\n",
      "# writing shards/train-389.tar 10000 0.0 GB 3890000\n",
      "3900000 written\n",
      "# writing shards/train-390.tar 10000 0.0 GB 3900000\n",
      "# writing shards/train-391.tar 10000 0.0 GB 3910000\n",
      "# writing shards/train-392.tar 10000 0.0 GB 3920000\n",
      "# writing shards/train-393.tar 10000 0.0 GB 3930000\n",
      "# writing shards/train-394.tar 10000 0.0 GB 3940000\n",
      "# writing shards/train-395.tar 10000 0.0 GB 3950000\n",
      "# writing shards/train-396.tar 10000 0.0 GB 3960000\n",
      "# writing shards/train-397.tar 10000 0.0 GB 3970000\n",
      "# writing shards/train-398.tar 10000 0.0 GB 3980000\n",
      "# writing shards/train-399.tar 10000 0.0 GB 3990000\n",
      "4000000 written\n",
      "# writing shards/train-400.tar 10000 0.0 GB 4000000\n",
      "# writing shards/train-401.tar 10000 0.0 GB 4010000\n",
      "# writing shards/train-402.tar 10000 0.0 GB 4020000\n",
      "# writing shards/train-403.tar 10000 0.0 GB 4030000\n",
      "# writing shards/train-404.tar 10000 0.0 GB 4040000\n",
      "# writing shards/train-405.tar 10000 0.0 GB 4050000\n",
      "# writing shards/train-406.tar 10000 0.0 GB 4060000\n",
      "# writing shards/train-407.tar 10000 0.0 GB 4070000\n",
      "# writing shards/train-408.tar 10000 0.0 GB 4080000\n",
      "# writing shards/train-409.tar 10000 0.0 GB 4090000\n",
      "4100000 written\n",
      "# writing shards/train-410.tar 10000 0.0 GB 4100000\n",
      "# writing shards/train-411.tar 10000 0.0 GB 4110000\n",
      "# writing shards/train-412.tar 10000 0.0 GB 4120000\n",
      "# writing shards/train-413.tar 10000 0.0 GB 4130000\n",
      "# writing shards/train-414.tar 10000 0.0 GB 4140000\n",
      "# writing shards/train-415.tar 10000 0.0 GB 4150000\n",
      "# writing shards/train-416.tar 10000 0.0 GB 4160000\n",
      "# writing shards/train-417.tar 10000 0.0 GB 4170000\n",
      "# writing shards/train-418.tar 10000 0.0 GB 4180000\n",
      "# writing shards/train-419.tar 10000 0.0 GB 4190000\n",
      "4200000 written\n",
      "# writing shards/train-420.tar 10000 0.0 GB 4200000\n",
      "# writing shards/train-421.tar 10000 0.0 GB 4210000\n",
      "# writing shards/train-422.tar 10000 0.0 GB 4220000\n",
      "# writing shards/train-423.tar 10000 0.0 GB 4230000\n",
      "# writing shards/train-424.tar 10000 0.0 GB 4240000\n",
      "# writing shards/train-425.tar 10000 0.0 GB 4250000\n",
      "# writing shards/train-426.tar 10000 0.0 GB 4260000\n",
      "# writing shards/train-427.tar 10000 0.0 GB 4270000\n",
      "# writing shards/train-428.tar 10000 0.0 GB 4280000\n",
      "# writing shards/train-429.tar 10000 0.0 GB 4290000\n",
      "4300000 written\n",
      "# writing shards/train-430.tar 10000 0.0 GB 4300000\n",
      "# writing shards/train-431.tar 10000 0.0 GB 4310000\n",
      "# writing shards/train-432.tar 10000 0.0 GB 4320000\n",
      "# writing shards/train-433.tar 10000 0.0 GB 4330000\n",
      "# writing shards/train-434.tar 10000 0.0 GB 4340000\n",
      "# writing shards/train-435.tar 10000 0.0 GB 4350000\n",
      "# writing shards/train-436.tar 10000 0.0 GB 4360000\n",
      "# writing shards/train-437.tar 10000 0.0 GB 4370000\n",
      "# writing shards/train-438.tar 10000 0.0 GB 4380000\n",
      "# writing shards/train-439.tar 10000 0.0 GB 4390000\n",
      "4400000 written\n",
      "# writing shards/train-440.tar 10000 0.0 GB 4400000\n",
      "# writing shards/train-441.tar 10000 0.0 GB 4410000\n",
      "# writing shards/train-442.tar 10000 0.0 GB 4420000\n",
      "# writing shards/train-443.tar 10000 0.0 GB 4430000\n",
      "# writing shards/train-444.tar 10000 0.0 GB 4440000\n",
      "# writing shards/train-445.tar 10000 0.0 GB 4450000\n",
      "# writing shards/train-446.tar 10000 0.0 GB 4460000\n",
      "# writing shards/train-447.tar 10000 0.0 GB 4470000\n",
      "# writing shards/train-448.tar 10000 0.0 GB 4480000\n",
      "# writing shards/train-449.tar 10000 0.0 GB 4490000\n",
      "4500000 written\n",
      "# writing shards/train-450.tar 10000 0.0 GB 4500000\n",
      "# writing shards/train-451.tar 10000 0.0 GB 4510000\n",
      "# writing shards/train-452.tar 10000 0.0 GB 4520000\n",
      "# writing shards/train-453.tar 10000 0.0 GB 4530000\n",
      "# writing shards/train-454.tar 10000 0.0 GB 4540000\n",
      "# writing shards/train-455.tar 10000 0.0 GB 4550000\n",
      "# writing shards/train-456.tar 10000 0.0 GB 4560000\n",
      "# writing shards/train-457.tar 10000 0.0 GB 4570000\n",
      "# writing shards/train-458.tar 10000 0.0 GB 4580000\n",
      "# writing shards/train-459.tar 10000 0.0 GB 4590000\n",
      "4600000 written\n",
      "# writing shards/train-460.tar 10000 0.0 GB 4600000\n",
      "# writing shards/train-461.tar 10000 0.0 GB 4610000\n",
      "# writing shards/train-462.tar 10000 0.0 GB 4620000\n",
      "# writing shards/train-463.tar 10000 0.0 GB 4630000\n",
      "# writing shards/train-464.tar 10000 0.0 GB 4640000\n",
      "# writing shards/train-465.tar 10000 0.0 GB 4650000\n",
      "# writing shards/train-466.tar 10000 0.0 GB 4660000\n",
      "# writing shards/train-467.tar 10000 0.0 GB 4670000\n",
      "# writing shards/train-468.tar 10000 0.0 GB 4680000\n",
      "# writing shards/train-469.tar 10000 0.0 GB 4690000\n",
      "4700000 written\n",
      "# writing shards/train-470.tar 10000 0.0 GB 4700000\n",
      "# writing shards/train-471.tar 10000 0.0 GB 4710000\n",
      "# writing shards/train-472.tar 10000 0.0 GB 4720000\n",
      "# writing shards/train-473.tar 10000 0.0 GB 4730000\n",
      "# writing shards/train-474.tar 10000 0.0 GB 4740000\n",
      "# writing shards/train-475.tar 10000 0.0 GB 4750000\n",
      "# writing shards/train-476.tar 10000 0.0 GB 4760000\n",
      "# writing shards/train-477.tar 10000 0.0 GB 4770000\n",
      "# writing shards/train-478.tar 10000 0.0 GB 4780000\n",
      "# writing shards/train-479.tar 10000 0.0 GB 4790000\n",
      "4800000 written\n",
      "# writing shards/train-480.tar 10000 0.0 GB 4800000\n",
      "# writing shards/train-481.tar 10000 0.0 GB 4810000\n",
      "# writing shards/train-482.tar 10000 0.0 GB 4820000\n",
      "# writing shards/train-483.tar 10000 0.0 GB 4830000\n",
      "# writing shards/train-484.tar 10000 0.0 GB 4840000\n",
      "# writing shards/train-485.tar 10000 0.0 GB 4850000\n",
      "# writing shards/train-486.tar 10000 0.0 GB 4860000\n",
      "# writing shards/train-487.tar 10000 0.0 GB 4870000\n",
      "# writing shards/train-488.tar 10000 0.0 GB 4880000\n",
      "# writing shards/train-489.tar 10000 0.0 GB 4890000\n",
      "4900000 written\n",
      "# writing shards/train-490.tar 10000 0.0 GB 4900000\n",
      "# writing shards/train-491.tar 10000 0.0 GB 4910000\n",
      "# writing shards/train-492.tar 10000 0.0 GB 4920000\n",
      "# writing shards/train-493.tar 10000 0.0 GB 4930000\n",
      "# writing shards/train-494.tar 10000 0.0 GB 4940000\n",
      "# writing shards/train-495.tar 10000 0.0 GB 4950000\n",
      "# writing shards/train-496.tar 10000 0.0 GB 4960000\n",
      "# writing shards/train-497.tar 10000 0.0 GB 4970000\n",
      "# writing shards/train-498.tar 10000 0.0 GB 4980000\n",
      "# writing shards/train-499.tar 10000 0.0 GB 4990000\n",
      "5000000 written\n",
      "# writing shards/train-500.tar 10000 0.0 GB 5000000\n",
      "# writing shards/train-501.tar 10000 0.0 GB 5010000\n",
      "# writing shards/train-502.tar 10000 0.0 GB 5020000\n",
      "# writing shards/train-503.tar 10000 0.0 GB 5030000\n",
      "# writing shards/train-504.tar 10000 0.0 GB 5040000\n",
      "# writing shards/train-505.tar 10000 0.0 GB 5050000\n",
      "# writing shards/train-506.tar 10000 0.0 GB 5060000\n",
      "# writing shards/train-507.tar 10000 0.0 GB 5070000\n",
      "# writing shards/train-508.tar 10000 0.0 GB 5080000\n",
      "# writing shards/train-509.tar 10000 0.0 GB 5090000\n",
      "5100000 written\n",
      "# writing shards/train-510.tar 10000 0.0 GB 5100000\n",
      "# writing shards/train-511.tar 10000 0.0 GB 5110000\n",
      "# writing shards/train-512.tar 10000 0.0 GB 5120000\n",
      "# writing shards/train-513.tar 10000 0.0 GB 5130000\n",
      "# writing shards/train-514.tar 10000 0.0 GB 5140000\n",
      "# writing shards/train-515.tar 10000 0.0 GB 5150000\n",
      "# writing shards/train-516.tar 10000 0.0 GB 5160000\n",
      "# writing shards/train-517.tar 10000 0.0 GB 5170000\n",
      "# writing shards/train-518.tar 10000 0.0 GB 5180000\n",
      "# writing shards/train-519.tar 10000 0.0 GB 5190000\n",
      "5200000 written\n",
      "# writing shards/train-520.tar 10000 0.0 GB 5200000\n",
      "# writing shards/train-521.tar 10000 0.0 GB 5210000\n",
      "# writing shards/train-522.tar 10000 0.0 GB 5220000\n",
      "# writing shards/train-523.tar 10000 0.0 GB 5230000\n",
      "# writing shards/train-524.tar 10000 0.0 GB 5240000\n",
      "# writing shards/train-525.tar 10000 0.0 GB 5250000\n",
      "# writing shards/train-526.tar 10000 0.0 GB 5260000\n",
      "# writing shards/train-527.tar 10000 0.0 GB 5270000\n",
      "# writing shards/train-528.tar 10000 0.0 GB 5280000\n",
      "# writing shards/train-529.tar 10000 0.0 GB 5290000\n",
      "5300000 written\n",
      "# writing shards/train-530.tar 10000 0.0 GB 5300000\n",
      "# writing shards/train-531.tar 10000 0.0 GB 5310000\n",
      "# writing shards/train-532.tar 10000 0.0 GB 5320000\n",
      "# writing shards/train-533.tar 10000 0.0 GB 5330000\n",
      "# writing shards/train-534.tar 10000 0.0 GB 5340000\n",
      "# writing shards/train-535.tar 10000 0.0 GB 5350000\n",
      "# writing shards/train-536.tar 10000 0.0 GB 5360000\n",
      "# writing shards/train-537.tar 10000 0.0 GB 5370000\n",
      "# writing shards/train-538.tar 10000 0.0 GB 5380000\n",
      "# writing shards/train-539.tar 10000 0.0 GB 5390000\n"
     ]
    }
   ],
   "source": [
    "writeWDS(dataset_train, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing shards/val-00.tar 0 0.0 GB 0\n",
      "# writing shards/val-01.tar 10000 0.0 GB 10000\n",
      "# writing shards/val-02.tar 10000 0.0 GB 20000\n",
      "# writing shards/val-03.tar 10000 0.0 GB 30000\n",
      "# writing shards/val-04.tar 10000 0.0 GB 40000\n",
      "# writing shards/val-05.tar 10000 0.0 GB 50000\n",
      "# writing shards/val-06.tar 10000 0.0 GB 60000\n",
      "# writing shards/val-07.tar 10000 0.0 GB 70000\n",
      "# writing shards/val-08.tar 10000 0.0 GB 80000\n",
      "# writing shards/val-09.tar 10000 0.0 GB 90000\n",
      "100000 written\n",
      "# writing shards/val-10.tar 10000 0.0 GB 100000\n",
      "# writing shards/val-11.tar 10000 0.0 GB 110000\n",
      "# writing shards/val-12.tar 10000 0.0 GB 120000\n",
      "# writing shards/val-13.tar 10000 0.0 GB 130000\n",
      "# writing shards/val-14.tar 10000 0.0 GB 140000\n",
      "# writing shards/val-15.tar 10000 0.0 GB 150000\n",
      "# writing shards/val-16.tar 10000 0.0 GB 160000\n",
      "# writing shards/val-17.tar 10000 0.0 GB 170000\n",
      "# writing shards/val-18.tar 10000 0.0 GB 180000\n",
      "# writing shards/val-19.tar 10000 0.0 GB 190000\n",
      "200000 written\n",
      "# writing shards/val-20.tar 10000 0.0 GB 200000\n",
      "# writing shards/val-21.tar 10000 0.0 GB 210000\n",
      "# writing shards/val-22.tar 10000 0.0 GB 220000\n",
      "# writing shards/val-23.tar 10000 0.0 GB 230000\n",
      "# writing shards/val-24.tar 10000 0.0 GB 240000\n",
      "# writing shards/val-25.tar 10000 0.0 GB 250000\n",
      "# writing shards/val-26.tar 10000 0.0 GB 260000\n",
      "# writing shards/val-27.tar 10000 0.0 GB 270000\n",
      "# writing shards/val-28.tar 10000 0.0 GB 280000\n",
      "# writing shards/val-29.tar 10000 0.0 GB 290000\n",
      "300000 written\n",
      "# writing shards/val-30.tar 10000 0.0 GB 300000\n",
      "# writing shards/val-31.tar 10000 0.0 GB 310000\n",
      "# writing shards/val-32.tar 10000 0.0 GB 320000\n",
      "# writing shards/val-33.tar 10000 0.0 GB 330000\n",
      "# writing shards/val-34.tar 10000 0.0 GB 340000\n",
      "# writing shards/val-35.tar 10000 0.0 GB 350000\n",
      "# writing shards/val-36.tar 10000 0.0 GB 360000\n",
      "# writing shards/val-37.tar 10000 0.0 GB 370000\n",
      "# writing shards/val-38.tar 10000 0.0 GB 380000\n",
      "# writing shards/val-39.tar 10000 0.0 GB 390000\n",
      "400000 written\n",
      "# writing shards/val-40.tar 10000 0.0 GB 400000\n",
      "# writing shards/val-41.tar 10000 0.0 GB 410000\n",
      "# writing shards/val-42.tar 10000 0.0 GB 420000\n",
      "# writing shards/val-43.tar 10000 0.0 GB 430000\n",
      "# writing shards/val-44.tar 10000 0.0 GB 440000\n",
      "# writing shards/val-45.tar 10000 0.0 GB 450000\n",
      "# writing shards/val-46.tar 10000 0.0 GB 460000\n",
      "# writing shards/val-47.tar 10000 0.0 GB 470000\n",
      "# writing shards/val-48.tar 10000 0.0 GB 480000\n",
      "# writing shards/val-49.tar 10000 0.0 GB 490000\n",
      "500000 written\n",
      "# writing shards/val-50.tar 10000 0.0 GB 500000\n",
      "# writing shards/val-51.tar 10000 0.0 GB 510000\n",
      "# writing shards/val-52.tar 10000 0.0 GB 520000\n",
      "# writing shards/val-53.tar 10000 0.0 GB 530000\n",
      "# writing shards/val-54.tar 10000 0.0 GB 540000\n",
      "# writing shards/val-55.tar 10000 0.0 GB 550000\n",
      "# writing shards/val-56.tar 10000 0.0 GB 560000\n",
      "# writing shards/val-57.tar 10000 0.0 GB 570000\n",
      "# writing shards/val-58.tar 10000 0.0 GB 580000\n",
      "# writing shards/val-59.tar 10000 0.0 GB 590000\n",
      "600000 written\n",
      "# writing shards/val-60.tar 10000 0.0 GB 600000\n",
      "# writing shards/val-61.tar 10000 0.0 GB 610000\n",
      "# writing shards/val-62.tar 10000 0.0 GB 620000\n",
      "# writing shards/val-63.tar 10000 0.0 GB 630000\n",
      "# writing shards/val-64.tar 10000 0.0 GB 640000\n",
      "# writing shards/val-65.tar 10000 0.0 GB 650000\n",
      "# writing shards/val-66.tar 10000 0.0 GB 660000\n",
      "# writing shards/val-67.tar 10000 0.0 GB 670000\n"
     ]
    }
   ],
   "source": [
    "writeWDS(dataset_val, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing shards/test-00.tar 0 0.0 GB 0\n",
      "# writing shards/test-01.tar 10000 0.0 GB 10000\n",
      "# writing shards/test-02.tar 10000 0.0 GB 20000\n",
      "# writing shards/test-03.tar 10000 0.0 GB 30000\n",
      "# writing shards/test-04.tar 10000 0.0 GB 40000\n",
      "# writing shards/test-05.tar 10000 0.0 GB 50000\n",
      "# writing shards/test-06.tar 10000 0.0 GB 60000\n",
      "# writing shards/test-07.tar 10000 0.0 GB 70000\n",
      "# writing shards/test-08.tar 10000 0.0 GB 80000\n",
      "# writing shards/test-09.tar 10000 0.0 GB 90000\n",
      "100000 written\n",
      "# writing shards/test-10.tar 10000 0.0 GB 100000\n",
      "# writing shards/test-11.tar 10000 0.0 GB 110000\n",
      "# writing shards/test-12.tar 10000 0.0 GB 120000\n",
      "# writing shards/test-13.tar 10000 0.0 GB 130000\n",
      "# writing shards/test-14.tar 10000 0.0 GB 140000\n",
      "# writing shards/test-15.tar 10000 0.0 GB 150000\n",
      "# writing shards/test-16.tar 10000 0.0 GB 160000\n",
      "# writing shards/test-17.tar 10000 0.0 GB 170000\n",
      "# writing shards/test-18.tar 10000 0.0 GB 180000\n",
      "# writing shards/test-19.tar 10000 0.0 GB 190000\n",
      "200000 written\n",
      "# writing shards/test-20.tar 10000 0.0 GB 200000\n",
      "# writing shards/test-21.tar 10000 0.0 GB 210000\n",
      "# writing shards/test-22.tar 10000 0.0 GB 220000\n",
      "# writing shards/test-23.tar 10000 0.0 GB 230000\n",
      "# writing shards/test-24.tar 10000 0.0 GB 240000\n",
      "# writing shards/test-25.tar 10000 0.0 GB 250000\n",
      "# writing shards/test-26.tar 10000 0.0 GB 260000\n",
      "# writing shards/test-27.tar 10000 0.0 GB 270000\n",
      "# writing shards/test-28.tar 10000 0.0 GB 280000\n",
      "# writing shards/test-29.tar 10000 0.0 GB 290000\n",
      "300000 written\n",
      "# writing shards/test-30.tar 10000 0.0 GB 300000\n",
      "# writing shards/test-31.tar 10000 0.0 GB 310000\n",
      "# writing shards/test-32.tar 10000 0.0 GB 320000\n",
      "# writing shards/test-33.tar 10000 0.0 GB 330000\n",
      "# writing shards/test-34.tar 10000 0.0 GB 340000\n",
      "# writing shards/test-35.tar 10000 0.0 GB 350000\n",
      "# writing shards/test-36.tar 10000 0.0 GB 360000\n",
      "# writing shards/test-37.tar 10000 0.0 GB 370000\n",
      "# writing shards/test-38.tar 10000 0.0 GB 380000\n",
      "# writing shards/test-39.tar 10000 0.0 GB 390000\n",
      "400000 written\n",
      "# writing shards/test-40.tar 10000 0.0 GB 400000\n",
      "# writing shards/test-41.tar 10000 0.0 GB 410000\n",
      "# writing shards/test-42.tar 10000 0.0 GB 420000\n",
      "# writing shards/test-43.tar 10000 0.0 GB 430000\n",
      "# writing shards/test-44.tar 10000 0.0 GB 440000\n",
      "# writing shards/test-45.tar 10000 0.0 GB 450000\n",
      "# writing shards/test-46.tar 10000 0.0 GB 460000\n",
      "# writing shards/test-47.tar 10000 0.0 GB 470000\n",
      "# writing shards/test-48.tar 10000 0.0 GB 480000\n",
      "# writing shards/test-49.tar 10000 0.0 GB 490000\n",
      "500000 written\n",
      "# writing shards/test-50.tar 10000 0.0 GB 500000\n",
      "# writing shards/test-51.tar 10000 0.0 GB 510000\n",
      "# writing shards/test-52.tar 10000 0.0 GB 520000\n",
      "# writing shards/test-53.tar 10000 0.0 GB 530000\n",
      "# writing shards/test-54.tar 10000 0.0 GB 540000\n",
      "# writing shards/test-55.tar 10000 0.0 GB 550000\n",
      "# writing shards/test-56.tar 10000 0.0 GB 560000\n",
      "# writing shards/test-57.tar 10000 0.0 GB 570000\n",
      "# writing shards/test-58.tar 10000 0.0 GB 580000\n",
      "# writing shards/test-59.tar 10000 0.0 GB 590000\n",
      "600000 written\n",
      "# writing shards/test-60.tar 10000 0.0 GB 600000\n",
      "# writing shards/test-61.tar 10000 0.0 GB 610000\n",
      "# writing shards/test-62.tar 10000 0.0 GB 620000\n",
      "# writing shards/test-63.tar 10000 0.0 GB 630000\n",
      "# writing shards/test-64.tar 10000 0.0 GB 640000\n",
      "# writing shards/test-65.tar 10000 0.0 GB 650000\n",
      "# writing shards/test-66.tar 10000 0.0 GB 660000\n",
      "# writing shards/test-67.tar 10000 0.0 GB 670000\n"
     ]
    }
   ],
   "source": [
    "writeWDS(dataset_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predict = pd.read_table(\"./data/test_sequences.txt\", header=None, names=['seq', 'target'], dtype={'seq':str, 'target':np.float32})\n",
    "dataset_predict = DreamChallengeDataset(df_predict, transform=DNA2OneHot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing shards/pred-00.tar 0 0.0 GB 0\n",
      "# writing shards/pred-01.tar 10000 0.0 GB 10000\n",
      "# writing shards/pred-02.tar 10000 0.0 GB 20000\n",
      "# writing shards/pred-03.tar 10000 0.0 GB 30000\n",
      "# writing shards/pred-04.tar 10000 0.0 GB 40000\n",
      "# writing shards/pred-05.tar 10000 0.0 GB 50000\n",
      "# writing shards/pred-06.tar 10000 0.0 GB 60000\n",
      "# writing shards/pred-07.tar 10000 0.0 GB 70000\n"
     ]
    }
   ],
   "source": [
    "writeWDS(dataset_predict, \"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmy5455/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-06-21 02:56:24.848661: E tensorflow/core/framework/op_kernel.cc:1676] OpKernel ('op: \"TPURoundRobin\" device_type: \"CPU\"') for unknown op: TPURoundRobin\n",
      "2022-06-21 02:56:24.848768: E tensorflow/core/framework/op_kernel.cc:1676] OpKernel ('op: \"TpuHandleToProtoKey\" device_type: \"CPU\"') for unknown op: TpuHandleToProtoKey\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jmy5455/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/xla_device.py\", line 32, in inner_f\n",
      "    queue.put(func(*args, **kwargs))\n",
      "  File \"/home/jmy5455/.local/lib/python3.8/site-packages/pytorch_lightning/utilities/xla_device.py\", line 73, in _is_device_tpu\n",
      "    return (xm.xrt_world_size() > 1) or bool(xm.get_xla_supported_devices(\"TPU\"))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 137, in get_xla_supported_devices\n",
      "    xla_devices = _DEVICES.value\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/utils/utils.py\", line 32, in value\n",
      "    self._value = self._gen_fn()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch_xla/core/xla_model.py\", line 19, in <lambda>\n",
      "    _DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\n",
      "RuntimeError: tensorflow/compiler/xla/xla_client/xrt_local_service.cc:56 : Check failed: tensorflow::NewServer(server_def, &server_) == ::tensorflow::Status::OK() (INVALID_ARGUMENT: Invalid fd: -1; Couldn't open device: /dev/accel0 (Operation not permitted); Unable to create Node RegisterInterface for node 0, config: device_path: \"/dev/accel0\" mode: KERNEL debug_data_directory: \"\" dump_anomalies_only: true crash_in_debug_dump: false allow_core_dump: true; could not create driver instance vs. OK)\n",
      "2022-06-21 02:56:25.079500: E tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:566] INVALID_ARGUMENT: Invalid fd: -1; Couldn't open device: /dev/accel0 (Operation not permitted); Unable to create Node RegisterInterface for node 0, config: device_path: \"/dev/accel0\" mode: KERNEL debug_data_directory: \"\" dump_anomalies_only: true crash_in_debug_dump: false allow_core_dump: true; could not create driver instance\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataloader.py\n",
    "\n",
    "# lightning data module\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.utilities import cli as pl_cli\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "import webdataset as wds\n",
    "\n",
    "class DNA2OneHot(object):\n",
    "    def __init__(self, expect_len=110, rev=False):\n",
    "        self.expect_len = expect_len\n",
    "        self.rev = rev\n",
    "        self.DNA2Index = {\n",
    "            \"A\": 0,\n",
    "            \"C\": 1,\n",
    "            \"G\": 2,\n",
    "            \"T\": 3\n",
    "        }\n",
    "    \n",
    "    def __call__(self, dnaSeq):\n",
    "        # initialize the matrix as 4 x self.expect_len\n",
    "        seqMatrixs = np.zeros((4, self.expect_len), dtype=np.float32)\n",
    "        # change the value to matrix\n",
    "        seqLen = len(dnaSeq)\n",
    "        dnaSeq = self.rev_comp(dnaSeq.upper()) if self.rev else dnaSeq.upper()\n",
    "        for j in range(0, min(seqLen, self.expect_len)):\n",
    "            try:\n",
    "                seqMatrixs[self.DNA2Index[dnaSeq[j]], j] = 1\n",
    "            except KeyError as e:\n",
    "                continue\n",
    "        return seqMatrixs\n",
    "\n",
    "class DreamChallengeDataset(Dataset):\n",
    "    def __init__(self, df=None, transform=None, target_transform=None):\n",
    "        self.seq = df.seq\n",
    "        self.target = df.target\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.seq)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.seq.iloc[idx]\n",
    "        target = self.target.iloc[idx][np.newaxis]\n",
    "        if self.transform:\n",
    "            seq = self.transform(seq)\n",
    "        if self.target_transform:\n",
    "            target = self.target_transform(target)\n",
    "        return seq, target\n",
    "\n",
    "    def rev_comp(self, inp_str):\n",
    "        rc_dict = {'A': 'T', 'G': 'C', 'T': 'A', 'C': 'G', 'c': 'g',\n",
    "                   'g': 'c', 't': 'a', 'a': 't', 'n': 'n', 'N': 'N'}\n",
    "        outp_str = list()\n",
    "        for nucl in inp_str:\n",
    "            outp_str.append(rc_dict[nucl])\n",
    "        return ''.join(outp_str)[::-1]    \n",
    "\n",
    "def npy_decoder(key, value):\n",
    "    if not key.endswith(\".npy\"):\n",
    "        return None\n",
    "    assert isinstance(value, bytes)\n",
    "    return np.load(io.BytesIO(value))\n",
    "\n",
    "def worker_splitter(urls):\n",
    "    urls = [url for url in urls]\n",
    "    assert isinstance(urls, list)\n",
    "\n",
    "    worker_info = torch.utils.data.get_worker_info()\n",
    "    if worker_info is not None:\n",
    "        wid = worker_info.id\n",
    "        num_workers = worker_info.num_workers\n",
    "\n",
    "        return iter(urls[wid::num_workers])\n",
    "    else:\n",
    "        return iter(urls)\n",
    "\n",
    "def node_splitter(urls):\n",
    "    urls = [url for url in urls]\n",
    "\n",
    "    rank = xm.get_ordinal()\n",
    "    num_replicas = xm.xrt_world_size()\n",
    "\n",
    "    urls_this = urls[rank::num_replicas]\n",
    "\n",
    "    return iter(urls_this)\n",
    "\n",
    "@pl_cli.DATAMODULE_REGISTRY\n",
    "class DreamChallengeDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str=\"./shards/\", batch_size=512, num_workers=6):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = DNA2OneHot()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.description = {\"seq\": \"float\", \"target\": \"float\"}\n",
    "\n",
    "        self.dataset_train = None\n",
    "        self.dataset_val = None\n",
    "        self.dataset_test = None\n",
    "        self.dataset_predict = None\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # Assume data already in self.data_dir\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            num_data_instances = xm.xrt_world_size() * self.num_workers\n",
    "            print(f\"Device {xm.get_ordinal()} in world size of {xm.xrt_world_size()}\")\n",
    "\n",
    "            self.dataset_train = wds.DataPipeline(\n",
    "                wds.ResampledShards(\"shards/train-{00..539}.tar\"),\n",
    "                #node_splitter,\n",
    "                worker_splitter,\n",
    "                wds.tarfile_to_samples(),\n",
    "                wds.decode(npy_decoder),\n",
    "                wds.to_tuple(\"seq.npy\", \"target.npy\"),\n",
    "                wds.batched(self.batch_size, partial=False)\n",
    "                ).with_epoch(5391406//num_data_instances//self.batch_size)\n",
    "\n",
    "            self.dataset_val = wds.DataPipeline(\n",
    "                wds.SimpleShardList(\"shards/val-{00..67}.tar\"),\n",
    "                node_splitter,\n",
    "                worker_splitter,\n",
    "                wds.tarfile_to_samples(),\n",
    "                wds.decode(npy_decoder),\n",
    "                wds.to_tuple(\"seq.npy\", \"target.npy\"),\n",
    "                wds.batched(self.batch_size, partial=False)\n",
    "                )\n",
    "            #.with_length(673926//num_data_instances)\n",
    "\n",
    "            self.dataset_val = wds.DataPipeline(\n",
    "                wds.SimpleShardList(\"shards/test-{00..67}.tar\"),\n",
    "                node_splitter,\n",
    "                worker_splitter,\n",
    "                wds.tarfile_to_samples(),\n",
    "                wds.decode(npy_decoder),\n",
    "                wds.to_tuple(\"seq.npy\", \"target.npy\"),\n",
    "                wds.batched(self.batch_size, partial=False)\n",
    "                )\n",
    "            #.with_length(673926)\n",
    "\n",
    "            self.dataset_pred = wds.DataPipeline(\n",
    "                wds.SimpleShardList(\"shards/pred-{00-07}.tar\"),\n",
    "                wds.tarfile_to_samples(),\n",
    "                wds.decode(npy_decoder),\n",
    "                wds.to_tuple(\"seq.npy\", \"target.npy\"),\n",
    "                wds.batched(self.batch_size, partial=False)\n",
    "            )\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.dataset_train, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.dataset_val, num_workers=self.num_workers)\n",
    "        \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.dataset_test, num_workers=self.num_workers)\n",
    "    \n",
    "    def pred_dataloader(self):\n",
    "        return DataLoader(self.dataset_predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in wds.WebLoader(dataset, batch_size=4): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 4, 110])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DreamChallengeDataModule(transform=DNA2OneHot())\n",
    "ds.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "ds_train = ds.train_dataloader()\n",
    "targets = []\n",
    "for seq, target in ds_train:\n",
    "    targets.append(target)\n",
    "targets = torch.cat(targets)\n",
    "# plot histogram on targets\n",
    "targets.numpy()\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = sns.histplot(x=targets, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# val\n",
    "ds_val = ds.val_dataloader()\n",
    "targets = []\n",
    "for seq, target in ds_val:\n",
    "    targets.append(target)\n",
    "targets = torch.cat(targets)\n",
    "# plot histogram on targets\n",
    "targets.numpy()\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = sns.histplot(x=targets, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test\n",
    "ds_test = ds.test_dataloader()\n",
    "targets = []\n",
    "for seq, target in ds_test:\n",
    "    targets.append(target)\n",
    "targets = torch.cat(targets)\n",
    "# plot histogram on targets\n",
    "targets.numpy()\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = sns.histplot(x=targets, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightning training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting lightning_dreamchallenge.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile lightning_dreamchallenge.py\n",
    "\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from pytorch_lightning.utilities.cli import LightningCLI\n",
    "from pytorch_lightning.utilities import cli as pl_cli\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from dataloader import DreamChallengeDataModule\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.flatten(x, start_dim=1)\n",
    "\n",
    "class NNHooks(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        seq, y = batch\n",
    "        y_hat = self(seq)\n",
    "\n",
    "        # compute loss\n",
    "        loss = F.mse_loss(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        seq, y = batch\n",
    "        y_hat = self(seq)\n",
    "\n",
    "        # compute loss\n",
    "        val_loss = F.mse_loss(y_hat, y)\n",
    "        self.log('val_loss', val_loss)\n",
    "        return {'val_loss': val_loss}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        seq, y = batch\n",
    "        y_hat = self(seq)\n",
    "\n",
    "        # compute loss\n",
    "        test_loss = F.mse_loss(y_hat, y)\n",
    "        self.log('test_loss', test_loss)\n",
    "        return {'test_loss': test_loss}\n",
    "\n",
    "@pl_cli.MODEL_REGISTRY\n",
    "class ConvolutionalModel(NNHooks):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_strand_specific_forward = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_strand_specific_reverse = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_body = nn.Sequential(OrderedDict([\n",
    "            ('conv3', nn.Conv1d(512, 256, 31, padding=15)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('conv4', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('faltten', Flatten()),\n",
    "            ('dense1', nn.Linear(110*256, 256)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2)),\n",
    "            ('dense2', nn.Linear(256, 256)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2))\n",
    "        ]))\n",
    "        self.model_head = nn.Sequential(OrderedDict([\n",
    "            ('dense_head', nn.Linear(256, 1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, seq):\n",
    "        seq_revcomp = torch.flip(seq.detach().clone(), [1, 2])\n",
    "        y_hat_for = self.model_strand_specific_forward(seq)\n",
    "        y_hat_rev = self.model_strand_specific_reverse(seq_revcomp)\n",
    "        y_hat = torch.cat([y_hat_for, y_hat_rev], dim=1)\n",
    "        y_hat = self.model_body(y_hat)\n",
    "        y_hat = self.model_head(y_hat)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "class MyLightningCLI(LightningCLI):\n",
    "    def add_arguments_to_parser(self, parser):\n",
    "        parser.add_optimizer_args(torch.optim.Adam)\n",
    "\n",
    "    def after_fit(self):\n",
    "        self.trainer.test(ckpt_path='best')\n",
    "\n",
    "def main():\n",
    "    cli = MyLightningCLI(auto_registry=True, save_config_overwrite=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting native_pytorch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile native_pytorch.py\n",
    "\n",
    "import sys\n",
    "import yaml\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "\n",
    "from dataloader import DreamChallengeDataModule\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ConvolutionalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalModel, self).__init__()\n",
    "\n",
    "        self.model_strand_specific_forward = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_strand_specific_reverse = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_body = nn.Sequential(OrderedDict([\n",
    "            ('conv3', nn.Conv1d(512, 256, 31, padding=15)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('conv4', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('faltten', nn.Flatten()),\n",
    "            ('dense1', nn.Linear(110*256, 256)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2)),\n",
    "            ('dense2', nn.Linear(256, 256)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2))\n",
    "        ]))\n",
    "        self.model_head = nn.Sequential(OrderedDict([\n",
    "            ('dense_head', nn.Linear(256, 1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, seq):\n",
    "        seq_revcomp = torch.flip(seq.detach().clone(), [1, 2])\n",
    "        y_hat_for = self.model_strand_specific_forward(seq)\n",
    "        y_hat_rev = self.model_strand_specific_reverse(seq_revcomp)\n",
    "        y_hat = torch.cat([y_hat_for, y_hat_rev], dim=1)\n",
    "        y_hat = self.model_body(y_hat)\n",
    "        y_hat = self.model_head(y_hat)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    num_batches = len(dataloader)\n",
    "    pbar = tqdm(enumerate(dataloader))\n",
    "    for batch, (X, y) in pbar:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        xm.mark_step()\n",
    "\n",
    "        pbar.set_description(f\"Training loss: {loss:>7f}  [{batch:>5d}/{num_batches:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    model = model.to(device)\n",
    "\n",
    "    metric = torchmetrics.MeanSquaredError()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            mse = metric(pred, y)\n",
    "\n",
    "    mse = metric.compute()\n",
    "    print(f\"MSE on all data: {mse}\") \n",
    "\n",
    "def main():\n",
    "    # load config\n",
    "    config = yaml.safe_load(open(sys.argv[1], \"r\"))\n",
    "\n",
    "    device = xm.xla_device()\n",
    "    print(f\"XLA device: {device}\")\n",
    "\n",
    "    model = ConvolutionalModel()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"optimizer\"][\"lr\"])\n",
    "\n",
    "    datamodule = DreamChallengeDataModule(batch_size=config[\"data\"][\"init_args\"][\"batch_size\"], num_workers=config[\"data\"][\"init_args\"][\"num_workers\"])\n",
    "    datamodule.setup()\n",
    "\n",
    "    epochs = 3\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------\")\n",
    "        train_loop(datamodule.train_dataloader(), model.train(), loss_fn, optimizer, device)\n",
    "        test_loop(datamodule.val_dataloader(), model.eval(), loss_fn, device)\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting args_parse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile args_parse.py\n",
    "\n",
    "# This module cannot import any other PyTorch/XLA module. Only Python core modules.\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def parse_common_options(datadir=None,\n",
    "                         logdir=None,\n",
    "                         num_cores=None,\n",
    "                         batch_size=128,\n",
    "                         num_epochs=10,\n",
    "                         num_workers=2,\n",
    "                         log_steps=20,\n",
    "                         lr=None,\n",
    "                         momentum=None,\n",
    "                         target_accuracy=None,\n",
    "                         profiler_port=9012,\n",
    "                         opts=None):\n",
    "  parser = argparse.ArgumentParser(add_help=True)\n",
    "  parser.add_argument('--datadir', type=str, default=datadir)\n",
    "  parser.add_argument('--logdir', type=str, default=logdir)\n",
    "  parser.add_argument('--num_cores', type=int, default=num_cores)\n",
    "  parser.add_argument('--batch_size', type=int, default=batch_size)\n",
    "  parser.add_argument('--num_epochs', type=int, default=num_epochs)\n",
    "  parser.add_argument('--num_workers', type=int, default=num_workers)\n",
    "  parser.add_argument('--log_steps', type=int, default=log_steps)\n",
    "  parser.add_argument('--profiler_port', type=int, default=profiler_port)\n",
    "  parser.add_argument('--lr', type=float, default=lr)\n",
    "  parser.add_argument('--momentum', type=float, default=momentum)\n",
    "  parser.add_argument('--target_accuracy', type=float, default=target_accuracy)\n",
    "  parser.add_argument('--drop_last', action='store_true')\n",
    "  parser.add_argument('--fake_data', action='store_true')\n",
    "  parser.add_argument('--tidy', action='store_true')\n",
    "  parser.add_argument('--metrics_debug', action='store_true')\n",
    "  parser.add_argument('--async_closures', action='store_true')\n",
    "  if opts:\n",
    "    for name, aopts in opts:\n",
    "      parser.add_argument(name, **aopts)\n",
    "  args, leftovers = parser.parse_known_args()\n",
    "  sys.argv = [sys.argv[0]] + leftovers\n",
    "  # Setup import folders.\n",
    "  xla_folder = os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))\n",
    "  sys.path.append(os.path.join(os.path.dirname(xla_folder), 'test'))\n",
    "  return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tpu_ddp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tpu_ddp.py\n",
    "\n",
    "import args_parse\n",
    "\n",
    "FLAGS = args_parse.parse_common_options(\n",
    "    log_steps=200,\n",
    "    datadir='./shards/',\n",
    "    logdir='./conv_model/',\n",
    "    batch_size=512,\n",
    "    momentum=0.5,\n",
    "    lr=0.001,\n",
    "    num_epochs=2,\n",
    "    num_workers=32)\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch_xla\n",
    "import torch_xla.debug.metrics as met\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.utils.utils as xu\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import webdataset as wds\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataloader import DreamChallengeDataModule\n",
    "from tqdm import tqdm\n",
    "from torchmetrics import MeanSquaredError\n",
    "\n",
    "class ConvolutionalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalModel, self).__init__()\n",
    "\n",
    "        self.model_strand_specific_forward = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_strand_specific_reverse = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv1d(4, 256, 31, padding=15)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('conv2', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu2', nn.ReLU())\n",
    "        ]))\n",
    "        self.model_body = nn.Sequential(OrderedDict([\n",
    "            ('conv3', nn.Conv1d(512, 256, 31, padding=15)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('conv4', nn.Conv1d(256, 256, 31, padding=15)),\n",
    "            ('relu4', nn.ReLU()),\n",
    "            ('faltten', nn.Flatten()),\n",
    "            ('dense1', nn.Linear(110*256, 256)),\n",
    "            ('relu5', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2)),\n",
    "            ('dense2', nn.Linear(256, 256)),\n",
    "            ('relu6', nn.ReLU()),\n",
    "            ('dropout1', nn.Dropout(0.2))\n",
    "        ]))\n",
    "        self.model_head = nn.Sequential(OrderedDict([\n",
    "            ('dense_head', nn.Linear(256, 1))\n",
    "        ]))\n",
    "\n",
    "        self.example_input = torch.zeros(512, 4, 500).index_fill_(1, torch.tensor(2), 1)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        seq_revcomp = torch.flip(seq.detach().clone(), [1, 2])\n",
    "        y_hat_for = self.model_strand_specific_forward(seq)\n",
    "        y_hat_rev = self.model_strand_specific_reverse(seq_revcomp)\n",
    "        y_hat = torch.cat([y_hat_for, y_hat_rev], dim=1)\n",
    "        y_hat = self.model_body(y_hat)\n",
    "        y_hat = self.model_head(y_hat)\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "def plot_scatter(x, y, writer, lim=20):\n",
    "  fig = plt.figure(12, 12)\n",
    "  ax = sns.scatterplot(x=x, y=y)\n",
    "  ax.set_xlim(left=0, right=lim)\n",
    "  ax.set_ylim(bottom=0, right=lim)\n",
    "  writer.add_figure(\"Prediction on test data\", fig)  \n",
    "\n",
    "def _train_update(device, x, loss, tracker, writer):\n",
    "  loss_item = loss.item()\n",
    "  test_utils.print_training_update(\n",
    "      device,\n",
    "      x,\n",
    "      loss_item,\n",
    "      tracker.rate(),\n",
    "      tracker.global_rate(),\n",
    "      summary_writer=writer)\n",
    "  test_utils.write_to_summary(\n",
    "      writer,\n",
    "      dict_to_write={\n",
    "        \"loss\": loss_item\n",
    "      })\n",
    "\n",
    "def train_model(flags, **kwargs):\n",
    "  torch.manual_seed(1)\n",
    "\n",
    "  datamodule = DreamChallengeDataModule(data_dir=flags.datadir, batch_size=flags.batch_size, num_workers=flags.num_workers)\n",
    "  datamodule.setup()\n",
    "  train_loader = datamodule.train_dataloader()\n",
    "  val_loader = datamodule.val_dataloader()\n",
    "\n",
    "  # Scale learning rate to num cores\n",
    "  lr = flags.lr * xm.xrt_world_size()\n",
    "\n",
    "  device = xm.xla_device()\n",
    "  model = ConvolutionalModel().to(device)\n",
    "  writer = None\n",
    "  if xm.is_master_ordinal():\n",
    "    writer = test_utils.get_summary_writer(flags.logdir)\n",
    "    writer.add_graph(model, model.example_input.to(device))\n",
    "  #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=flags.momentum)\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "  loss_fn = nn.MSELoss()\n",
    "\n",
    "  def train_loop_fn(epoch, loader):\n",
    "    tracker = xm.RateTracker()\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(loader))\n",
    "    for step, (data, target) in pbar:\n",
    "      optimizer.zero_grad()\n",
    "      data = torch.squeeze(data, 0)\n",
    "      target = torch.squeeze(target, 0)\n",
    "      output = model(data)\n",
    "      loss = loss_fn(output, target)\n",
    "      loss.backward()\n",
    "      xm.optimizer_step(optimizer)\n",
    "      tracker.add(flags.batch_size)\n",
    "      pbar.set_description(\"Loss %s\" % loss)\n",
    "      if step % flags.log_steps == 0:\n",
    "        xm.add_step_closure(\n",
    "          _train_update,\n",
    "          args=(device, step, loss, tracker, writer),\n",
    "          run_async=flags.async_closures\n",
    "        )\n",
    "    if xm.is_master_ordinal():\n",
    "      torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss.detach().cpu().numpy()\n",
    "      }, os.path.join(flags.logdir, f\"checkpoint-epoch{epoch}.ckpt\"))\n",
    "      \n",
    "  def val_loop_fn(loader):\n",
    "    model.eval()\n",
    "    metrics = MeanSquaredError()\n",
    "    pbar = tqdm(enumerate(loader))\n",
    "    for step, (data, target) in pbar:\n",
    "      data = torch.squeeze(data, 0)\n",
    "      target = torch.squeeze(target, 0)\n",
    "      pred = model(data)\n",
    "      metrics.update(pred, target)\n",
    "\n",
    "    mse = metrics.compute()  \n",
    "    mse = xm.mesh_reduce('val_mse', mse, np.mean)\n",
    "    return mse\n",
    "\n",
    "  def test_loop_fn(loader):\n",
    "    model.eval()\n",
    "    metrics = MeanSquaredError()\n",
    "    pbar = tqdm(enumerate(loader))\n",
    "    targets = []\n",
    "    preds = []\n",
    "    for step, (data, target) in pbar:\n",
    "      data = torch.squeeze(data, 0)\n",
    "      target = torch.squeeze(target, 0)\n",
    "      pred = model(data)\n",
    "      metrics.update(pred, target)\n",
    "      targets.append(target)\n",
    "      preds.append(pred)\n",
    "    # collect data from all processes\n",
    "    targets = xm.all_gather(torch.stack(targets))\n",
    "    preds = xm.all_gather(torch.stack(preds))\n",
    "    if xm.is_master_ordinal(): plot_scatter(targets, preds, writer)\n",
    "    # compute the MSE per-process then reduce\n",
    "    mse = metrics.compute()  \n",
    "    mse = xm.mesh_reduce('test_mse', mse, np.mean)\n",
    "    return mse\n",
    "\n",
    "  train_device_loader = pl.MpDeviceLoader(train_loader, device)\n",
    "  val_device_loader = pl.MpDeviceLoader(val_loader, device)\n",
    "  mse, min_mse = 1e3, 1e3\n",
    "  best_epoch = -1\n",
    "  for epoch in range(1, flags.num_epochs + 1):\n",
    "    xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n",
    "    train_loop_fn(epoch, train_device_loader)\n",
    "    xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))\n",
    "\n",
    "    mse = val_loop_fn(val_device_loader)\n",
    "    xm.master_print('Epoch {} test end {}, MSE={:.2f}'.format(\n",
    "        epoch, test_utils.now(), mse))\n",
    "    if mse < min_mse:\n",
    "      min_mse = mse\n",
    "      best_epoch = epoch\n",
    "    test_utils.write_to_summary(\n",
    "        writer,\n",
    "        epoch,\n",
    "        dict_to_write={'MSE/val': mse},\n",
    "        write_xla_metrics=True)\n",
    "    if flags.metrics_debug:\n",
    "      xm.master_print(met.metrics_report())\n",
    "\n",
    "  test_utils.close_summary_writer(writer)\n",
    "  xm.master_print(f'Min MSE: {min_mse:.2f} from Epoch {best_epoch}')\n",
    "  return min_mse, best_epoch\n",
    "\n",
    "def _mp_fn(index, flags):\n",
    "  torch.set_default_tensor_type('torch.FloatTensor')\n",
    "  mse, best_epoch = train_model(flags)\n",
    "  if flags.tidy and os.path.isdir(flags.datadir):\n",
    "    shutil.rmtree(flags.datadir)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS.num_cores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
